---
tags:
  - AWS
  - DynamoDB
  - NoSQL
  - Database
---

# DynamoDBì˜ ë¬´í•œ í™•ì¥: Consistent Hashingì˜ ë§ˆë²•

## ğŸ¯ Lyftì˜ 10ì–µ ë¼ì´ë“œ ë„ì „

### 2021ë…„ íŒ¬ë°ë¯¹ ì´í›„ í­ë°œì  ì„±ì¥

```text
ğŸ“… 2021ë…„ 7ì›”, ê²½ì œ ì¬ê°œë°©
ğŸš— ì¼ì¼ ë¼ì´ë“œ: 300ë§Œ â†’ 1,000ë§Œ
ğŸ“ ìœ„ì¹˜ ì—…ë°ì´íŠ¸: ì´ˆë‹¹ 500ë§Œ ê±´
ğŸ’¾ ë°ì´í„° ì¦ê°€: ì¼ 10TB
âš¡ ì‘ë‹µ ì‹œê°„ ìš”êµ¬: < 10ms
```

Lyftì˜ ì—”ì§€ë‹ˆì–´ë§ íŒ€ì€ ê¸‰ê²©í•œ ì„±ì¥ì— ì§ë©´í–ˆìŠµë‹ˆë‹¤. ê¸°ì¡´ ê´€ê³„í˜• ë°ì´í„°ë² ì´ìŠ¤ëŠ” í•œê³„ë¥¼ ë³´ì˜€ì£ :

- **ìƒ¤ë”© ì§€ì˜¥**: ìˆ˜ë™ ìƒ¤ë“œ ë¦¬ë°¸ëŸ°ì‹±ì— ì£¼ 40ì‹œê°„
- **í•« íŒŒí‹°ì…˜**: ë‰´ìš•/SF ì§€ì—­ ë°ì´í„° ì§‘ì¤‘
- **ìŠ¤ì¼€ì¼ í•œê³„**: ìˆ˜ì§ í™•ì¥ ë¶ˆê°€ëŠ¥
- **ë³µì¡í•œ ì¿¼ë¦¬**: ìœ„ì¹˜ ê¸°ë°˜ ì¡°íšŒ ì§€ì—°

**"ë” ì´ìƒ ìƒ¤ë”©ìœ¼ë¡œëŠ” í•´ê²°í•  ìˆ˜ ì—†ë‹¤. ë¬´í•œ í™•ì¥ì´ í•„ìš”í•˜ë‹¤!"**

## ğŸš€ DynamoDB ì•„í‚¤í…ì²˜: ë¬´í•œ í™•ì¥ì˜ ë¹„ë°€

### Consistent Hashingê³¼ íŒŒí‹°ì…˜

```mermaid
graph TB
    subgraph "Consistent Hash Ring"
        N1["Node 1: 0-85"]
        N2["Node 2: 86-170"]
        N3["Node 3: 171-255"]
        N4["Node 4: 256-341"]
        N5["Node 5: 342-427"]
        N6["Node 6: 428-512"]

        N1 --> N2
        N2 --> N3
        N3 --> N4
        N4 --> N5
        N5 --> N6
        N6 --> N1
    end

    subgraph "Virtual Nodes (Tokens)"
        V1[vNode 1-1]
        V2[vNode 1-2]
        V3[vNode 2-1]
        V4[vNode 2-2]
    end

    subgraph "Replication"
        R1["Replica 1: Leader"]
        R2["Replica 2: Follower"]
        R3["Replica 3: Follower"]
    end

    Item["Item: ride_id=123"] --> |"Hash(ride_id)=250"| N4
    N4 --> R1
    R1 -.->|Quorum Write| R2
    R1 -.->|Quorum Write| R3

    style R1 fill:#90EE90
    style R2 fill:#FFB6C1
    style R3 fill:#FFB6C1
```

### íŒŒí‹°ì…˜ í‚¤ í•´ì‹±ê³¼ ë¶„ì‚°

```python
class DynamoDBPartitioning:
    def __init__(self):
        self.hash_space = 2**128  # MD5 í•´ì‹œ ê³µê°„
        self.partition_count = 1000  # ì´ˆê¸° íŒŒí‹°ì…˜

    def calculate_partition(self, partition_key):
        """
        íŒŒí‹°ì…˜ í‚¤ë¥¼ ë¬¼ë¦¬ì  íŒŒí‹°ì…˜ìœ¼ë¡œ ë§¤í•‘
        """
        # 1. MD5 í•´ì‹œ ê³„ì‚°
        hash_value = hashlib.md5(partition_key.encode()).hexdigest()
        hash_int = int(hash_value, 16)

        # 2. í•´ì‹œ ë§ì—ì„œ ìœ„ì¹˜ ê²°ì •
        position = hash_int % self.hash_space

        # 3. ë‹´ë‹¹ íŒŒí‹°ì…˜ ì°¾ê¸°
        partition = self.find_partition(position)

        # 4. ë³µì œë³¸ ê²°ì • (N=3)
        replicas = self.get_replicas(partition, count=3)

        return {
            "partition": partition,
            "replicas": replicas,
            "consistency": "eventual",  # ê¸°ë³¸ê°’
            "quorum": "W=1, R=1"  # ì“°ê¸° 1ê°œ, ì½ê¸° 1ê°œ
        }
```

## ğŸ­ Auto Scalingê³¼ Adaptive Capacity

### íŒŒí‹°ì…˜ ìë™ ë¶„í• 

```mermaid
sequenceDiagram
    participant App as Application
    participant DDB as DynamoDB
    participant AS as Auto Scaling
    participant PM as Partition Manager
    participant S1 as Storage Node 1
    participant S2 as Storage Node 2

    Note over App, DDB: === íŠ¸ë˜í”½ ì¦ê°€ ê°ì§€ ===
    App->>DDB: High Write Rate
    DDB->>AS: Metrics: WCU > 3000
    AS->>PM: Trigger Partition Split

    Note over PM, S2: === íŒŒí‹°ì…˜ ë¶„í•  í”„ë¡œì„¸ìŠ¤ ===
    PM->>S1: Prepare Split
    S1->>S1: Create Checkpoint
    PM->>S2: Allocate New Partition
    S1->>S2: Transfer Half Data
    PM->>PM: Update Routing Table

    Note over DDB: === ë¬´ì¤‘ë‹¨ ì „í™˜ ===
    DDB->>App: Continue Serving
    Note right of DDB: Split Time: < 1ì´ˆ
```

### Adaptive Capacity ë™ì‘

```python
class AdaptiveCapacity:
    def __init__(self):
        self.base_capacity = {
            "RCU": 3000,  # Read Capacity Units
            "WCU": 1000   # Write Capacity Units
        }

    def handle_hot_partition(self, partition_id):
        """
        í•« íŒŒí‹°ì…˜ ìë™ ì²˜ë¦¬
        """
        # 1. í•« íŒŒí‹°ì…˜ ê°ì§€
        metrics = self.get_partition_metrics(partition_id)

        if metrics["consumed_capacity"] > metrics["provisioned_capacity"]:
            # 2. Adaptive Capacity í™œì„±í™”
            isolation_capacity = self.isolate_partition(partition_id)

            # 3. ìµœëŒ€ 5ë¶„ê°„ ë²„ìŠ¤íŠ¸ í—ˆìš©
            burst_result = {
                "original_capacity": 100,  # WCU
                "burst_capacity": 300,     # 3x ë²„ìŠ¤íŠ¸
                "duration": "5 minutes",
                "auto_scaling": "triggered"
            }

            # 4. ì˜êµ¬ì  í•´ê²°: íŒŒí‹°ì…˜ ë¶„í• 
            if metrics["sustained_high_traffic"]:
                self.split_partition(partition_id)

            return burst_result
```

## ğŸ” Global Tables: ë‹¤ì¤‘ ë¦¬ì „ ë³µì œ

### Cross-Region ë³µì œ ì•„í‚¤í…ì²˜

```mermaid
graph TB
    subgraph "US-West-2 (Primary)"
        USW[DynamoDB Table, rides-usw]
        USWL[Local Secondary Index]
        USWG[Global Secondary Index]
    end

    subgraph "US-East-1 (Replica)"
        USE[DynamoDB Table, rides-use]
        USEL[Local Secondary Index]
        USEG[Global Secondary Index]
    end

    subgraph "EU-West-1 (Replica)"
        EU[DynamoDB Table, rides-eu]
        EUL[Local Secondary Index]
        EUG[Global Secondary Index]
    end

    subgraph "Global Table Stream"
        GTS[DynamoDB Streams]
        CRDT[Conflict Resolution, Last Writer Wins]
    end

    USW -.->|Async Replication, < 1ì´ˆ| USE
    USW -.->|Async Replication, < 1ì´ˆ| EU
    USE -.->|Async Replication| USW
    USE -.->|Async Replication| EU
    EU -.->|Async Replication| USW
    EU -.->|Async Replication| USE

    USW --> GTS
    USE --> GTS
    EU --> GTS
    GTS --> CRDT

    style USW fill:#90EE90
    style USE fill:#87CEEB
    style EU fill:#FFB6C1
```

### Conflict Resolution

```python
class GlobalTableConflictResolution:
    def __init__(self):
        self.resolution_strategy = "Last Writer Wins"

    def resolve_conflict(self, item_versions):
        """
        ê¸€ë¡œë²Œ í…Œì´ë¸” ì¶©ëŒ í•´ê²°
        """
        # ì—¬ëŸ¬ ë¦¬ì „ì—ì„œ ë™ì‹œ ì—…ë°ì´íŠ¸ ë°œìƒ
        conflicts = [
            {
                "region": "us-west-2",
                "timestamp": 1640000000.123,
                "version": {"status": "picked_up"},
                "vector_clock": {"usw": 5, "use": 3, "eu": 2}
            },
            {
                "region": "us-east-1",
                "timestamp": 1640000000.124,
                "version": {"status": "completed"},
                "vector_clock": {"usw": 4, "use": 4, "eu": 2}
            }
        ]

        # Last Writer Wins ì „ëµ
        winner = max(conflicts, key=lambda x: x["timestamp"])

        # ëª¨ë“  ë¦¬ì „ì— winner ì „íŒŒ
        self.propagate_to_all_regions(winner["version"])

        return {
            "resolution": "Last Writer Wins",
            "winner": winner["region"],
            "propagation_time": "< 1 second"
        }
```

## ğŸ¨ ì¸ë±ìŠ¤ ì „ëµ

### GSI vs LSI ì„ íƒ

```python
class DynamoDBIndexStrategy:
    def __init__(self):
        self.table_schema = {
            "table_name": "rides",
            "partition_key": "ride_id",
            "sort_key": "timestamp"
        }

    def create_indexes(self):
        """
        Lyft ë¼ì´ë“œ í…Œì´ë¸” ì¸ë±ìŠ¤ ì„¤ê³„
        """
        indexes = {
            # Local Secondary Index (ê°™ì€ íŒŒí‹°ì…˜ í‚¤)
            "LSI": {
                "DriverIndex": {
                    "partition_key": "ride_id",  # ë™ì¼
                    "sort_key": "driver_id",      # ë‹¤ë¦„
                    "projection": "ALL",
                    "use_case": "íŠ¹ì • ë¼ì´ë“œì˜ ë“œë¼ì´ë²„ ì´ë ¥"
                }
            },

            # Global Secondary Index (ë‹¤ë¥¸ íŒŒí‹°ì…˜ í‚¤)
            "GSI": {
                "UserRidesIndex": {
                    "partition_key": "user_id",
                    "sort_key": "timestamp",
                    "projection": "INCLUDE",
                    "attributes": ["ride_id", "status", "fare"],
                    "use_case": "ì‚¬ìš©ìë³„ ë¼ì´ë“œ ì´ë ¥",
                    "capacity": {
                        "RCU": 1000,
                        "WCU": 500
                    }
                },

                "CityStatusIndex": {
                    "partition_key": "city#status",  # Composite key
                    "sort_key": "timestamp",
                    "projection": "KEYS_ONLY",
                    "use_case": "ë„ì‹œë³„ ìƒíƒœë³„ ë¼ì´ë“œ ì¡°íšŒ"
                },

                "DriverEarningsIndex": {
                    "partition_key": "driver_id",
                    "sort_key": "date",
                    "projection": "ALL",
                    "use_case": "ë“œë¼ì´ë²„ ìˆ˜ìµ ê³„ì‚°"
                }
            }
        }

        return indexes
```

### ì¸ë±ìŠ¤ ì˜¤ë²„ë¡œë”© íŒ¨í„´

```python
def index_overloading_pattern():
    """
    í•˜ë‚˜ì˜ GSIë¡œ ì—¬ëŸ¬ ì•¡ì„¸ìŠ¤ íŒ¨í„´ ì§€ì›
    """
    # ë‹¨ì¼ GSIë¡œ ì—¬ëŸ¬ ì¿¼ë¦¬ ì§€ì›
    gsi_design = {
        "GSI_name": "GSI1",
        "partition_key": "GSI1PK",
        "sort_key": "GSI1SK",

        "access_patterns": [
            {
                "pattern": "Get rides by user",
                "GSI1PK": "USER#123",
                "GSI1SK": "RIDE#2024-01-01#456"
            },
            {
                "pattern": "Get rides by driver",
                "GSI1PK": "DRIVER#789",
                "GSI1SK": "RIDE#2024-01-01#456"
            },
            {
                "pattern": "Get rides by city",
                "GSI1PK": "CITY#SF",
                "GSI1SK": "STATUS#active#RIDE#456"
            }
        ]
    }

    return gsi_design
```

## ğŸš€ DynamoDB Streamsì™€ CDC

### Change Data Capture íŒŒì´í”„ë¼ì¸

```mermaid
graph LR
    subgraph "DynamoDB"
        T[Table: rides]
        S[DynamoDB Streams]
    end

    subgraph "Stream Processing"
        L[Lambda Function]
        K[Kinesis Data Firehose]
    end

    subgraph "Analytics"
        S3[S3 Data Lake]
        ES[OpenSearch]
        RDS[Aurora Analytics]
    end

    subgraph "Real-time"
        WS[WebSocket API]
        APP[Mobile App]
    end

    T --> S
    S --> L
    L --> K
    L --> WS
    K --> S3
    K --> ES
    L --> RDS
    WS --> APP

    style L fill:#90EE90
    style S fill:#FFB6C1
```

### Stream ì²˜ë¦¬ êµ¬í˜„

```python
def process_dynamodb_stream(event):
    """
    DynamoDB Streams ì´ë²¤íŠ¸ ì²˜ë¦¬
    """
    for record in event['Records']:
        event_name = record['eventName']

        if event_name == 'INSERT':
            # ìƒˆ ë¼ì´ë“œ ìƒì„±
            new_ride = record['dynamodb']['NewImage']

            # ì‹¤ì‹œê°„ ì•Œë¦¼
            notify_driver(new_ride['driver_id']['S'])

            # ë¶„ì„ ë°ì´í„° ì €ì¥
            send_to_analytics(new_ride)

        elif event_name == 'MODIFY':
            # ë¼ì´ë“œ ìƒíƒœ ë³€ê²½
            old_image = record['dynamodb']['OldImage']
            new_image = record['dynamodb']['NewImage']

            if old_image['status']['S'] != new_image['status']['S']:
                # ìƒíƒœ ë³€ê²½ ì¶”ì 
                track_status_change(
                    ride_id=new_image['ride_id']['S'],
                    old_status=old_image['status']['S'],
                    new_status=new_image['status']['S']
                )

        elif event_name == 'REMOVE':
            # ë¼ì´ë“œ ì‚­ì œ (ê±°ì˜ ì—†ìŒ)
            archive_ride(record['dynamodb']['OldImage'])

    return {'statusCode': 200}
```

## ğŸ’° ë¹„ìš© ìµœì í™” ì „ëµ

### On-Demand vs Provisioned

```python
class CostOptimization:
    def analyze_pricing_model(self):
        """
        Lyftì˜ ë¹„ìš© ìµœì í™” ë¶„ì„
        """
        # íŠ¸ë˜í”½ íŒ¨í„´ ë¶„ì„
        traffic_pattern = {
            "peak_hours": {
                "7-9am": 10000,   # RCU
                "5-7pm": 15000,   # RCU
                "fri_sat_night": 20000  # RCU
            },
            "off_peak": 2000  # RCU
        }

        # Provisioned Capacity ë¹„ìš©
        provisioned_cost = {
            "base_capacity": 5000,  # RCU
            "auto_scaling": True,
            "monthly_cost": 2500,   # USD
            "reserved_capacity": {
                "1_year": 1750,     # 30% í• ì¸
                "3_year": 1250      # 50% í• ì¸
            }
        }

        # On-Demand ë¹„ìš©
        on_demand_cost = {
            "price_per_million": 0.25,  # USD per million reads
            "monthly_requests": 10_000_000_000,  # 100ì–µ ì½ê¸°
            "monthly_cost": 2500,
            "benefits": [
                "ì˜ˆì¸¡ ë¶ˆê°€ëŠ¥í•œ íŠ¸ë˜í”½",
                "ìë™ ìŠ¤ì¼€ì¼ë§",
                "ê´€ë¦¬ ë¶ˆí•„ìš”"
            ]
        }

        # í•˜ì´ë¸Œë¦¬ë“œ ì „ëµ
        hybrid_strategy = {
            "base_tables": "Provisioned with Auto Scaling",
            "new_features": "On-Demand",
            "analytics": "On-Demand",
            "cost_savings": "40%"
        }

        return hybrid_strategy
```

### TTLì„ í™œìš©í•œ ìë™ ì •ë¦¬

```python
def configure_ttl():
    """
    Time-To-Liveë¡œ ì˜¤ë˜ëœ ë°ì´í„° ìë™ ì‚­ì œ
    """
    ttl_config = {
        "attribute_name": "expiration_time",
        "strategy": {
            "completed_rides": {
                "retention": "90 days",
                "ttl_value": "timestamp + 7776000"  # 90ì¼
            },
            "cancelled_rides": {
                "retention": "30 days",
                "ttl_value": "timestamp + 2592000"  # 30ì¼
            },
            "location_history": {
                "retention": "7 days",
                "ttl_value": "timestamp + 604800"   # 7ì¼
            }
        },
        "benefits": [
            "ìë™ ë°ì´í„° ì •ë¦¬",
            "ìŠ¤í† ë¦¬ì§€ ë¹„ìš© ì ˆê°",
            "ë°±ê·¸ë¼ìš´ë“œ ì‚­ì œ (ë¬´ë£Œ)"
        ]
    }

    return ttl_config
```

## ğŸš¨ ì‹¤ì „ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### Case 1: Hot Partition

```python
def handle_hot_partition():
    """
    í•« íŒŒí‹°ì…˜ ë¬¸ì œ í•´ê²°
    """
    problem = {
        "symptom": "ProvisionedThroughputExceededException",
        "cause": "íŠ¹ì • íŒŒí‹°ì…˜ì— íŠ¸ë˜í”½ ì§‘ì¤‘",
        "example": "ë„ì‹œë³„ ë¼ì´ë“œ - NYCì— 50% ì§‘ì¤‘"
    }

    solutions = {
        "1_add_randomness": {
            "before": "partition_key = 'CITY#NYC'",
            "after": "partition_key = f'CITY#NYC#{random.randint(0,9)}'",
            "effect": "10ê°œ íŒŒí‹°ì…˜ìœ¼ë¡œ ë¶„ì‚°"
        },

        "2_write_sharding": {
            "implementation": """
                # ì“°ê¸° ë¶„ì‚°
                for i in range(10):
                    shard_key = f"{base_key}#{i}"
                    write_to_shard(shard_key, data/10)

                # ì½ê¸° ì§‘ê³„
                total = sum(read_from_shard(f"{base_key}#{i}")
                           for i in range(10))
            """
        },

        "3_adaptive_capacity": {
            "action": "ìë™ í™œì„±í™”",
            "burst": "ìµœëŒ€ 5ë¶„ê°„ 3x ìš©ëŸ‰",
            "long_term": "íŒŒí‹°ì…˜ ìë™ ë¶„í• "
        }
    }

    return solutions
```

### Case 2: GSI Throttling

```python
class GSIThrottling:
    def diagnose_and_fix(self):
        """
        GSI ìŠ¤ë¡œí‹€ë§ í•´ê²°
        """
        diagnosis = {
            "symptoms": [
                "UserErrors.ProvisionedThroughputExceeded",
                "GSI ConsumedCapacity > Provisioned"
            ],

            "root_causes": {
                "uneven_distribution": "GSI íŒŒí‹°ì…˜ í‚¤ ë¶„í¬ ë¶ˆê· í˜•",
                "insufficient_capacity": "GSI RCU/WCU ë¶€ì¡±",
                "projection_size": "ë¶ˆí•„ìš”í•œ ì†ì„± í”„ë¡œì ì…˜"
            },

            "solutions": {
                "immediate": [
                    "GSI ìš©ëŸ‰ ì¦ê°€",
                    "Auto Scaling í™œì„±í™”",
                    "ë°±ì˜¤í”„ ì¬ì‹œë„ êµ¬í˜„"
                ],

                "long_term": [
                    "GSI íŒŒí‹°ì…˜ í‚¤ ì¬ì„¤ê³„",
                    "Projection ìµœì í™” (KEYS_ONLY)",
                    "ì¿¼ë¦¬ íŒ¨í„´ ìºì‹±"
                ]
            }
        }

        return diagnosis
```

### Case 3: ì¼ê´€ì„± ë¬¸ì œ

```python
def handle_consistency_issues():
    """
    Eventually Consistent ì½ê¸° ë¬¸ì œ í•´ê²°
    """
    consistency_patterns = {
        "strong_consistency": {
            "use_case": "ê¸ˆìœµ ê±°ë˜, ì¬ê³  í™•ì¸",
            "implementation": {
                "ConsistentRead": True,
                "cost": "2x RCU ì†Œë¹„"
            }
        },

        "eventual_consistency": {
            "use_case": "ë¶„ì„, ë¦¬í¬íŠ¸",
            "implementation": {
                "ConsistentRead": False,
                "latency": "< 1ì´ˆ ë³µì œ ì§€ì—°"
            }
        },

        "hybrid_approach": {
            "critical_reads": "Strong Consistency",
            "normal_reads": "Eventual Consistency",
            "cost_optimization": "30% RCU ì ˆê°"
        }
    }

    return consistency_patterns
```

## ğŸ¯ DynamoDB Accelerator (DAX)

### ë§ˆì´í¬ë¡œì´ˆ ë ˆì´í„´ì‹œ ë‹¬ì„±

```mermaid
graph TB
    subgraph "Application Layer"
        APP[Application]
    end

    subgraph "DAX Cluster"
        DAX1[DAX Node 1, Primary]
        DAX2[DAX Node 2, Replica]
        DAX3[DAX Node 3, Replica]

        CACHE[In-Memory Cache, Item Cache: 5ë¶„, Query Cache: 5ë¶„]
    end

    subgraph "DynamoDB"
        DDB[DynamoDB Table]
    end

    APP -->|1Î¼s| DAX1
    DAX1 -->|Cache Hit| CACHE
    DAX1 -->|Cache Miss, 10ms| DDB
    DAX1 -.->|Replication| DAX2
    DAX1 -.->|Replication| DAX3

    style CACHE fill:#90EE90
```

```python
def implement_dax():
    """
    DAX êµ¬í˜„ ë° ìµœì í™”
    """
    dax_config = {
        "cluster_size": "3 nodes",
        "node_type": "dax.r4.large",
        "cache_ttl": {
            "item_cache": 300,  # 5ë¶„
            "query_cache": 300   # 5ë¶„
        },

        "performance": {
            "read_latency": {
                "without_dax": "10ms",
                "with_dax_miss": "10ms",
                "with_dax_hit": "1Î¼s"  # ë§ˆì´í¬ë¡œì´ˆ!
            },
            "throughput": "10x ì¦ê°€"
        },

        "best_practices": [
            "ì½ê¸° ì§‘ì•½ì  ì›Œí¬ë¡œë“œ",
            "ë°˜ë³µì ì¸ ì½ê¸° íŒ¨í„´",
            "ì‹¤ì‹œê°„ ë¦¬ë”ë³´ë“œ",
            "ì„¸ì…˜ ìŠ¤í† ì–´"
        ]
    }

    return dax_config
```

## ğŸ¬ ë§ˆë¬´ë¦¬: Lyftì˜ 10ì–µ ë¼ì´ë“œ ë‹¬ì„±

2024ë…„ í˜„ì¬, LyftëŠ” DynamoDBë¡œ:

- **í™•ì¥ì„±**: 0 â†’ 10ì–µ ë¼ì´ë“œ/ì›” ìë™ í™•ì¥
- **ì„±ëŠ¥**: í‰ê·  ì‘ë‹µ ì‹œê°„ 5ms
- **ê°€ìš©ì„±**: 99.999% (Five 9s)
- **ë¹„ìš©**: RDS ëŒ€ë¹„ 60% ì ˆê°
- **ìš´ì˜**: ìë™í™”ë¡œ DBA ë¶ˆí•„ìš”

**"DynamoDBëŠ” ìš°ë¦¬ê°€ ì¸í”„ë¼ê°€ ì•„ë‹Œ ê³ ê° ê²½í—˜ì— ì§‘ì¤‘í•  ìˆ˜ ìˆê²Œ í•´ì£¼ì—ˆë‹¤."**

ë‹¤ìŒ ë¬¸ì„œì—ì„œëŠ” [Auroraì˜ í´ë¼ìš°ë“œ ë„¤ì´í‹°ë¸Œ í˜ì‹ ](03-aurora.md)ì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤!
