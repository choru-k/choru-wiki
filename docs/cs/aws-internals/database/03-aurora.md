---
tags:
  - AWS
  - Aurora
  - Database
  - MySQL
  - PostgreSQL
---

# Auroraì˜ í´ë¼ìš°ë“œ ë„¤ì´í‹°ë¸Œ í˜ì‹ : Log-as-Databaseì˜ ë§ˆë²•

## ğŸ¯ Netflixì˜ ìŠ¤íŠ¸ë¦¬ë° ë©”íƒ€ë°ì´í„° í˜ëª…

### 2020ë…„ ê¸€ë¡œë²Œ íŒ¬ë°ë¯¹ ìŠ¤íŠ¸ë¦¬ë° í­ì¦

```text
ğŸ“… 2020ë…„ 3ì›”, ì „ ì„¸ê³„ ë½ë‹¤ìš´
ğŸ“º ë™ì‹œ ìŠ¤íŠ¸ë¦¬ë°: 2ì–µ ì„¸ì…˜
ğŸ¬ ì½˜í…ì¸  ë©”íƒ€ë°ì´í„°: 100TB
ğŸ” ì´ˆë‹¹ ê²€ìƒ‰ ì¿¼ë¦¬: 500ë§Œ
âš¡ ìš”êµ¬ ë ˆì´í„´ì‹œ: < 100ms
```

Netflixì˜ ë°ì´í„°ë² ì´ìŠ¤ íŒ€ì€ ì „ë¡€ ì—†ëŠ” ë„ì „ì— ì§ë©´í–ˆìŠµë‹ˆë‹¤. ê¸°ì¡´ RDS MySQLì€ í•œê³„ë¥¼ ë³´ì˜€ì£ :

- **ë³µì œ ì§€ì—°**: ì½ê¸° ë³µì œë³¸ 5ì´ˆ ì§€ì—°
- **ìŠ¤í† ë¦¬ì§€ í•œê³„**: 64TB ì œí•œ ì„ë°•
- **ë°±ì—… ì‹œê°„**: ì¼ì¼ ë°±ì—… 6ì‹œê°„
- **í˜ì¼ì˜¤ë²„**: 30ì´ˆ ì´ìƒ

**"ì „í†µì ì¸ ë°ì´í„°ë² ì´ìŠ¤ ì•„í‚¤í…ì²˜ë¡œëŠ” ë¶ˆê°€ëŠ¥í•˜ë‹¤. í´ë¼ìš°ë“œ ë„¤ì´í‹°ë¸Œê°€ í•„ìš”í•˜ë‹¤!"**

## ğŸš€ Aurora ì•„í‚¤í…ì²˜: ìŠ¤í† ë¦¬ì§€ì™€ ì»´í“¨íŒ… ë¶„ë¦¬

### Log-Structured Storageì˜ í˜ì‹ 

```mermaid
graph TB
    subgraph "Database Layer"
        P[Primary Instance, r6g.16xlarge]
        R1[Read Replica 1]
        R2[Read Replica 2]
        R3[Read Replica 3]
        R4[Read Replica 4]
        R5[Read Replica 5]
    end
    
    subgraph "Aurora Storage Layer"
        subgraph "AZ-1"
            S11[Segment 1-1]
            S12[Segment 1-2]
        end
        
        subgraph "AZ-2"
            S21[Segment 2-1]
            S22[Segment 2-2]
        end
        
        subgraph "AZ-3"
            S31[Segment 3-1]
            S32[Segment 3-2]
        end
        
        LOG[Distributed Log, 6-way Replication]
    end
    
    P -->|Write: Log Records Only| LOG
    R1 -->|Read: Page Cache| LOG
    R2 -->|Read: Page Cache| LOG
    
    LOG --> S11
    LOG --> S12
    LOG --> S21
    LOG --> S22
    LOG --> S31
    LOG --> S32
    
    style P fill:#90EE90
    style LOG fill:#FFB6C1
```

### Quorum ê¸°ë°˜ ë¶„ì‚° ìŠ¤í† ë¦¬ì§€

```python
class AuroraStorageQuorum:
    def __init__(self):
        self.replicas = 6  # 6ê°œ ë³µì œë³¸
        self.write_quorum = 4  # 4/6 ì“°ê¸°
        self.read_quorum = 3   # 3/6 ì½ê¸°
        
    def write_operation(self, log_record):
        """
        Auroraì˜ ì¿¼ëŸ¼ ê¸°ë°˜ ì“°ê¸°
        """
        # 1. Log recordë§Œ ì „ì†¡ (í˜ì´ì§€ ì „ì²´ X)
        log_lsn = self.generate_lsn()
        
        # 2. 6ê°œ ìŠ¤í† ë¦¬ì§€ ë…¸ë“œì— ë³‘ë ¬ ì „ì†¡
        write_futures = []
        for node in self.storage_nodes[:6]:
            future = self.async_write(node, log_record, log_lsn)
            write_futures.append(future)
        
        # 3. 4ê°œ ë…¸ë“œ ACK ëŒ€ê¸° (Quorum)
        acks = 0
        for future in write_futures:
            try:
                if future.get(timeout_ms=10):
                    acks += 1
                    if acks >= self.write_quorum:
                        # 4/6 ë‹¬ì„± ì‹œ ì¦‰ì‹œ ì»¤ë°‹
                        return {"status": "COMMITTED", "lsn": log_lsn}
            except TimeoutError:
                continue
        
        # ì¿¼ëŸ¼ ì‹¤íŒ¨
        if acks < self.write_quorum:
            raise QuorumNotMetException()
```

### ìŠ¤í† ë¦¬ì§€ ë…¸ë“œì˜ ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬

```mermaid
sequenceDiagram
    participant DB as Database Instance
    participant SN as Storage Node
    participant BG as Background Process
    participant S3 as S3 Backup
    
    Note over DB, SN: === Write Path (Foreground) ===
    DB->>SN: Send Log Record (100 bytes)
    SN->>SN: Persist to SSD
    SN-->>DB: ACK (< 1ms)
    
    Note over SN, S3: === Background Processing ===
    BG->>SN: Coalesce Log Records
    BG->>BG: Apply to Page
    BG->>BG: Generate New Page Version
    BG->>S3: Backup to S3
    
    Note over DB, SN: === Read Path ===
    DB->>SN: Request Page
    alt Page in Cache
        SN-->>DB: Return Cached Page
    else Page Not in Cache
        SN->>SN: Reconstruct from Logs
        SN-->>DB: Return Page
    end
```

## ğŸ­ Auroraì˜ í˜ì‹ ì  ê¸°ëŠ¥ë“¤

### Fast Cloning: Copy-on-Write

```python
class AuroraFastClone:
    def __init__(self):
        self.clone_time = "ì´ˆ ë‹¨ìœ„"
        self.storage_overhead = "0% (ì´ˆê¸°)"
        
    def create_clone(self, source_cluster):
        """
        Aurora Fast Clone ìƒì„±
        """
        # 1. ë©”íƒ€ë°ì´í„°ë§Œ ë³µì‚¬ (ì´ˆ ë‹¨ìœ„)
        clone_metadata = {
            "source_cluster": source_cluster,
            "clone_time": datetime.now(),
            "storage_pointers": self.copy_storage_pointers(source_cluster)
        }
        
        # 2. Copy-on-Write ì„¤ì •
        cow_config = {
            "shared_pages": True,  # ì´ˆê¸°ì—ëŠ” ëª¨ë“  í˜ì´ì§€ ê³µìœ 
            "divergence_tracking": True,
            "space_efficiency": "ë³€ê²½ëœ í˜ì´ì§€ë§Œ ìƒˆë¡œ ì €ì¥"
        }
        
        # 3. í´ë¡  í™œì„±í™”
        clone_cluster = self.activate_clone(clone_metadata, cow_config)
        
        return {
            "clone_id": clone_cluster.id,
            "creation_time": "5ì´ˆ",
            "initial_storage_cost": "$0",
            "use_cases": [
                "ê°œë°œ/í…ŒìŠ¤íŠ¸ í™˜ê²½",
                "ë¶„ì„ ì›Œí¬ë¡œë“œ ê²©ë¦¬",
                "A/B í…ŒìŠ¤íŒ…",
                "ì¬í•´ ë³µêµ¬ í…ŒìŠ¤íŠ¸"
            ]
        }
```

### Backtrack: ì‹œê°„ ì—¬í–‰

```python
class AuroraBacktrack:
    def __init__(self):
        self.backtrack_window = 72  # ìµœëŒ€ 72ì‹œê°„
        
    def backtrack_database(self, target_time):
        """
        ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ê³¼ê±° ì‹œì ìœ¼ë¡œ ë˜ëŒë¦¬ê¸°
        """
        # 1. ë˜ëŒë¦´ ì‹œì  ê²€ì¦
        if not self.is_within_window(target_time):
            raise BacktrackWindowExceededException()
        
        # 2. íƒ€ê²Ÿ LSN ì°¾ê¸°
        target_lsn = self.find_lsn_at_time(target_time)
        
        # 3. ë˜ëŒë¦¬ê¸° ì‹¤í–‰ (ë°ì´í„° ë³µì‚¬ ì—†ìŒ!)
        backtrack_process = {
            "current_lsn": self.current_lsn,
            "target_lsn": target_lsn,
            "method": "Rewind log pointer",
            "downtime": "ìˆ˜ ì´ˆ",
            "data_movement": "ì—†ìŒ"
        }
        
        # 4. ì¸ìŠ¤í„´ìŠ¤ ì¬ì‹œì‘
        self.restart_instances()
        
        return {
            "backtrack_time": "10ì´ˆ",
            "vs_pitr": "PITRì€ ìƒˆ í´ëŸ¬ìŠ¤í„° ìƒì„± í•„ìš” (30ë¶„)",
            "use_cases": [
                "ì‹¤ìˆ˜ë¡œ ì‚­ì œí•œ ë°ì´í„° ë³µêµ¬",
                "ì˜ëª»ëœ ë°°í¬ ë¡¤ë°±",
                "íŠ¹ì • ì‹œì  ë°ì´í„° ë¶„ì„"
            ]
        }
```

## ğŸš€ Aurora Serverless v2: ìë™ ìŠ¤ì¼€ì¼ë§

### ì„¸ë°€í•œ ìš©ëŸ‰ ì¡°ì •

```mermaid
graph TB
    subgraph "Aurora Serverless v2 Scaling"
        subgraph "Morning Peak"
            M[0.5 ACU â†’ 4 ACU, in 30 seconds]
        end
        
        subgraph "Afternoon"
            A[4 ACU â†’ 1 ACU, Scale down]
        end
        
        subgraph "Evening Peak"
            E[1 ACU â†’ 8 ACU, Auto scale]
        end
        
        subgraph "Night"
            N[8 ACU â†’ 0.5 ACU, Minimum]
        end
    end
    
    subgraph "Cost"
        C[Pay per ACU-hour, $0.12 per ACU-hour]
    end
    
    M --> A
    A --> E
    E --> N
    N --> M
    
    style M fill:#FFB6C1
    style E fill:#FFB6C1
```

```python
class AuroraServerlessV2:
    def __init__(self):
        self.acu_config = {
            "min_capacity": 0.5,  # ACU
            "max_capacity": 128,   # ACU
            "1_acu": {
                "memory": "2 GB",
                "cpu": "~2 vCPUs",
                "network": "~4.5 Gbps"
            }
        }
    
    def auto_scaling_logic(self, metrics):
        """
        Serverless v2 ìë™ ìŠ¤ì¼€ì¼ë§ ë¡œì§
        """
        current_acu = self.current_capacity
        
        # CPU, ë©”ëª¨ë¦¬, ë„¤íŠ¸ì›Œí¬ ê¸°ë°˜ ìŠ¤ì¼€ì¼ë§
        scale_factors = {
            "cpu_utilization": metrics["cpu"] / 70,  # 70% íƒ€ê²Ÿ
            "memory_utilization": metrics["memory"] / 75,
            "connection_count": metrics["connections"] / 100,
            "query_latency": metrics["p99_latency"] / 100  # 100ms íƒ€ê²Ÿ
        }
        
        # ê°€ì¥ ë†’ì€ factor ê¸°ì¤€ ìŠ¤ì¼€ì¼ë§
        required_acu = current_acu * max(scale_factors.values())
        
        # 0.5 ACU ë‹¨ìœ„ë¡œ ì¡°ì •
        new_acu = round(required_acu * 2) / 2
        new_acu = max(self.acu_config["min_capacity"], 
                     min(new_acu, self.acu_config["max_capacity"]))
        
        # ìŠ¤ì¼€ì¼ë§ ì‹¤í–‰
        if new_acu != current_acu:
            self.scale_to(new_acu)
            
        return {
            "previous": current_acu,
            "new": new_acu,
            "scale_time": "< 15 seconds",
            "no_disruption": True
        }
```

## ğŸ” Aurora Global Database

### Cross-Region ì¬í•´ ë³µêµ¬

```mermaid
graph TB
    subgraph "Primary Region (us-west-2)"
        PC[Primary Cluster]
        PR1[Read Replica 1]
        PR2[Read Replica 2]
        PS[Primary Storage]
    end
    
    subgraph "Secondary Region (us-east-1)"
        SC[Secondary Cluster, Read-Only]
        SR1[Read Replica 1]
        SR2[Read Replica 2]
        SS[Secondary Storage]
    end
    
    subgraph "Secondary Region (eu-west-1)"
        EC[Secondary Cluster, Read-Only]
        ER1[Read Replica 1]
        ES[Secondary Storage]
    end
    
    PC -.->|Physical Replication, < 1ì´ˆ| SC
    PC -.->|Physical Replication, < 1ì´ˆ| EC
    
    PS -.->|Storage-level, Replication| SS
    PS -.->|Storage-level, Replication| ES
    
    Note1[RPO: 1ì´ˆ]
    Note2[RTO: 1ë¶„]
    
    style PC fill:#90EE90
    style SC fill:#87CEEB
    style EC fill:#FFB6C1
```

### Global Database í˜ì¼ì˜¤ë²„

```python
class AuroraGlobalDatabase:
    def __init__(self):
        self.regions = {
            "primary": "us-west-2",
            "secondaries": ["us-east-1", "eu-west-1"]
        }
        
    def planned_failover(self, new_primary_region):
        """
        ê³„íšëœ í˜ì¼ì˜¤ë²„ (0 ë°ì´í„° ì†ì‹¤)
        """
        steps = [
            # 1. ì“°ê¸° ì¤‘ì§€
            self.stop_writes(),
            
            # 2. ë³µì œ ë™ê¸°í™” ëŒ€ê¸°
            self.wait_for_replication_sync(),
            
            # 3. ìƒˆ Primary ìŠ¹ê²©
            self.promote_secondary(new_primary_region),
            
            # 4. DNS ì—…ë°ì´íŠ¸
            self.update_dns_cname(new_primary_region),
            
            # 5. ì´ì „ Primaryë¥¼ Secondaryë¡œ
            self.demote_to_secondary(self.regions["primary"])
        ]
        
        return {
            "data_loss": "0 bytes",
            "downtime": "< 1ë¶„",
            "automatic_backtrack": True
        }
    
    def unplanned_failover(self):
        """
        ë¹„ê³„íš í˜ì¼ì˜¤ë²„ (ì¬í•´ ìƒí™©)
        """
        # ìë™ ê°ì§€ ë° í˜ì¼ì˜¤ë²„
        return {
            "detection_time": "30ì´ˆ",
            "failover_time": "30ì´ˆ",
            "total_rto": "1ë¶„",
            "potential_data_loss": "< 1ì´ˆ (RPO)"
        }
```

## ğŸ’° ë¹„ìš© ìµœì í™” ì „ëµ

### Netflixì˜ Aurora ìµœì í™”

```python
class AuroraCostOptimization:
    def __init__(self):
        self.netflix_setup = {
            "before": {
                "solution": "RDS MySQL Multi-AZ",
                "instances": "db.r5.24xlarge x 10",
                "storage": "100TB Provisioned IOPS",
                "monthly_cost": 150000
            },
            "after": {
                "solution": "Aurora MySQL",
                "instances": "db.r6g.16xlarge x 5",
                "storage": "Pay-per-use (100TB)",
                "monthly_cost": 75000
            }
        }
    
    def optimization_strategies(self):
        return {
            "1_serverless_for_dev": {
                "env": "Development/Test",
                "config": "Serverless v2 (0.5-2 ACU)",
                "savings": "90% vs always-on"
            },
            
            "2_reserved_instances": {
                "production": "3-year reserved",
                "savings": "50% discount"
            },
            
            "3_io_optimized": {
                "high_io_workloads": "Aurora I/O-Optimized",
                "benefit": "No I/O charges",
                "break_even": "> 25% I/O cost"
            },
            
            "4_cross_region_replicas": {
                "strategy": "Read locally, write globally",
                "latency_reduction": "100ms â†’ 10ms",
                "cost": "Only storage + compute"
            }
        }
```

## ğŸ¨ Aurora Machine Learning

### ë‚´ì¥ ML í†µí•©

```python
def aurora_ml_integration():
    """
    Auroraì™€ SageMaker/Comprehend í†µí•©
    """
    ml_queries = {
        "sentiment_analysis": """
            SELECT 
                review_id,
                review_text,
                aws_comprehend_detect_sentiment(
                    review_text, 'en'
                ) as sentiment
            FROM product_reviews
            WHERE created_date = CURDATE();
        """,
        
        "fraud_detection": """
            SELECT 
                transaction_id,
                amount,
                aws_sagemaker_invoke_endpoint(
                    'fraud-detector-endpoint',
                    CONCAT(user_id, ',', amount, ',', merchant)
                ) as fraud_score
            FROM transactions
            WHERE fraud_score > 0.8;
        """,
        
        "batch_inference": """
            CREATE TABLE product_recommendations AS
            SELECT 
                user_id,
                aws_sagemaker_invoke_endpoint(
                    'recommendation-model',
                    user_features
                ) as recommended_products
            FROM user_profiles;
        """
    }
    
    return {
        "benefits": [
            "SQLì—ì„œ ì§ì ‘ ML í˜¸ì¶œ",
            "ë°ì´í„° ì´ë™ ì—†ìŒ",
            "ì‹¤ì‹œê°„ ì¶”ë¡ ",
            "ë°°ì¹˜ ì²˜ë¦¬ ì§€ì›"
        ],
        "use_cases": [
            "ê°ì„± ë¶„ì„",
            "ì‚¬ê¸° íƒì§€",
            "ì¶”ì²œ ì‹œìŠ¤í…œ",
            "ì´ìƒ íƒì§€"
        ]
    }
```

## ğŸš¨ ì‹¤ì „ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### Case 1: Reader Endpoint ë¶€í•˜ ë¶ˆê· í˜•

```python
def troubleshoot_reader_imbalance():
    """
    ì½ê¸° ë³µì œë³¸ ë¶€í•˜ ë¶ˆê· í˜• í•´ê²°
    """
    problem = {
        "symptom": "íŠ¹ì • ì½ê¸° ë³µì œë³¸ë§Œ ë†’ì€ CPU",
        "cause": "DNS ìºì‹±ìœ¼ë¡œ ì¸í•œ ê³ ì • ì—°ê²°"
    }
    
    solutions = {
        "1_custom_endpoints": {
            "config": """
                # ì»¤ìŠ¤í…€ ì—”ë“œí¬ì¸íŠ¸ ìƒì„±
                - analytics-endpoint: r6g.16xlarge ì¸ìŠ¤í„´ìŠ¤ë§Œ
                - app-endpoint: r6g.4xlarge ì¸ìŠ¤í„´ìŠ¤ë§Œ
            """,
            "benefit": "ì›Œí¬ë¡œë“œë³„ ë¶„ë¦¬"
        },
        
        "2_connection_pool": {
            "ttl": "60ì´ˆ",
            "max_idle": "30ì´ˆ",
            "effect": "ì£¼ê¸°ì  ì¬ì—°ê²°ë¡œ ë¶€í•˜ ë¶„ì‚°"
        },
        
        "3_smart_driver": {
            "driver": "AWS JDBC Driver",
            "feature": "Reader endpoint load balancing",
            "config": "aurora-load-balanced=true"
        }
    }
    
    return solutions
```

### Case 2: Storage ì‚¬ìš©ëŸ‰ ê¸‰ì¦

```python
class StorageTroubleshooting:
    def diagnose_storage_growth(self):
        """
        ë¹„ì •ìƒì ì¸ ìŠ¤í† ë¦¬ì§€ ì¦ê°€ ì§„ë‹¨
        """
        queries = {
            "check_table_sizes": """
                SELECT 
                    table_schema,
                    table_name,
                    ROUND(data_length/1024/1024/1024, 2) as data_gb,
                    ROUND(index_length/1024/1024/1024, 2) as index_gb
                FROM information_schema.tables
                ORDER BY data_length DESC
                LIMIT 10;
            """,
            
            "check_history_list": """
                -- Long-running transactions
                SHOW ENGINE INNODB STATUS;
                -- History list length í™•ì¸
            """,
            
            "check_binlog": """
                SHOW BINARY LOGS;
                -- Binlog í¬ê¸° í™•ì¸
            """
        }
        
        solutions = {
            "large_tables": "íŒŒí‹°ì…”ë‹ ë˜ëŠ” ì•„ì¹´ì´ë¹™",
            "history_list": "ì¥ê¸° ì‹¤í–‰ íŠ¸ëœì­ì…˜ ì¢…ë£Œ",
            "binlog": "binlog_expire_logs_seconds ì¡°ì •",
            "temp_tables": "ì„ì‹œ í…Œì´ë¸” ì •ë¦¬"
        }
        
        return solutions
```

### Case 3: ì¿¼ë¦¬ ì„±ëŠ¥ ì €í•˜

```python
def optimize_query_performance():
    """
    Aurora ì¿¼ë¦¬ ìµœì í™”
    """
    optimization_steps = {
        "1_enable_performance_insights": {
            "retention": "7 days (free)",
            "metrics": ["top SQL", "wait events", "DB load"]
        },
        
        "2_parallel_query": {
            "enable": "aurora_parallel_query = ON",
            "benefit": "ëŒ€ìš©ëŸ‰ ìŠ¤ìº” 16x ë¹ ë¦„",
            "use_case": "ë¶„ì„ ì¿¼ë¦¬"
        },
        
        "3_hash_joins": {
            "version": "Aurora MySQL 8.0+",
            "config": "optimizer_switch='hash_join=on'",
            "improvement": "ì¡°ì¸ ì„±ëŠ¥ 10x"
        },
        
        "4_fast_ddl": {
            "operation": "ALTER TABLE ADD COLUMN",
            "time": "ì¦‰ì‹œ (ë©”íƒ€ë°ì´í„°ë§Œ ë³€ê²½)",
            "vs_standard": "í‘œì¤€ MySQLì€ ì „ì²´ í…Œì´ë¸” ì¬ì‘ì„±"
        }
    }
    
    return optimization_steps
```

## ğŸ¯ Aurora vs RDS vs DynamoDB ì„ íƒ ê°€ì´ë“œ

```python
def choose_database():
    """
    AWS ë°ì´í„°ë² ì´ìŠ¤ ì„ íƒ ê°€ì´ë“œ
    """
    decision_matrix = {
        "Aurora": {
            "best_for": [
                "ë³µì¡í•œ ì¿¼ë¦¬ì™€ ì¡°ì¸",
                "ACID íŠ¸ëœì­ì…˜",
                "MySQL/PostgreSQL í˜¸í™˜ì„±",
                "ìë™ ìŠ¤ì¼€ì¼ë§ í•„ìš”"
            ],
            "capacity": "128TB",
            "replicas": "15ê°œ",
            "failover": "30ì´ˆ"
        },
        
        "RDS": {
            "best_for": [
                "ì˜ˆì¸¡ ê°€ëŠ¥í•œ ì›Œí¬ë¡œë“œ",
                "íŠ¹ì • DB ì—”ì§„ í•„ìš” (Oracle, SQL Server)",
                "ë‚®ì€ ë¹„ìš©",
                "ê°„ë‹¨í•œ ì„¤ì •"
            ],
            "capacity": "64TB",
            "replicas": "5ê°œ",
            "failover": "60-120ì´ˆ"
        },
        
        "DynamoDB": {
            "best_for": [
                "Key-Value ì•¡ì„¸ìŠ¤ íŒ¨í„´",
                "ë¬´í•œ í™•ì¥",
                "ê¸€ë¡œë²Œ ë¶„ì‚°",
                "ì„œë²„ë¦¬ìŠ¤"
            ],
            "capacity": "ë¬´ì œí•œ",
            "replicas": "ê¸€ë¡œë²Œ",
            "failover": "ìë™"
        }
    }
    
    return decision_matrix
```

## ğŸ¬ ë§ˆë¬´ë¦¬: Netflixì˜ Aurora ì„±ê³µ

2024ë…„ í˜„ì¬, NetflixëŠ” Auroraë¡œ:

- **í™•ì¥ì„±**: 100TB â†’ 500TB ìë™ í™•ì¥
- **ì„±ëŠ¥**: 5x ì²˜ë¦¬ëŸ‰ ì¦ê°€
- **ê°€ìš©ì„±**: 99.99% â†’ 99.999%
- **ë¹„ìš©**: 50% ì ˆê°
- **ìš´ì˜**: ë°±ì—…/ë³µêµ¬ ìë™í™”ë¡œ 80% ì‹œê°„ ì ˆì•½

**"AuroraëŠ” ìš°ë¦¬ê°€ ì§„ì •í•œ í´ë¼ìš°ë“œ ë„¤ì´í‹°ë¸Œ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ê²½í—˜í•˜ê²Œ í•´ì£¼ì—ˆë‹¤."**

ë‹¤ìŒ ë¬¸ì„œì—ì„œëŠ” [ElastiCacheì˜ ì¸ë©”ëª¨ë¦¬ ìºì‹± ì „ëµ](04-elasticache.md)ì„ íƒí—˜í•´ë³´ê² ìŠµë‹ˆë‹¤!
