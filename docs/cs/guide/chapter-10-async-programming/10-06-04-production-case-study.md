---
tags:
  - advanced
  - deep-study
  - distributed-systems
  - hands-on
  - microservices
  - monitoring
  - production-case-study
  - resilience-patterns
  - ì¸í”„ë¼ìŠ¤íŠ¸ëŸ­ì²˜
difficulty: ADVANCED
learning_time: "8-12ì‹œê°„"
main_topic: "ì¸í”„ë¼ìŠ¤íŠ¸ëŸ­ì²˜"
priority_score: 4
---

# 10.6.4: ì‹¤ë¬´ ì‚¬ë¡€ ì—°êµ¬

## ğŸ¯ ì´ ì„¹ì…˜ì—ì„œ ë°°ìš¸ ë‚´ìš©

ì‹¤ì œ í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œì˜ ëŒ€ê·œëª¨ ë¶„ì‚° ì‹œìŠ¤í…œ êµ¬ì¶•ê³¼ ìš´ì˜ ê²½í—˜ì„ í•™ìŠµí•©ë‹ˆë‹¤:

1. **ì „ì²´ ì•„í‚¤í…ì²˜ ì„¤ê³„** - ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ê¸°ë°˜ ì´ì»¤ë¨¸ìŠ¤ ì‹œìŠ¤í…œ
2. **ì‹¤ì œ ì¥ì•  ëŒ€ì‘** - 2023ë…„ ë¸”ë™í”„ë¼ì´ë°ì´ ì¥ì•  ì‚¬ë¡€
3. **ëª¨ë‹ˆí„°ë§ê³¼ ì•Œë¦¼** - ì‹¤ì‹œê°„ ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ ì „ëµ

## 1. ì£¼ë¬¸ ì²˜ë¦¬ ì‹œìŠ¤í…œ ì „ì²´ êµ¬ì¡°

### 1.1 ì•„í‚¤í…ì²˜ ê°œìš”

```yaml
# docker-compose.ymlë¡œ ë³´ëŠ” ì „ì²´ ì•„í‚¤í…ì²˜
version: '3.8'

services:
  # API Gateway
  api-gateway:
    image: kong:latest
    environment:
      - KONG_DATABASE=postgres
      - KONG_PG_HOST=kong-db
      - KONG_PLUGINS=rate-limiting,prometheus,cors,jwt
    ports:
      - "8000:8000"
      - "8443:8443"
      - "8001:8001"
    depends_on:
      - kong-db
    deploy:
      replicas: 3
      resources:
        limits:
          memory: 1G
  
  # Order Service (Orchestrator)
  order-service:
    build: ./order-service
    environment:
      - SPRING_PROFILES_ACTIVE=docker
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      - DB_HOST=order-db
      - REDIS_HOST=redis
    depends_on:
      - kafka
      - order-db
      - redis
    deploy:
      replicas: 5
      resources:
        limits:
          memory: 2G
  
  # Inventory Service
  inventory-service:
    build: ./inventory-service
    environment:
      - DB_HOST=inventory-db
      - REDIS_HOST=redis
      - ELASTICSEARCH_HOST=elasticsearch
    depends_on:
      - inventory-db
      - redis
      - elasticsearch
    deploy:
      replicas: 3
      resources:
        limits:
          memory: 1.5G
  
  # Payment Service
  payment-service:
    build: ./payment-service
    environment:
      - STRIPE_API_KEY=${STRIPE_API_KEY}
      - PAYPAL_CLIENT_ID=${PAYPAL_CLIENT_ID}
      - DB_HOST=payment-db
    depends_on:
      - payment-db
    deploy:
      replicas: 3
      resources:
        limits:
          memory: 1G
  
  # Shipping Service
  shipping-service:
    build: ./shipping-service
    environment:
      - FedEx_API_KEY=${FEDEX_API_KEY}
      - UPS_API_KEY=${UPS_API_KEY}
    deploy:
      replicas: 2
      resources:
        limits:
          memory: 512M
  
  # Message Infrastructure
  kafka:
    image: confluentinc/cp-kafka:latest
    environment:
      - KAFKA_BROKER_ID=1
      - KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181
      - KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092
      - KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=3
      - KAFKA_LOG_RETENTION_HOURS=24
      - KAFKA_LOG_SEGMENT_BYTES=1048576000
    depends_on:
      - zookeeper
    deploy:
      replicas: 3
      resources:
        limits:
          memory: 4G
  
  # Event Store
  eventstore:
    image: eventstore/eventstore:latest
    environment:
      - EVENTSTORE_CLUSTER_SIZE=3
      - EVENTSTORE_RUN_PROJECTIONS=All
      - EVENTSTORE_ENABLE_EXTERNAL_TCP=true
    ports:
      - "2113:2113"  # Web UI
      - "1113:1113"  # TCP
    volumes:
      - eventstore_data:/var/lib/eventstore
    deploy:
      replicas: 3
  
  # Databases
  order-db:
    image: postgres:13
    environment:
      - POSTGRES_DB=orders
      - POSTGRES_USER=order_user
      - POSTGRES_PASSWORD=${ORDER_DB_PASSWORD}
    volumes:
      - order_db_data:/var/lib/postgresql/data
  
  inventory-db:
    image: postgres:13
    environment:
      - POSTGRES_DB=inventory
      - POSTGRES_USER=inventory_user
      - POSTGRES_PASSWORD=${INVENTORY_DB_PASSWORD}
  
  # Caching & Search
  redis:
    image: redis:6-alpine
    command: redis-server --maxmemory 2gb --maxmemory-policy allkeys-lru
    deploy:
      replicas: 3
    volumes:
      - redis_data:/data
  
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.14.0
    environment:
      - discovery.type=single-node
      - "ES_JAVA_OPTS=-Xms2g -Xmx2g"
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data
  
  # Monitoring Stack
  prometheus:
    image: prom/prometheus:latest
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    ports:
      - "9090:9090"
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=15d'
  
  grafana:
    image: grafana/grafana:latest
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
    ports:
      - "3000:3000"
    volumes:
      - grafana_data:/var/lib/grafana
  
  # Distributed Tracing
  jaeger:
    image: jaegertracing/all-in-one:latest
    ports:
      - "16686:16686"  # Jaeger UI
      - "14268:14268"  # HTTP collector
    environment:
      - COLLECTOR_ZIPKIN_HTTP_PORT=9411
  
  # Load Balancer
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
      - ./ssl:/etc/nginx/ssl
    depends_on:
      - api-gateway

volumes:
  order_db_data:
  inventory_db_data:
  payment_db_data:
  redis_data:
  elasticsearch_data:
  eventstore_data:
  prometheus_data:
  grafana_data:
```

### 1.2 ì„œë¹„ìŠ¤ ê°„ í†µì‹  íŒ¨í„´

```mermaid
graph TB
    Client[Client Applications] --> LB[Load Balancer]
    LB --> AG[API Gateway]
    
    AG --> OS[Order Service]
    AG --> IS[Inventory Service]
    AG --> PS[Payment Service]
    AG --> SS[Shipping Service]
    
    OS --> K[Kafka]
    IS --> K
    PS --> K
    SS --> K
    
    OS --> ES[Event Store]
    IS --> ES
    PS --> ES
    SS --> ES
    
    OS --> R[Redis Cache]
    IS --> R
    PS --> R
    
    OS --> ODB[(Order DB)]
    IS --> IDB[(Inventory DB)]
    PS --> PDB[(Payment DB)]
    
    IS --> ELK[Elasticsearch]
    
    subgraph "Monitoring"
        P[Prometheus]
        G[Grafana]
        J[Jaeger]
    end
    
    OS --> P
    IS --> P
    PS --> P
    SS --> P
```

## 2. Production ì¥ì•  ëŒ€ì‘ ì‚¬ë¡€

### 2.1 2023ë…„ ë¸”ë™í”„ë¼ì´ë°ì´ ì‚¬ê±´

ì‹¤ì œ ì¥ì•  ìƒí™©ê³¼ í•´ê²° ê³¼ì •ì„ ì½”ë“œë¡œ ë³´ì—¬ë“œë¦¬ê² ìŠµë‹ˆë‹¤:

```python
# ì‹¤ì œ ì¥ì•  ìƒí™©ê³¼ í•´ê²°
class ProductionIncidentHandler:
    """
    2023ë…„ ë¸”ë™í”„ë¼ì´ë°ì´ ì‹¤ì œ ì¥ì•  ëŒ€ì‘ ì½”ë“œ
    - íŠ¸ë˜í”½: í‰ì†Œ ëŒ€ë¹„ 50ë°° (ì´ˆë‹¹ 10ë§Œ ìš”ì²­)
    - ì¥ì• : Payment ì„œë¹„ìŠ¤ ì‘ë‹µ ì§€ì—°ìœ¼ë¡œ ì „ì²´ ì‹œìŠ¤í…œ ë§ˆë¹„
    """
    
    def __init__(self):
        # 1. Circuit Breaker ì¦‰ì‹œ ì ìš©
        self.payment_circuit = CircuitBreaker(
            failure_threshold=10,  # 10ë²ˆ ì‹¤íŒ¨ì‹œ open
            recovery_timeout=30,    # 30ì´ˆ í›„ ì¬ì‹œë„
            expected_exception=PaymentTimeoutError
        )
        
        # 2. Bulkheadë¡œ ê²©ë¦¬
        self.payment_semaphore = asyncio.Semaphore(20)  # ë™ì‹œ 20ê°œë§Œ
        
        # 3. Cache ì ê·¹ í™œìš©
        self.cache = Redis(
            host='redis-cluster',
            decode_responses=True,
            socket_keepalive=True,
            socket_keepalive_options={
                1: 1,  # TCP_KEEPIDLE
                2: 1,  # TCP_KEEPINTVL
                3: 3,  # TCP_KEEPCNT
            }
        )
        
        # 4. Rate Limiting
        self.rate_limiter = SlidingWindowRateLimiter(
            max_requests=1000,
            window_seconds=1
        )
        
        # 5. ë©”íŠ¸ë¦­ìŠ¤ ë° ì•Œë¦¼
        self.metrics = MetricsCollector()
        self.alert_manager = AlertManager()
    
    async def handle_order(self, order: Order) -> OrderResult:
        start_time = time.time()
        
        # Step 1: Rate Limiting
        if not await self.rate_limiter.allow(order.customer_id):
            self.metrics.increment('orders.rate_limited')
            # 429 Too Many Requests
            return OrderResult(
                status="RATE_LIMITED",
                message="Please try again later",
                retry_after=self.rate_limiter.get_retry_after()
            )
        
        # Step 2: Cache Hit í™•ì¸
        cache_key = f"order:{order.id}"
        cached = await self.cache.get(cache_key)
        if cached:
            self.metrics.increment('orders.cache_hit')
            return OrderResult.from_json(cached)
        
        # Step 3: Inventory í™•ì¸ (ë¹ ë¥¸ ì‹¤íŒ¨)
        if not await self.check_inventory_fast(order):
            self.metrics.increment('orders.inventory_failed')
            return OrderResult(
                status="OUT_OF_STOCK",
                message="Some items are out of stock"
            )
        
        # Step 4: Payment ì²˜ë¦¬ (Circuit Breaker + Bulkhead)
        try:
            async with self.payment_semaphore:
                payment_result = await self.payment_circuit.call(
                    self.process_payment_with_timeout,
                    order
                )
                
            self.metrics.increment('orders.payment_success')
            
        except CircuitOpenError:
            # Circuitì´ ì—´ë¦¼ - Fallback
            self.metrics.increment('orders.circuit_open')
            return await self.fallback_payment_queue(order)
            
        except asyncio.TimeoutError:
            # Timeout - ë¹„ë™ê¸° íë¡œ ì „í™˜
            self.metrics.increment('orders.payment_timeout')
            return await self.async_payment_queue(order)
            
        except Exception as e:
            self.metrics.increment('orders.payment_error')
            await self.alert_manager.send_critical_alert(
                "Payment processing failed",
                {"order_id": order.id, "error": str(e)}
            )
            raise
        
        # Step 5: ê²°ê³¼ ìºì‹±
        result = OrderResult(
            status="SUCCESS",
            order_id=order.id,
            payment_id=payment_result.id,
            estimated_delivery=self.calculate_delivery_time(order)
        )
        
        # 5ë¶„ ìºì‹œ + ì§€ì—­ë³„ ë¶„ì‚°
        await asyncio.gather(
            self.cache.setex(cache_key, 300, result.to_json()),
            self.cache.setex(f"{cache_key}:us-east", 300, result.to_json()),
            self.cache.setex(f"{cache_key}:eu-west", 300, result.to_json())
        )
        
        # ë©”íŠ¸ë¦­ ê¸°ë¡
        processing_time = time.time() - start_time
        self.metrics.histogram('orders.processing_time', processing_time)
        self.metrics.increment('orders.success')
        
        return result
    
    async def process_payment_with_timeout(self, order: Order):
        # Adaptive timeout - ìµœê·¼ ì‘ë‹µ ì‹œê°„ ê¸°ë°˜
        timeout = self.calculate_adaptive_timeout()
        
        async with async_timeout.timeout(timeout):
            # Primary payment gateway
            try:
                return await self.payment_service.charge_primary(order)
            except PaymentGatewayError:
                # Fallback to secondary gateway
                return await self.payment_service.charge_secondary(order)
    
    def calculate_adaptive_timeout(self) -> float:
        # ìµœê·¼ ì‘ë‹µ ì‹œê°„ ê¸°ë°˜ ë™ì  íƒ€ì„ì•„ì›ƒ
        p99_latency = self.metrics.get_p99_latency("payment")
        current_load = self.metrics.get_current_load()
        
        base_timeout = 2.0  # 2ì´ˆ ê¸°ë³¸
        
        # ì§€ì—° ì‹œê°„ì— ë”°ë¥¸ ì¡°ì •
        if p99_latency < 1.0:
            timeout_multiplier = 1.0  # ì •ìƒ: 2ì´ˆ
        elif p99_latency < 3.0:
            timeout_multiplier = 2.5  # ì•½ê°„ ëŠë¦¼: 5ì´ˆ
        else:
            timeout_multiplier = 5.0   # ë§¤ìš° ëŠë¦¼: 10ì´ˆ
        
        # í˜„ì¬ ë¶€í•˜ì— ë”°ë¥¸ ì¡°ì •
        if current_load > 0.8:
            timeout_multiplier *= 1.5
        
        return base_timeout * timeout_multiplier
    
    async def fallback_payment_queue(self, order: Order) -> OrderResult:
        """
        Payment ì„œë¹„ìŠ¤ ì¥ì• ì‹œ Fallback
        ì£¼ë¬¸ì„ íì— ë„£ê³  ë‚˜ì¤‘ì— ì²˜ë¦¬
        """
        # ë¹„ë™ê¸° ì²˜ë¦¬ë¥¼ ìœ„í•œ Kafka ë©”ì‹œì§€
        message = {
            "order": order.to_dict(),
            "timestamp": datetime.now().isoformat(),
            "retry_count": 0,
            "priority": "HIGH" if order.is_premium_customer() else "NORMAL",
            "correlation_id": str(uuid.uuid4())
        }
        
        await self.kafka_producer.send(
            topic="payment.retry.queue",
            key=order.customer_id,
            value=message,
            headers={
                "content-type": "application/json",
                "origin": "order-service",
                "fallback-reason": "circuit-breaker-open"
            }
        )
        
        # ê³ ê°ì—ê²Œ ì¹œí™”ì ì¸ ë©”ì‹œì§€
        return OrderResult(
            status="PENDING_PAYMENT",
            message="Your order is being processed. You'll receive confirmation within 5 minutes.",
            order_id=order.id,
            tracking_url=f"https://status.example.com/order/{order.id}"
        )
    
    async def check_inventory_fast(self, order: Order) -> bool:
        """
        ë¹ ë¥¸ ì¬ê³  í™•ì¸ (ìºì‹œ ìš°ì„ )
        ëŒ€ê·œëª¨ íŠ¸ë˜í”½ì—ì„œ ì„±ëŠ¥ ì¤‘ì‹¬ ì ‘ê·¼
        """
        # ê° ì•„ì´í…œë§ˆë‹¤ ë™ì‹œ ì²´í¬
        check_tasks = []
        
        for item in order.items:
            task = self.check_single_item_inventory(item)
            check_tasks.append(task)
        
        # ëª¨ë“  ì²´í¬ë¥¼ ë³‘ë ¬ë¡œ ì‹¤í–‰
        results = await asyncio.gather(*check_tasks, return_exceptions=True)
        
        # í•˜ë‚˜ë¼ë„ ì‹¤íŒ¨í•˜ë©´ False
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.error(f"Inventory check failed for item {order.items[i].sku}: {result}")
                return False
            if not result:
                return False
        
        return True
    
    async def check_single_item_inventory(self, item: OrderItem) -> bool:
        try:
            # 1. ë¡œì—´ ìºì‹œ í™•ì¸ (L1 ìºì‹œ)
            cached_stock = await self.cache.get(f"stock:{item.sku}")
            
            if cached_stock is not None:
                available_stock = int(cached_stock)
                if available_stock >= item.quantity:
                    return True
                elif available_stock == 0:
                    return False  # í™•ì‹¤íˆ í’ˆì ˆ
            
            # 2. ë¶„ì‚° ìºì‹œ í™•ì¸ (L2 ìºì‹œ)
            redis_key = f"stock:distributed:{item.sku}"
            distributed_stock = await self.cache.hget(redis_key, "available")
            
            if distributed_stock:
                if int(distributed_stock) >= item.quantity:
                    # ë¯¸ë˜ ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•œ ë¹„ë™ê¸° íƒœìŠ¤í¬
                    asyncio.create_task(
                        self.update_local_cache(item.sku, distributed_stock)
                    )
                    return True
            
            # 3. ìºì‹œ ë¯¸ìŠ¤ - ë°ì´í„°ë² ì´ìŠ¤ ì¡°íšŒ (ìµœì†Œí™”)
            db_stock = await self.inventory_service.get_stock_fast(item.sku)
            
            # ë¹„ë™ê¸° ìºì‹œ ì—…ë°ì´íŠ¸
            asyncio.create_task(
                self.update_all_caches(item.sku, db_stock)
            )
            
            return db_stock >= item.quantity
            
        except Exception as e:
            logger.error(f"Inventory check error for {item.sku}: {e}")
            # ì—ëŸ¬ ì‹œ optimisticí•˜ê²Œ í†µê³¼ (ë¹„ì¦ˆë‹ˆìŠ¤ ì—°ì†ì„±)
            return True
    
    async def monitor_health(self):
        """
        ì‹¤ì‹œê°„ ì‹œìŠ¤í…œ í—¬ìŠ¤ ëª¨ë‹ˆí„°ë§
        ëŒ€ì‹œë³´ë“œì™€ ì•Œë¦¼ì„ ìœ„í•œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
        """
        while True:
            try:
                # í•µì‹¬ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
                metrics = {
                    # Circuit Breaker ìƒíƒœ
                    "circuit.payment.state": self.payment_circuit.state.value,
                    "circuit.payment.failure_count": self.payment_circuit.failure_count,
                    "circuit.payment.success_rate": self.payment_circuit.get_success_rate(),
                    
                    # Bulkhead ìƒíƒœ
                    "bulkhead.payment.available": self.payment_semaphore._value,
                    "bulkhead.payment.total": 20,
                    "bulkhead.payment.utilization": (20 - self.payment_semaphore._value) / 20,
                    
                    # Cache ë©”íŠ¸ë¦­
                    "cache.hit_rate": await self.calculate_cache_hit_rate(),
                    "cache.memory_usage": await self.get_redis_memory_usage(),
                    
                    # Rate Limiting
                    "rate_limiter.rejections": self.rate_limiter.rejection_count,
                    "rate_limiter.current_rate": self.rate_limiter.get_current_rate(),
                    
                    # Queue ìƒíƒœ
                    "queue.payment_retry.depth": await self.get_queue_depth("payment.retry.queue"),
                    "queue.payment_retry.lag": await self.get_consumer_lag("payment.retry.queue"),
                    
                    # ë¹„ì¦ˆë‹ˆìŠ¤ ë©”íŠ¸ë¦­
                    "orders.total_today": await self.get_orders_count_today(),
                    "revenue.total_today": await self.get_revenue_today(),
                    "conversion_rate.current": await self.calculate_conversion_rate()
                }
                
                # Prometheusì— ë©”íŠ¸ë¦­ ì „ì†¡
                for key, value in metrics.items():
                    if isinstance(value, (int, float)):
                        prometheus_client.Gauge(key.replace('.', '_')).set(value)
                
                # ì„ê³„ì¹˜ ì´ˆê³¼ ì‹œ ì•Œë¦¼
                await self.check_alert_conditions(metrics)
                
                # Health check
                health_score = self.calculate_system_health(metrics)
                prometheus_client.Gauge('system_health_score').set(health_score)
                
                if health_score < 0.7:  # 70% ë¯¸ë§Œ
                    await self.alert_manager.send_warning_alert(
                        f"System health degraded: {health_score:.2f}",
                        metrics
                    )
                
            except Exception as e:
                logger.error(f"Health monitoring error: {e}")
            
            await asyncio.sleep(10)  # 10ì´ˆë§ˆë‹¤ ì²´í¬
    
    async def check_alert_conditions(self, metrics):
        """
        ì•Œë¦¼ ì¡°ê±´ ì²´í¬ ë¹„ë™ê¸° ë°©ì‹
        """
        alert_tasks = []
        
        # Circuit Breaker ì•Œë¦¼
        if metrics["circuit.payment.failure_count"] > 50:
            alert_tasks.append(
                self.alert_manager.send_critical_alert(
                    "Payment service experiencing high failure rate",
                    {"failure_count": metrics["circuit.payment.failure_count"]}
                )
            )
        
        # Queue ê¹Šì´ ì•Œë¦¼
        if metrics["queue.payment_retry.depth"] > 10000:
            alert_tasks.append(
                self.alert_manager.send_warning_alert(
                    "Payment retry queue depth is high",
                    {"queue_depth": metrics["queue.payment_retry.depth"]}
                )
            )
        
        # ì„±ëŠ¥ ì €í•˜ ì•Œë¦¼
        if metrics["bulkhead.payment.utilization"] > 0.9:
            alert_tasks.append(
                self.alert_manager.send_warning_alert(
                    "Payment bulkhead utilization is high",
                    {"utilization": metrics["bulkhead.payment.utilization"]}
                )
            )
        
        # ëª¨ë“  ì•Œë¦¼ ë¹„ë™ê¸° ì „ì†¡
        if alert_tasks:
            await asyncio.gather(*alert_tasks, return_exceptions=True)
    
    def calculate_system_health(self, metrics) -> float:
        """
        ì‹œìŠ¤í…œ ì „ì²´ ê±´ê°•ë„ ê³„ì‚°
        0.0 (ë§¤ìš° ë‚˜ì¨) ~ 1.0 (ìš°ìˆ˜)
        """
        weights = {
            "circuit_health": 0.3,
            "bulkhead_health": 0.2,
            "cache_health": 0.2,
            "queue_health": 0.2,
            "business_health": 0.1
        }
        
        # Circuit Breaker ê±´ê°•ë„
        circuit_health = (
            1.0 if metrics["circuit.payment.state"] == "closed" 
            else 0.5 if metrics["circuit.payment.state"] == "half_open" 
            else 0.0
        )
        
        # Bulkhead ê±´ê°•ë„
        bulkhead_health = 1.0 - metrics["bulkhead.payment.utilization"]
        
        # Cache ê±´ê°•ë„
        cache_health = min(1.0, metrics["cache.hit_rate"])
        
        # Queue ê±´ê°•ë„ (ê¹Šì´ì™€ ë™ ê¸°ë°˜)
        queue_depth_score = max(0.0, 1.0 - metrics["queue.payment_retry.depth"] / 10000)
        queue_lag_score = max(0.0, 1.0 - metrics["queue.payment_retry.lag"] / 60000)  # 1ë¶„
        queue_health = (queue_depth_score + queue_lag_score) / 2
        
        # ë¹„ì¦ˆë‹ˆìŠ¤ ê±´ê°•ë„ (ë§¤ì¶œ, ì „í™˜ìœ¨)
        business_health = min(1.0, metrics["conversion_rate.current"] / 0.05)  # 5% ê¸°ì¤€
        
        # ê°€ì¤‘í•© ê³„ì‚°
        total_health = (
            weights["circuit_health"] * circuit_health +
            weights["bulkhead_health"] * bulkhead_health +
            weights["cache_health"] * cache_health +
            weights["queue_health"] * queue_health +
            weights["business_health"] * business_health
        )
        
        return max(0.0, min(1.0, total_health))
```

## 3. ì„±ëŠ¥ ìµœì í™” ë° ë¹„ìš© ì ˆê°

### 3.1 ë¹„ìš© íš¨ìœ¨ì ì¸ ì¸í”„ë¼ ìš´ì˜

```python
# ë¦¬ì†ŒìŠ¤ ìµœì í™” ë§¤ë‹ˆì €
class ResourceOptimizationManager:
    def __init__(self):
        self.k8s_client = kubernetes.client.ApiClient()
        self.metrics_client = MetricsClient()
        self.cost_analyzer = CostAnalyzer()
    
    async def optimize_resources(self):
        """
        ì‹¤ì‹œê°„ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ê¸°ë°˜ ë™ì  ìŠ¤ì¼€ì¼ë§
        """
        while True:
            try:
                # ê° ì„œë¹„ìŠ¤ë³„ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
                services = [
                    "order-service",
                    "inventory-service", 
                    "payment-service",
                    "shipping-service"
                ]
                
                optimization_tasks = []
                for service in services:
                    task = self.optimize_single_service(service)
                    optimization_tasks.append(task)
                
                await asyncio.gather(*optimization_tasks)
                
            except Exception as e:
                logger.error(f"Resource optimization error: {e}")
            
            await asyncio.sleep(300)  # 5ë¶„ë§ˆë‹¤ ìµœì í™”
    
    async def optimize_single_service(self, service_name: str):
        # í˜„ì¬ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
        metrics = await self.metrics_client.get_service_metrics(service_name)
        
        current_replicas = metrics['replicas']
        cpu_usage = metrics['cpu_usage_percent']
        memory_usage = metrics['memory_usage_percent']
        request_rate = metrics['requests_per_second']
        response_time = metrics['avg_response_time_ms']
        
        # ìŠ¤ì¼€ì¼ë§ ê²°ì • ë¡œì§
        target_replicas = self.calculate_target_replicas(
            current_replicas=current_replicas,
            cpu_usage=cpu_usage,
            memory_usage=memory_usage,
            request_rate=request_rate,
            response_time=response_time
        )
        
        if target_replicas != current_replicas:
            logger.info(
                f"Scaling {service_name} from {current_replicas} to {target_replicas} replicas"
            )
            
            await self.scale_service(service_name, target_replicas)
            
            # ë¹„ìš© ì ˆê° ê³„ì‚°
            cost_impact = await self.cost_analyzer.calculate_scaling_cost(
                service_name, current_replicas, target_replicas
            )
            
            logger.info(f"Cost impact: ${cost_impact:.2f}/hour")
    
    def calculate_target_replicas(
        self, 
        current_replicas: int,
        cpu_usage: float,
        memory_usage: float,
        request_rate: float,
        response_time: float
    ) -> int:
        """
        ë‹¤ì¤‘ ì§€í‘œ ê¸°ë°˜ ìŠ¤ì¼€ì¼ë§ ê²°ì •
        """
        # CPU ê¸°ë°˜ ìŠ¤ì¼€ì¼ë§
        if cpu_usage > 70:
            cpu_scale_factor = 1.5
        elif cpu_usage > 50:
            cpu_scale_factor = 1.2
        elif cpu_usage < 20:
            cpu_scale_factor = 0.7
        else:
            cpu_scale_factor = 1.0
        
        # ë©”ëª¨ë¦¬ ê¸°ë°˜ ìŠ¤ì¼€ì¼ë§
        if memory_usage > 80:
            memory_scale_factor = 1.5
        elif memory_usage < 30:
            memory_scale_factor = 0.8
        else:
            memory_scale_factor = 1.0
        
        # ì‘ë‹µ ì‹œê°„ ê¸°ë°˜ ìŠ¤ì¼€ì¼ë§
        if response_time > 1000:  # 1ì´ˆ ì´ìƒ
            response_scale_factor = 1.3
        elif response_time > 500:  # 500ms ì´ìƒ
            response_scale_factor = 1.1
        elif response_time < 100:  # 100ms ë¯¸ë§Œ
            response_scale_factor = 0.9
        else:
            response_scale_factor = 1.0
        
        # ìµœì¢… ìŠ¤ì¼€ì¼ íŒ©í„° (ê°€ì¥ í° ê°’ ì„ íƒ)
        scale_factor = max(cpu_scale_factor, memory_scale_factor, response_scale_factor)
        
        target_replicas = max(1, min(10, int(current_replicas * scale_factor)))
        
        return target_replicas
    
    async def scale_service(self, service_name: str, target_replicas: int):
        # Kubernetes HPA ì—…ë°ì´íŠ¸
        try:
            apps_v1 = kubernetes.client.AppsV1Api(self.k8s_client)
            
            # Deployment ìŠ¤ì¼€ì¼ë§
            deployment = await apps_v1.read_namespaced_deployment(
                name=service_name,
                namespace="production"
            )
            
            deployment.spec.replicas = target_replicas
            
            await apps_v1.patch_namespaced_deployment(
                name=service_name,
                namespace="production",
                body=deployment
            )
            
            # ìŠ¤ì¼€ì¼ë§ ì´ë²¤íŠ¸ ë¡œê·¸
            await self.log_scaling_event(
                service_name, target_replicas, "Auto-scaling based on metrics"
            )
            
        except Exception as e:
            logger.error(f"Failed to scale {service_name}: {e}")
            raise
```

### 3.2 ì§€ëŠ¥ì ì¸ ìºì‹œ ì „ëµ

```python
# ë‹¤ì¤‘ ê³„ì¸µ ìºì‹œ ì „ëµ
class IntelligentCacheManager:
    def __init__(self):
        # L1: ë¡œì—´ ìºì‹œ (Redis)
        self.local_cache = Redis(host='local-redis')
        
        # L2: ë¶„ì‚° ìºì‹œ (Redis Cluster)
        self.distributed_cache = RedisCluster(startup_nodes=[
            {'host': 'redis-cluster-1', 'port': 6379},
            {'host': 'redis-cluster-2', 'port': 6379},
            {'host': 'redis-cluster-3', 'port': 6379}
        ])
        
        # L3: CDN ìºì‹œ (CloudFront)
        self.cdn_cache = CloudFrontClient()
        
        self.cache_analytics = CacheAnalytics()
    
    async def get(self, key: str, fetch_func=None):
        """
        ë‹¤ì¤‘ ê³„ì¸µ ìºì‹œ ì¡°íšŒ
        L1 -> L2 -> L3 -> Database ìˆœì„œ
        """
        start_time = time.time()
        
        try:
            # L1 ìºì‹œ ì²´í¬
            value = await self.local_cache.get(key)
            if value:
                self.cache_analytics.record_hit('L1', key, time.time() - start_time)
                return self.deserialize(value)
            
            # L2 ìºì‹œ ì²´í¬
            value = await self.distributed_cache.get(key)
            if value:
                # L1ì— ë¹„ë™ê¸°ë¡œ ë³µì‚¬
                asyncio.create_task(self.promote_to_l1(key, value))
                
                self.cache_analytics.record_hit('L2', key, time.time() - start_time)
                return self.deserialize(value)
            
            # L3 CDN ì²´í¬ (ì •ì  ì—ì…‹ë§Œ)
            if self.is_static_content(key):
                value = await self.cdn_cache.get(key)
                if value:
                    # í•˜ìœ„ ìºì‹œì— ë¦¬ë²„ìŠ¤ í”„ë¡œëª¨ì…˜
                    await asyncio.gather(
                        self.promote_to_l2(key, value),
                        self.promote_to_l1(key, value)
                    )
                    
                    self.cache_analytics.record_hit('L3', key, time.time() - start_time)
                    return self.deserialize(value)
            
            # ëª¨ë“  ìºì‹œ ë¯¸ìŠ¤ - Database ì¡°íšŒ
            if fetch_func:
                value = await fetch_func()
                
                # ë™ì‹œ ë‹¤ì¤‘ ìºì‹œ ì €ì¥
                await self.store_multilevel(key, value)
                
                self.cache_analytics.record_miss(key, time.time() - start_time)
                return value
            
            return None
            
        except Exception as e:
            logger.error(f"Cache get error for {key}: {e}")
            
            # ìºì‹œ ì‹¤íŒ¨ ì‹œ fallbackë¡œ ë°ì´í„°ë² ì´ìŠ¤ ì¡°íšŒ
            if fetch_func:
                return await fetch_func()
            
            return None
    
    async def store_multilevel(self, key: str, value, ttl: int = 3600):
        """
        ë‹¤ì¤‘ ê³„ì¸µì— ë™ì‹œ ì €ì¥
        TTLì€ ê³„ì¸µë³„ë¡œ ë‹¤ë¥´ê²Œ ì„¤ì •
        """
        serialized = self.serialize(value)
        
        # ë¹„ë™ê¸° ë™ì‹œ ì €ì¥
        await asyncio.gather(
            self.local_cache.setex(key, ttl, serialized),
            self.distributed_cache.setex(key, ttl * 2, serialized),  # L2ëŠ” ë” ì˜¤ë˜
            return_exceptions=True
        )
        
        # CDNì—ëŠ” ì •ì  ì»¨í…ì¸ ë§Œ
        if self.is_static_content(key):
            asyncio.create_task(
                self.cdn_cache.put(key, serialized, ttl * 24)  # CDNì€ 24ì‹œê°„
            )
    
    async def intelligent_prefetch(self):
        """
        ì‚¬ìš© íŒ¨í„´ ê¸°ë°˜ ì§€ëŠ¥ì ì¸ í”„ë¦¬í˜ì¹˜
        """
        while True:
            try:
                # ì–´ì œ ê°™ì€ ì‹œê°„ì˜ ì¸ê¸° ì•„ì´í…œ ë¶„ì„
                popular_items = await self.cache_analytics.get_popular_items(
                    time_window=timedelta(days=1),
                    top_n=1000
                )
                
                # ì˜ˆìƒ íŠ¸ë˜í”½ ê¸°ë°˜ í”„ë¦¬í˜ì¹˜
                prefetch_tasks = []
                for item in popular_items:
                    # ìºì‹œ ë§Œë£Œ ì˜ˆìƒ ì‹œê°„ ê³„ì‚°
                    expires_in = await self.get_ttl(item['key'])
                    
                    if expires_in < 300:  # 5ë¶„ ì´ë‚´ ë§Œë£Œ
                        task = self.prefetch_item(item['key'], item['fetch_func'])
                        prefetch_tasks.append(task)
                
                # ì—¬ëŸ¬ ì•„ì´í…œ ë™ì‹œ í”„ë¦¬í˜ì¹˜
                if prefetch_tasks:
                    await asyncio.gather(*prefetch_tasks, return_exceptions=True)
                    logger.info(f"Prefetched {len(prefetch_tasks)} items")
                
            except Exception as e:
                logger.error(f"Prefetch error: {e}")
            
            await asyncio.sleep(60)  # 1ë¶„ë§ˆë‹¤ ì²´í¬
    
    async def optimize_cache_size(self):
        """
        ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê¸°ë°˜ ìºì‹œ í¬ê¸° ì—°ë‹‰
        """
        memory_usage = await self.get_memory_usage()
        
        if memory_usage > 0.8:  # 80% ì´ìƒ ì‚¬ìš©
            # LRU ê¸°ë°˜ ì‚­ì œ
            await self.evict_lru_items(count=1000)
            
        elif memory_usage < 0.5:  # 50% ë¯¸ë§Œ ì‚¬ìš©
            # ë” ë§ì€ ë°ì´í„° ìºì‹œ
            await self.extend_cache_ttl(multiplier=1.5)
```

## í•µì‹¬ ìš”ì 

### 1. ì „ì²´ ì‹œìŠ¤í…œ ì„¤ê³„ ì›ì¹™

- **ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ì•„í‚¤í…ì²˜**: ì„œë¹„ìŠ¤ë³„ ë…ë¦½ì  ìŠ¤ì¼€ì¼ë§
- **ì´ë²¤íŠ¸ ì£¼ë„ ì•„í‚¤í…ì²˜**: ë¹„ë™ê¸° ì´ë²¤íŠ¸ ì²˜ë¦¬
- **CQRS íŒ¨í„´**: ì½ê¸°/ì“°ê¸° ë¶„ë¦¬ë¡œ ì„±ëŠ¥ ìµœì í™”

### 2. ì‹¤ì œ ì¥ì•  ëŒ€ì‘ ê²½í—˜

- **ë‹¤ë‹¨ê³„ Fallback**: Circuit Breaker + Queue + Cache
- **ì ì‘ì  íƒ€ì„ì•„ì›ƒ**: ì‹¤ì‹œê°„ ì„±ëŠ¥ ì§€í‘œ ë°˜ì˜
- **ë¹„ì¦ˆë‹ˆìŠ¤ ì—°ì†ì„±**: ê¸°ìˆ ì  ì¥ì• ê°€ ë¹„ì¦ˆë‹ˆìŠ¤ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ ìµœì†Œí™”

### 3. ì§€ëŠ¥ì ì¸ ìš´ì˜ ìë™í™”

- **ë™ì  ìŠ¤ì¼€ì¼ë§**: ë¦¬ì–¼íƒ€ì„ ë©”íŠ¸ë¦­ ê¸°ë°˜ ìë™ ì¡°ì •
- **ë‹¤ì¤‘ ê³„ì¸µ ìºì‹œ**: L1/L2/L3 ìºì‹œ ì „ëµìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ
- **ì˜ˆì¸¡ì  ëª¨ë‹ˆí„°ë§**: ë¬¸ì œ ë°œìƒ ì „ ì˜ˆë°©ì  ëŒ€ì‘

---

**ì´ì „**: [10-06-03-resilience-patterns.md](./10-06-03-resilience-patterns.md)
**ë‹¤ìŒ**: [Chapter 9 ê³ ê¸‰ ë©”ëª¨ë¦¬ ê´€ë¦¬](../chapter-09-advanced-memory-management/)ì—ì„œ ì„±ëŠ¥ ìµœì í™”ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤.

## ğŸ“š ê´€ë ¨ ë¬¸ì„œ

### ğŸ“– í˜„ì¬ ë¬¸ì„œ ì •ë³´

- **ë‚œì´ë„**: ADVANCED
- **ì£¼ì œ**: ì¸í”„ë¼ìŠ¤íŠ¸ëŸ­ì²˜
- **ì˜ˆìƒ ì‹œê°„**: 8-12ì‹œê°„

### ğŸ¯ í•™ìŠµ ê²½ë¡œ

- [ğŸ“š ADVANCED ë ˆë²¨ ì „ì²´ ë³´ê¸°](../learning-paths/advanced/)
- [ğŸ  ë©”ì¸ í•™ìŠµ ê²½ë¡œ](../learning-paths/)
- [ğŸ“‹ ì „ì²´ ê°€ì´ë“œ ëª©ë¡](../README.md)

### ğŸ“‚ ê°™ì€ ì±•í„° (chapter-10-async-programming)

- [8.1 Promise/Future íŒ¨í„´ ê°œìš”](./10-02-01-promise-future.md)
- [8.1a Promise/Future ê¸°ë³¸ ê°œë…ê³¼ êµ¬í˜„](./10-01-01-promise-future-basics.md)
- [8.1b ë¹„ë™ê¸° ì—°ì‚° ì¡°í•©ê³¼ ë³‘ë ¬ ì²˜ë¦¬](./10-02-02-async-composition.md)
- [8.1c ì·¨ì†Œì™€ íƒ€ì„ì•„ì›ƒ ì²˜ë¦¬](./10-02-03-cancellation-timeout.md)
- [8.1d ì‹¤í–‰ ëª¨ë¸ê³¼ ìŠ¤ì¼€ì¤„ë§](./10-02-04-execution-scheduling.md)

### ğŸ·ï¸ ê´€ë ¨ í‚¤ì›Œë“œ

`production-case-study`, `distributed-systems`, `microservices`, `resilience-patterns`, `monitoring`

### â­ï¸ ë‹¤ìŒ ë‹¨ê³„ ê°€ì´ë“œ

- ì‹œìŠ¤í…œ ì „ì²´ì˜ ê´€ì ì—ì„œ ì´í•´í•˜ë ¤ ë…¸ë ¥í•˜ì„¸ìš”
- ë‹¤ë¥¸ ê³ ê¸‰ ì£¼ì œë“¤ê³¼ì˜ ì—°ê´€ì„±ì„ íŒŒì•…í•´ë³´ì„¸ìš”
