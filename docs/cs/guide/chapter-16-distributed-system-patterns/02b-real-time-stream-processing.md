---
tags:
  - advanced
  - apache-flink
  - cep
  - deep-study
  - event-driven
  - hands-on
  - real-time-analytics
  - stream-processing
  - ì¸í”„ë¼ìŠ¤íŠ¸ëŸ­ì²˜
difficulty: ADVANCED
learning_time: "40-60ì‹œê°„"
main_topic: "ì¸í”„ë¼ìŠ¤íŠ¸ëŸ­ì²˜"
priority_score: 4
---

# 16.2B ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ - Apache Flinkì™€ ë³µí•© ì´ë²¤íŠ¸ ì²˜ë¦¬

## âš¡ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬

### Apache Flinkë¡œ ë³µí•© ì´ë²¤íŠ¸ ì²˜ë¦¬

```scala
// Scalaë¡œ êµ¬í˜„í•œ Flink ì‹¤ì‹œê°„ ë¶„ì„ íŒŒì´í”„ë¼ì¸
import org.apache.flink.streaming.api.scala._
import org.apache.flink.streaming.api.windowing.time.Time
import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer
import org.apache.flink.cep.scala.CEP
import org.apache.flink.cep.scala.pattern.Pattern

case class UserEvent(
  userId: String,
  eventType: String,
  contentId: String,
  timestamp: Long,
  sessionId: String,
  device: String
)

case class ContentTrendingSignal(
  contentId: String,
  trendingScore: Double,
  timestamp: Long,
  reason: String
)

case class UserEngagementScore(
  userId: String,
  engagementScore: Double,
  sessionDuration: Long,
  contentCount: Int,
  timestamp: Long
)

object RealTimeAnalyticsPipeline {
  
  def main(args: Array[String]): Unit = {
    val env = StreamExecutionEnvironment.getExecutionEnvironment
    env.setParallelism(4)
    env.enableCheckpointing(60000) // 1ë¶„ë§ˆë‹¤ ì²´í¬í¬ì¸íŠ¸
    
    // Kafka ì†ŒìŠ¤ ì„¤ì •
    val kafkaProps = new Properties()
    kafkaProps.setProperty("bootstrap.servers", "kafka-cluster:9092")
    kafkaProps.setProperty("group.id", "flink-analytics")
    
    val kafkaConsumer = new FlinkKafkaConsumer[UserEvent](
      "user-events",
      new UserEventDeserializer(),
      kafkaProps
    )
    
    // ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¼ ìƒì„±
    val eventStream = env.addSource(kafkaConsumer)
      .assignTimestampsAndWatermarks(
        WatermarkStrategy
          .forBoundedOutOfOrderness[UserEvent](Duration.ofSeconds(5))
          .withTimestampAssigner((event, _) => event.timestamp)
      )
    
    // 1. ì‹¤ì‹œê°„ ì½˜í…ì¸  íŠ¸ë Œë”© ë¶„ì„
    val trendingAnalysis = eventStream
      .filter(_.eventType == "content.watched")
      .keyBy(_.contentId)
      .window(SlidingEventTimeWindows.of(Time.minutes(15), Time.minutes(1)))
      .aggregate(new ContentTrendingAggregator())
      .filter(_.trendingScore > 0.7) // íŠ¸ë Œë”© ì„ê³„ê°’
    
    // 2. ì‚¬ìš©ì ì°¸ì—¬ë„ ì‹¤ì‹œê°„ ë¶„ì„
    val engagementAnalysis = eventStream
      .keyBy(_.userId)
      .window(SessionWindows.withGap(Time.minutes(30)))
      .aggregate(new UserEngagementAggregator())
    
    // 3. ë³µí•© ì´ë²¤íŠ¸ íŒ¨í„´ íƒì§€ (CEP - Complex Event Processing)
    val bingeWatchingPattern = Pattern
      .begin[UserEvent]("start")
      .where(_.eventType == "content.watched")
      .next("continue")
      .where(_.eventType == "content.watched")
      .times(3) // 3ê°œ ì´ìƒì˜ ì—°ì† ì‹œì²­
      .within(Time.hours(2)) // 2ì‹œê°„ ì´ë‚´
    
    val bingeWatchingDetection = CEP
      .pattern(eventStream.keyBy(_.userId), bingeWatchingPattern)
      .select(new BingeWatchingSelector())
    
    // 4. ì‹¤ì‹œê°„ ì´ìƒ íƒì§€ (ê°‘ì‘ìŠ¤ëŸ¬ìš´ íŠ¸ë˜í”½ ì¦ê°€)
    val anomalyDetection = eventStream
      .windowAll(TumblingEventTimeWindows.of(Time.minutes(1)))
      .aggregate(new TrafficAnomalyDetector())
      .filter(_.isAnomaly)
    
    // ê²°ê³¼ë¥¼ ë‹¤ì‹œ Kafkaë¡œ ì „ì†¡
    trendingAnalysis.addSink(new FlinkKafkaProducer[ContentTrendingSignal](
      "trending-signals",
      new TrendingSignalSerializer(),
      kafkaProps
    ))
    
    engagementAnalysis.addSink(new FlinkKafkaProducer[UserEngagementScore](
      "user-engagement",
      new EngagementScoreSerializer(),
      kafkaProps
    ))
    
    bingeWatchingDetection.addSink(new FlinkKafkaProducer[BingeWatchingAlert](
      "user-behavior-alerts",
      new BingeWatchingAlertSerializer(),
      kafkaProps
    ))
    
    anomalyDetection.addSink(new FlinkKafkaProducer[TrafficAnomalyAlert](
      "system-alerts",
      new TrafficAnomalySerializer(),
      kafkaProps
    ))
    
    env.execute("Real-time Analytics Pipeline")
  }
}

// ì½˜í…ì¸  íŠ¸ë Œë”© ì ìˆ˜ ê³„ì‚° Aggregator
class ContentTrendingAggregator extends AggregateFunction[UserEvent, TrendingAccumulator, ContentTrendingSignal] {
  
  override def createAccumulator(): TrendingAccumulator = 
    TrendingAccumulator(0, 0, 0, Set.empty, 0L)
  
  override def add(event: UserEvent, acc: TrendingAccumulator): TrendingAccumulator = {
    val newUniqueUsers = acc.uniqueUsers + event.userId
    val newWatchCount = acc.watchCount + 1
    val newTotalDuration = acc.totalDuration + extractDuration(event)
    val newDeviceCount = acc.deviceTypes + event.device
    
    acc.copy(
      watchCount = newWatchCount,
      uniqueUserCount = newUniqueUsers.size,
      totalDuration = newTotalDuration,
      uniqueUsers = newUniqueUsers,
      deviceTypes = newDeviceCount
    )
  }
  
  override def getResult(acc: TrendingAccumulator): ContentTrendingSignal = {
    // íŠ¸ë Œë”© ì ìˆ˜ ê³„ì‚° ì•Œê³ ë¦¬ì¦˜
    val userDiversityScore = math.min(acc.uniqueUserCount / 100.0, 1.0) // ìµœëŒ€ 100ëª… ê¸°ì¤€
    val watchIntensityScore = math.min(acc.watchCount / 500.0, 1.0) // ìµœëŒ€ 500íšŒ ê¸°ì¤€  
    val deviceDiversityScore = acc.deviceTypes.size / 4.0 // ìµœëŒ€ 4ê°œ ê¸°ê¸° íƒ€ì…
    val avgWatchTime = if (acc.watchCount > 0) acc.totalDuration / acc.watchCount else 0
    val completionScore = math.min(avgWatchTime / 3600.0, 1.0) // 1ì‹œê°„ ê¸°ì¤€
    
    val trendingScore = (userDiversityScore * 0.3 + 
                        watchIntensityScore * 0.3 + 
                        deviceDiversityScore * 0.2 + 
                        completionScore * 0.2)
    
    ContentTrendingSignal(
      contentId = acc.contentId,
      trendingScore = trendingScore,
      timestamp = System.currentTimeMillis(),
      reason = s"users:${acc.uniqueUserCount}, watches:${acc.watchCount}, devices:${acc.deviceTypes.size}"
    )
  }
  
  override def merge(acc1: TrendingAccumulator, acc2: TrendingAccumulator): TrendingAccumulator = {
    acc1.copy(
      watchCount = acc1.watchCount + acc2.watchCount,
      uniqueUserCount = (acc1.uniqueUsers ++ acc2.uniqueUsers).size,
      totalDuration = acc1.totalDuration + acc2.totalDuration,
      uniqueUsers = acc1.uniqueUsers ++ acc2.uniqueUsers,
      deviceTypes = acc1.deviceTypes ++ acc2.deviceTypes
    )
  }
  
  private def extractDuration(event: UserEvent): Long = {
    // ì´ë²¤íŠ¸ì—ì„œ ì‹œì²­ ì‹œê°„ ì¶”ì¶œ ë¡œì§
    // ì‹¤ì œë¡œëŠ” event.dataì—ì„œ duration í•„ë“œ íŒŒì‹±
    1800L // 30ë¶„ ê¸°ë³¸ê°’
  }
}

case class TrendingAccumulator(
  watchCount: Int,
  uniqueUserCount: Int,
  totalDuration: Long,
  uniqueUsers: Set[String],
  deviceTypes: Set[String]
) {
  def contentId: String = "" // ì‹¤ì œë¡œëŠ” í‚¤ì—ì„œ ê°€ì ¸ì˜´
}

// ì‚¬ìš©ì ì°¸ì—¬ë„ ë¶„ì„ Aggregator  
class UserEngagementAggregator extends AggregateFunction[UserEvent, EngagementAccumulator, UserEngagementScore] {
  
  override def createAccumulator(): EngagementAccumulator = 
    EngagementAccumulator(0, 0, 0L, 0L, Set.empty)
  
  override def add(event: UserEvent, acc: EngagementAccumulator): EngagementAccumulator = {
    val isWatchEvent = event.eventType == "content.watched"
    val isInteractionEvent = Set("content.liked", "content.shared", "content.rated").contains(event.eventType)
    
    acc.copy(
      contentCount = if (isWatchEvent) acc.contentCount + 1 else acc.contentCount,
      interactionCount = if (isInteractionEvent) acc.interactionCount + 1 else acc.interactionCount,
      sessionStart = if (acc.sessionStart == 0) event.timestamp else math.min(acc.sessionStart, event.timestamp),
      sessionEnd = math.max(acc.sessionEnd, event.timestamp),
      contentTypes = acc.contentTypes + extractContentType(event)
    )
  }
  
  override def getResult(acc: EngagementAccumulator): UserEngagementScore = {
    val sessionDuration = acc.sessionEnd - acc.sessionStart
    val contentDiversity = acc.contentTypes.size
    val interactionRate = if (acc.contentCount > 0) acc.interactionCount.toDouble / acc.contentCount else 0.0
    
    // ì°¸ì—¬ë„ ì ìˆ˜ ê³„ì‚°
    val engagementScore = math.min(
      (acc.contentCount * 0.3 + 
       contentDiversity * 0.3 + 
       interactionRate * 0.4) / 3.0 * 100, 
      100.0
    )
    
    UserEngagementScore(
      userId = "", // í‚¤ì—ì„œ ì¶”ì¶œ
      engagementScore = engagementScore,
      sessionDuration = sessionDuration,
      contentCount = acc.contentCount,
      timestamp = acc.sessionEnd
    )
  }
  
  override def merge(acc1: EngagementAccumulator, acc2: EngagementAccumulator): EngagementAccumulator = {
    acc1.copy(
      contentCount = acc1.contentCount + acc2.contentCount,
      interactionCount = acc1.interactionCount + acc2.interactionCount,
      sessionStart = if (acc1.sessionStart == 0) acc2.sessionStart else math.min(acc1.sessionStart, acc2.sessionStart),
      sessionEnd = math.max(acc1.sessionEnd, acc2.sessionEnd),
      contentTypes = acc1.contentTypes ++ acc2.contentTypes
    )
  }
  
  private def extractContentType(event: UserEvent): String = {
    // ì‹¤ì œë¡œëŠ” contentIdë¡œ ì½˜í…ì¸  íƒ€ì… ì¡°íšŒ
    "drama" // ê¸°ë³¸ê°’
  }
}

case class EngagementAccumulator(
  contentCount: Int,
  interactionCount: Int,
  sessionStart: Long,
  sessionEnd: Long,
  contentTypes: Set[String]
)
```

### ë³µí•© ì´ë²¤íŠ¸ ì²˜ë¦¬ (CEP) ê³ ê¸‰ íŒ¨í„´

```scala
// ë”ìš± ë³µì¡í•œ ì‚¬ìš©ì í–‰ë™ íŒ¨í„´ íƒì§€
object AdvancedCEPPatterns {
  
  // 1. ì‚¬ìš©ì ì´íƒˆ ìœ„í—˜ íŒ¨í„´ íƒì§€
  val churnRiskPattern = Pattern
    .begin[UserEvent]("login")
    .where(_.eventType == "user.login")
    .followedBy("watch")
    .where(_.eventType == "content.watched")
    .times(1, 3) // 1-3ê°œì˜ ì‹œì²­ ì´ë²¤íŠ¸
    .followedBy("exit")
    .where(_.eventType == "user.logout")
    .within(Time.minutes(15)) // 15ë¶„ ì´ë‚´ì— ë¹ ë¥¸ ì´íƒˆ
  
  // 2. í”„ë¦¬ë¯¸ì—„ ì „í™˜ ê°€ëŠ¥ì„± ë†’ì€ ì‚¬ìš©ì íŒ¨í„´
  val premiumConversionPattern = Pattern
    .begin[UserEvent]("start")
    .where(_.eventType == "content.watched")
    .next("highEngagement")
    .where(event => 
      Set("content.liked", "content.shared", "content.rated").contains(event.eventType)
    )
    .timesOrMore(3) // 3ë²ˆ ì´ìƒì˜ ìƒí˜¸ì‘ìš©
    .followedBy("qualityUpgrade")
    .where(event => 
      event.eventType == "content.watched" && 
      extractQuality(event) == "4K" // 4K í’ˆì§ˆë¡œ ì‹œì²­ ì‹œë„
    )
    .within(Time.hours(24))
  
  // 3. ì½˜í…ì¸  ë°”ì´ëŸ´ í™•ì‚° íŒ¨í„´
  val viralContentPattern = Pattern
    .begin[UserEvent]("watch")
    .where(_.eventType == "content.watched")
    .followedBy("share")
    .where(_.eventType == "content.shared")
    .timesOrMore(5) // 5íšŒ ì´ìƒ ê³µìœ 
    .within(Time.hours(2)) // 2ì‹œê°„ ì´ë‚´
  
  // 4. ì‚¬ìš©ì ì·¨í–¥ ë³€í™” íŒ¨í„´ íƒì§€
  val tasteChangePattern = Pattern
    .begin[UserEvent]("oldPreference")
    .where(event => extractGenre(event) == "comedy")
    .times(5) // ê¸°ì¡´ ì„ í˜¸ ì¥ë¥´ 5íšŒ ì‹œì²­
    .followedBy("newPreference")
    .where(event => extractGenre(event) == "thriller")
    .timesOrMore(3) // ìƒˆë¡œìš´ ì¥ë¥´ 3íšŒ ì´ìƒ ì‹œì²­
    .within(Time.days(7)) // ì¼ì£¼ì¼ ë‚´
  
  def setupCEPProcessing(eventStream: DataStream[UserEvent]): Unit = {
    // ì´íƒˆ ìœ„í—˜ ì‚¬ìš©ì íƒì§€
    val churnRiskAlerts = CEP
      .pattern(eventStream.keyBy(_.userId), churnRiskPattern)
      .select(new ChurnRiskSelector())
    
    // í”„ë¦¬ë¯¸ì—„ ì „í™˜ í›„ë³´ íƒì§€  
    val conversionCandidates = CEP
      .pattern(eventStream.keyBy(_.userId), premiumConversionPattern)
      .select(new PremiumConversionSelector())
    
    // ë°”ì´ëŸ´ ì½˜í…ì¸  íƒì§€
    val viralContent = CEP
      .pattern(eventStream.keyBy(_.contentId), viralContentPattern)
      .select(new ViralContentSelector())
    
    // ì·¨í–¥ ë³€í™” ì‚¬ìš©ì íƒì§€
    val tasteChangeUsers = CEP
      .pattern(eventStream.keyBy(_.userId), tasteChangePattern)
      .select(new TasteChangeSelector())
    
    // ê° ê²°ê³¼ë¥¼ ì ì ˆí•œ Kafka í† í”½ìœ¼ë¡œ ì „ì†¡
    churnRiskAlerts.addSink(new FlinkKafkaProducer[ChurnRiskAlert](
      "churn-risk-alerts",
      new ChurnRiskAlertSerializer(),
      kafkaProps
    ))
    
    conversionCandidates.addSink(new FlinkKafkaProducer[ConversionCandidate](
      "conversion-candidates",
      new ConversionCandidateSerializer(), 
      kafkaProps
    ))
    
    viralContent.addSink(new FlinkKafkaProducer[ViralContentAlert](
      "viral-content-alerts",
      new ViralContentAlertSerializer(),
      kafkaProps
    ))
    
    tasteChangeUsers.addSink(new FlinkKafkaProducer[TasteChangeAlert](
      "taste-change-alerts",
      new TasteChangeAlertSerializer(),
      kafkaProps
    ))
  }
  
  private def extractQuality(event: UserEvent): String = {
    // ì‹¤ì œë¡œëŠ” event.dataì—ì„œ í’ˆì§ˆ ì •ë³´ ì¶”ì¶œ
    "HD" // ê¸°ë³¸ê°’
  }
  
  private def extractGenre(event: UserEvent): String = {
    // ì‹¤ì œë¡œëŠ” contentIdë¡œ ì¥ë¥´ ì¡°íšŒ
    "unknown" // ê¸°ë³¸ê°’
  }
}

// íŒ¨í„´ ì„ íƒìë“¤
class ChurnRiskSelector extends PatternSelectFunction[UserEvent, ChurnRiskAlert] {
  override def select(pattern: util.Map[String, util.List[UserEvent]]): ChurnRiskAlert = {
    val loginEvents = pattern.get("login").asScala
    val watchEvents = pattern.get("watch").asScala
    val exitEvents = pattern.get("exit").asScala
    
    val sessionDuration = exitEvents.head.timestamp - loginEvents.head.timestamp
    val watchCount = watchEvents.size
    
    ChurnRiskAlert(
      userId = loginEvents.head.userId,
      sessionDuration = sessionDuration,
      watchCount = watchCount,
      riskScore = calculateChurnRisk(sessionDuration, watchCount),
      timestamp = System.currentTimeMillis()
    )
  }
  
  private def calculateChurnRisk(sessionDuration: Long, watchCount: Int): Double = {
    // ì„¸ì…˜ ì‹œê°„ì´ ì§§ê³  ì‹œì²­ íšŸìˆ˜ê°€ ì ì„ìˆ˜ë¡ ì´íƒˆ ìœ„í—˜ ì¦ê°€
    val durationScore = math.max(0, 1.0 - sessionDuration / (15 * 60 * 1000.0)) // 15ë¶„ ê¸°ì¤€
    val watchScore = math.max(0, 1.0 - watchCount / 5.0) // 5íšŒ ê¸°ì¤€
    (durationScore * 0.6 + watchScore * 0.4) * 100
  }
}

class PremiumConversionSelector extends PatternSelectFunction[UserEvent, ConversionCandidate] {
  override def select(pattern: util.Map[String, util.List[UserEvent]]): ConversionCandidate = {
    val watchEvents = pattern.get("start").asScala
    val engagementEvents = pattern.get("highEngagement").asScala
    val qualityUpgradeEvents = pattern.get("qualityUpgrade").asScala
    
    ConversionCandidate(
      userId = watchEvents.head.userId,
      engagementCount = engagementEvents.size,
      qualityUpgradeAttempts = qualityUpgradeEvents.size,
      conversionScore = calculateConversionProbability(engagementEvents.size, qualityUpgradeEvents.size),
      timestamp = System.currentTimeMillis()
    )
  }
  
  private def calculateConversionProbability(engagementCount: Int, upgradeAttempts: Int): Double = {
    val engagementScore = math.min(engagementCount / 10.0, 1.0) // ìµœëŒ€ 10íšŒ ê¸°ì¤€
    val upgradeScore = math.min(upgradeAttempts / 3.0, 1.0) // ìµœëŒ€ 3íšŒ ê¸°ì¤€
    (engagementScore * 0.7 + upgradeScore * 0.3) * 100
  }
}

// ë°ì´í„° í´ë˜ìŠ¤ë“¤
case class ChurnRiskAlert(
  userId: String,
  sessionDuration: Long,
  watchCount: Int,
  riskScore: Double,
  timestamp: Long
)

case class ConversionCandidate(
  userId: String,
  engagementCount: Int,
  qualityUpgradeAttempts: Int,
  conversionScore: Double,
  timestamp: Long
)

case class ViralContentAlert(
  contentId: String,
  shareCount: Int,
  viralScore: Double,
  timestamp: Long
)

case class TasteChangeAlert(
  userId: String,
  oldGenre: String,
  newGenre: String,
  confidence: Double,
  timestamp: Long
)
```

### ì‹¤ì‹œê°„ A/B í…ŒìŠ¤íŠ¸ì™€ ê°œì¸í™”

```scala
// ì‹¤ì‹œê°„ A/B í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„
object RealTimeABTesting {
  
  case class ABTestEvent(
    userId: String,
    experimentId: String,
    variant: String,
    eventType: String,
    timestamp: Long,
    metadata: Map[String, String]
  )
  
  case class ABTestResult(
    experimentId: String,
    variant: String,
    conversionRate: Double,
    sampleSize: Int,
    confidence: Double,
    timestamp: Long
  )
  
  def setupABTestAnalysis(eventStream: DataStream[UserEvent]): DataStream[ABTestResult] = {
    // A/B í…ŒìŠ¤íŠ¸ ì´ë²¤íŠ¸ í•„í„°ë§ ë° ë³€í™˜
    val abTestEvents = eventStream
      .filter(event => extractExperimentId(event).nonEmpty)
      .map(event => ABTestEvent(
        userId = event.userId,
        experimentId = extractExperimentId(event).get,
        variant = extractVariant(event),
        eventType = event.eventType,
        timestamp = event.timestamp,
        metadata = extractMetadata(event)
      ))
    
    // ì‹¤ì‹œê°„ ì „í™˜ìœ¨ ê³„ì‚° (15ë¶„ ìœˆë„ìš°)
    abTestEvents
      .keyBy(event => (event.experimentId, event.variant))
      .window(TumblingEventTimeWindows.of(Time.minutes(15)))
      .aggregate(new ABTestAggregator())
      .filter(_.sampleSize >= 100) // ìµœì†Œ ìƒ˜í”Œ ì‚¬ì´ì¦ˆ í™•ë³´
  }
  
  // A/B í…ŒìŠ¤íŠ¸ í†µê³„ ì§‘ê³„
  class ABTestAggregator extends AggregateFunction[ABTestEvent, ABTestAccumulator, ABTestResult] {
    
    override def createAccumulator(): ABTestAccumulator = 
      ABTestAccumulator("", "", 0, 0, Set.empty)
    
    override def add(event: ABTestEvent, acc: ABTestAccumulator): ABTestAccumulator = {
      val isConversion = isConversionEvent(event.eventType)
      
      acc.copy(
        experimentId = event.experimentId,
        variant = event.variant,
        totalUsers = acc.uniqueUsers.size + (if (acc.uniqueUsers.contains(event.userId)) 0 else 1),
        conversions = acc.conversions + (if (isConversion) 1 else 0),
        uniqueUsers = acc.uniqueUsers + event.userId
      )
    }
    
    override def getResult(acc: ABTestAccumulator): ABTestResult = {
      val conversionRate = if (acc.totalUsers > 0) acc.conversions.toDouble / acc.totalUsers else 0.0
      val confidence = calculateStatisticalConfidence(acc.conversions, acc.totalUsers)
      
      ABTestResult(
        experimentId = acc.experimentId,
        variant = acc.variant,
        conversionRate = conversionRate,
        sampleSize = acc.totalUsers,
        confidence = confidence,
        timestamp = System.currentTimeMillis()
      )
    }
    
    override def merge(acc1: ABTestAccumulator, acc2: ABTestAccumulator): ABTestAccumulator = {
      acc1.copy(
        totalUsers = (acc1.uniqueUsers ++ acc2.uniqueUsers).size,
        conversions = acc1.conversions + acc2.conversions,
        uniqueUsers = acc1.uniqueUsers ++ acc2.uniqueUsers
      )
    }
    
    private def isConversionEvent(eventType: String): Boolean = {
      Set("subscription.purchased", "content.completed", "user.upgraded").contains(eventType)
    }
    
    private def calculateStatisticalConfidence(conversions: Int, totalUsers: Int): Double = {
      // ê°„ë‹¨í•œ í†µê³„ì  ìœ ì˜ì„± ê³„ì‚° (ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ í†µê³„ ê²€ì • í•„ìš”)
      if (totalUsers < 30) 0.0
      else {
        val p = conversions.toDouble / totalUsers
        val se = math.sqrt(p * (1 - p) / totalUsers)
        val z = p / se
        math.min(math.abs(z) / 1.96 * 100, 99.9) // 95% ì‹ ë¢°êµ¬ê°„ ê¸°ì¤€
      }
    }
  }
  
  case class ABTestAccumulator(
    experimentId: String,
    variant: String,
    totalUsers: Int,
    conversions: Int,
    uniqueUsers: Set[String]
  )
  
  private def extractExperimentId(event: UserEvent): Option[String] = {
    // ì‹¤ì œë¡œëŠ” event.metadataì—ì„œ ì‹¤í—˜ ID ì¶”ì¶œ
    Some("exp_001") // ì˜ˆì‹œ
  }
  
  private def extractVariant(event: UserEvent): String = {
    // ì‹¤ì œë¡œëŠ” event.metadataì—ì„œ ë³€í˜• ì¶”ì¶œ
    "control" // ì˜ˆì‹œ
  }
  
  private def extractMetadata(event: UserEvent): Map[String, String] = {
    // ì‹¤ì œë¡œëŠ” eventì—ì„œ ì¶”ê°€ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
    Map.empty // ì˜ˆì‹œ
  }
}
```

## í•µì‹¬ ìš”ì 

### 1. Apache Flinkì˜ ê°•ë ¥í•œ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬

Scala ê¸°ë°˜ì˜ Flink íŒŒì´í”„ë¼ì¸ì„ í†µí•´ ì‹¤ì‹œê°„ ì½˜í…ì¸  íŠ¸ë Œë”© ë¶„ì„, ì‚¬ìš©ì ì°¸ì—¬ë„ ì¸¡ì •, ì´ìƒ íƒì§€ê¹Œì§€ í¬ê´„ì ì¸ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ë¥¼ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤.

### 2. ë³µí•© ì´ë²¤íŠ¸ ì²˜ë¦¬ (CEP)

ì‚¬ìš©ì í–‰ë™ íŒ¨í„´ íƒì§€ë¶€í„° ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸ ë„ì¶œê¹Œì§€, CEPë¥¼ í™œìš©í•œ ê³ ê¸‰ íŒ¨í„´ ë§¤ì¹­ ê¸°ë²•ì„ í•™ìŠµí–ˆìŠµë‹ˆë‹¤.

### 3. ì‹¤ì‹œê°„ A/B í…ŒìŠ¤íŠ¸ ë¶„ì„

ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ í™˜ê²½ì—ì„œ ì‹¤ì‹œê°„ìœ¼ë¡œ A/B í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ê³  í†µê³„ì  ìœ ì˜ì„±ì„ ê²€ì¦í•˜ëŠ” ë°©ë²•ì„ ìµí˜”ìŠµë‹ˆë‹¤.

---

**ì´ì „**: [16.2A Event-Driven ì•„í‚¤í…ì²˜ ê¸°ì´ˆ](chapter-16-distributed-system-patterns/16-04-event-driven-fundamentals.md)  
**ë‹¤ìŒ**: [16.2C ì´ë²¤íŠ¸ ì†Œì‹± êµ¬í˜„](chapter-16-distributed-system-patterns/02c-event-sourcing-implementation.md)ì—ì„œ ì´ë²¤íŠ¸ ìŠ¤í† ì–´ì™€ ì§‘í•©ì²´ íŒ¨í„´ì„ í•™ìŠµí•©ë‹ˆë‹¤.

## ğŸ“š ê´€ë ¨ ë¬¸ì„œ

### ğŸ“– í˜„ì¬ ë¬¸ì„œ ì •ë³´

- **ë‚œì´ë„**: ADVANCED
- **ì£¼ì œ**: ì¸í”„ë¼ìŠ¤íŠ¸ëŸ­ì²˜
- **ì˜ˆìƒ ì‹œê°„**: 40-60ì‹œê°„

### ğŸ¯ í•™ìŠµ ê²½ë¡œ

- [ğŸ“š ADVANCED ë ˆë²¨ ì „ì²´ ë³´ê¸°](../learning-paths/advanced/)
- [ğŸ  ë©”ì¸ í•™ìŠµ ê²½ë¡œ](../learning-paths/)
- [ğŸ“‹ ì „ì²´ ê°€ì´ë“œ ëª©ë¡](../README.md)

### ğŸ“‚ ê°™ì€ ì±•í„° (chapter-16-system-design-patterns)

- [15.1 ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ì•„í‚¤í…ì²˜ ê°œìš”](../chapter-15-microservices-architecture/16-01-microservices-architecture.md)
- [15.1A ëª¨ë†€ë¦¬ìŠ¤ ë¬¸ì œì ê³¼ ì „í™˜ ì „ëµ](../chapter-15-microservices-architecture/16-10-monolith-to-microservices.md)
- [16.1B ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ì„¤ê³„ ì›ì¹™ê³¼ íŒ¨í„´ ê°œìš”](./16-11-design-principles.md)
- [16.1B1 ë‹¨ì¼ ì±…ì„ ì›ì¹™ (Single Responsibility Principle)](./16-12-1-single-responsibility-principle.md)
- [16.1B1 ë‹¨ì¼ ì±…ì„ ì›ì¹™ (Single Responsibility Principle)](./16-13-1-single-responsibility.md)

### ğŸ·ï¸ ê´€ë ¨ í‚¤ì›Œë“œ

`apache-flink`, `cep`, `stream-processing`, `real-time-analytics`, `event-driven`

### â­ï¸ ë‹¤ìŒ ë‹¨ê³„ ê°€ì´ë“œ

- ì‹œìŠ¤í…œ ì „ì²´ì˜ ê´€ì ì—ì„œ ì´í•´í•˜ë ¤ ë…¸ë ¥í•˜ì„¸ìš”
- ë‹¤ë¥¸ ê³ ê¸‰ ì£¼ì œë“¤ê³¼ì˜ ì—°ê´€ì„±ì„ íŒŒì•…í•´ë³´ì„¸ìš”
