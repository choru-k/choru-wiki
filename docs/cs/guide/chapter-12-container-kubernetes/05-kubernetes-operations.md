---
tags:
  - Kubernetes
  - Operations
  - Production
  - Troubleshooting
  - Performance
  - Security
---

# 12.5 Kubernetes ìš´ì˜ ë° íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

## 2022ë…„ 2ì›”, ìƒˆë²½ 3ì‹œì˜ í´ëŸ¬ìŠ¤í„° ì¬ë‚œ

2022ë…„ 2ì›” 14ì¼, ë°œë Œíƒ€ì¸ë°ì´ ìƒˆë²½ 3ì‹œ. ëª¨ë‹ˆí„°ë§ ì•ŒëŒì´ ë¯¸ì¹œ ë“¯ì´ ìš¸ë ¸ë‹¤.

"ğŸš¨ Kubernetes cluster is down! All services unavailable!"

ë°˜ë…„ ë„˜ê²Œ ì•ˆì •ì ìœ¼ë¡œ ìš´ì˜ë˜ë˜ í”„ë¡œë•ì…˜ í´ëŸ¬ìŠ¤í„°ê°€ ì™„ì „íˆ ë§ˆë¹„ëë‹¤.

- ëª¨ë“  Podê°€ Pending ìƒíƒœ
- etcd í´ëŸ¬ìŠ¤í„° Split-brain ë°œìƒ
- ë§ˆìŠ¤í„° ë…¸ë“œ 3ëŒ€ ì¤‘ 2ëŒ€ê°€ ì‘ë‹µ ì—†ìŒ
- ì›Œì»¤ ë…¸ë“œë“¤ì€ ì‚´ì•„ìˆì§€ë§Œ ìƒˆë¡œìš´ Pod ìŠ¤ì¼€ì¤„ë§ ë¶ˆê°€

**ê·¸ë‚  ìš°ë¦¬ê°€ ë°°ìš´ êµí›ˆ**: ì¿ ë²„ë„¤í‹°ìŠ¤ëŠ” ê°•ë ¥í•˜ì§€ë§Œ **ë³µì¡í•œ ë¶„ì‚° ì‹œìŠ¤í…œ**ì´ë‹¤. ìš´ì˜í•˜ë ¤ë©´ ë‹¨ìˆœíˆ YAML íŒŒì¼ë§Œ ì•Œì•„ì„œëŠ” ì•ˆ ëœë‹¤.

## í´ëŸ¬ìŠ¤í„° ì•„í‚¤í…ì²˜ ì„¤ê³„ ì›ì¹™

### 1. ê³ ê°€ìš©ì„± (High Availability) ì„¤ê³„

```bash
# etcd í´ëŸ¬ìŠ¤í„° ì„¤ì • (í™€ìˆ˜ ê°œ, ìµœì†Œ 3ê°œ)
ETCD_CLUSTER="etcd1=https://10.0.1.10:2380,etcd2=https://10.0.1.11:2380,etcd3=https://10.0.1.12:2380"

# API Server ë¡œë“œë°¸ëŸ°ì„œ ë’¤ì— ë°°ì¹˜
# HAProxy ì„¤ì • ì˜ˆì‹œ
backend k8s-api
    balance roundrobin
    option httpchk GET /healthz
    server master1 10.0.1.10:6443 check
    server master2 10.0.1.11:6443 check  
    server master3 10.0.1.12:6443 check
```

```yaml
# kube-apiserver.yaml (static pod)
apiVersion: v1
kind: Pod
metadata:
  name: kube-apiserver
  namespace: kube-system
spec:
  containers:
  - name: kube-apiserver
    image: k8s.gcr.io/kube-apiserver:v1.25.0
    command:
    - kube-apiserver
    - --advertise-address=10.0.1.10
    - --etcd-servers=https://10.0.1.10:2379,https://10.0.1.11:2379,https://10.0.1.12:2379
    - --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
    - --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
    - --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
    - --audit-log-path=/var/log/audit.log
    - --audit-log-maxage=30
    - --audit-log-maxbackup=10
    - --audit-log-maxsize=100
    - --enable-admission-plugins=NodeRestriction,ResourceQuota,PodSecurity
    - --anonymous-auth=false
    - --request-timeout=300s
    - --max-requests-inflight=400
    - --max-mutating-requests-inflight=200
    livenessProbe:
      httpGet:
        path: /healthz
        port: 6443
        scheme: HTTPS
      initialDelaySeconds: 15
      periodSeconds: 10
      timeoutSeconds: 15
    readinessProbe:
      httpGet:
        path: /readyz
        port: 6443
        scheme: HTTPS
      periodSeconds: 1
      timeoutSeconds: 15
```

### 2. ë…¸ë“œ ë¶„ì‚° ì „ëµ

```yaml
# node-affinity.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: critical-app
spec:
  replicas: 6
  selector:
    matchLabels:
      app: critical-app
  template:
    metadata:
      labels:
        app: critical-app
    spec:
      affinity:
        # ë…¸ë“œ ë¶„ì‚°
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - critical-app
            topologyKey: kubernetes.io/hostname
        # ê°€ìš© ì˜ì—­ ë¶„ì‚°
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - critical-app
              topologyKey: topology.kubernetes.io/zone
      containers:
      - name: app
        image: critical-app:v1.0
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
```

## í´ëŸ¬ìŠ¤í„° ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼

### Prometheus ê¸°ë°˜ ì¢…í•© ëª¨ë‹ˆí„°ë§

```yaml
# cluster-monitoring.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: kubernetes-cluster-alerts
spec:
  groups:
  - name: kubernetes-cluster
    rules:
    # etcd í´ëŸ¬ìŠ¤í„° ìƒíƒœ
    - alert: EtcdClusterDown
      expr: up{job="etcd"} == 0
      for: 1m
      labels:
        severity: critical
      annotations:
        summary: "etcd cluster member is down"
        description: "etcd cluster member {{ $labels.instance }} has been down for more than 1 minute"

    # API Server ì‘ë‹µì„±
    - alert: KubernetesAPIServerDown
      expr: up{job="apiserver"} == 0
      for: 1m
      labels:
        severity: critical
      annotations:
        summary: "Kubernetes API server is down"

    - alert: KubernetesAPIServerLatency
      expr: histogram_quantile(0.99, sum(rate(apiserver_request_duration_seconds_bucket{subresource!="log",verb!~"^(?:CONNECT|WATCH)$"}[5m])) without (instance, pod)) > 1
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "Kubernetes API server high latency"

    # ë…¸ë“œ ìƒíƒœ
    - alert: KubernetesNodeNotReady
      expr: kube_node_status_condition{condition="Ready",status="true"} == 0
      for: 10m
      labels:
        severity: critical
      annotations:
        summary: "Node {{ $labels.node }} is not ready"

    - alert: KubernetesNodeDiskPressure
      expr: kube_node_status_condition{condition="DiskPressure",status="true"} == 1
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Node {{ $labels.node }} has disk pressure"

    # Pod ìƒíƒœ
    - alert: KubernetesPodCrashLooping
      expr: rate(kube_pod_container_status_restarts_total[15m]) * 60 * 15 > 0
      for: 1m
      labels:
        severity: warning
      annotations:
        summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is crash looping"

    - alert: KubernetesPodNotScheduled
      expr: kube_pod_status_phase{phase="Pending"} == 1
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} has been pending for more than 10 minutes"

    # ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰
    - alert: KubernetesNodeCpuUsageHigh
      expr: (100 - (avg by (instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)) > 80
      for: 15m
      labels:
        severity: warning
      annotations:
        summary: "Node {{ $labels.instance }} CPU usage is high"

    - alert: KubernetesNodeMemoryUsageHigh
      expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 85
      for: 15m
      labels:
        severity: warning
      annotations:
        summary: "Node {{ $labels.instance }} memory usage is high"
```

### í´ëŸ¬ìŠ¤í„° ëŒ€ì‹œë³´ë“œ

```json
{
  "dashboard": {
    "title": "Kubernetes Cluster Overview",
    "panels": [
      {
        "title": "Cluster Health",
        "type": "stat",
        "targets": [
          {
            "expr": "up{job=\"apiserver\"}",
            "legendFormat": "API Server"
          },
          {
            "expr": "up{job=\"etcd\"}",
            "legendFormat": "etcd"
          }
        ]
      },
      {
        "title": "Node Status",
        "type": "table",
        "targets": [
          {
            "expr": "kube_node_info",
            "format": "table"
          }
        ]
      },
      {
        "title": "Resource Usage",
        "type": "graph",
        "targets": [
          {
            "expr": "sum(kube_pod_container_resource_requests_cpu_cores)",
            "legendFormat": "CPU Requests"
          },
          {
            "expr": "sum(kube_node_status_allocatable_cpu_cores)",
            "legendFormat": "CPU Allocatable"
          }
        ]
      }
    ]
  }
}
```

## ì¥ì•  ëŒ€ì‘ ë° íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ì¼ë°˜ì ì¸ ë¬¸ì œ í•´ê²° í”Œë ˆì´ë¶

#### 1. Podê°€ Pending ìƒíƒœì¼ ë•Œ

```bash
# 1ë‹¨ê³„: Pod ìƒíƒœ ìƒì„¸ í™•ì¸
$ kubectl describe pod <pod-name> -n <namespace>

# ì£¼ìš” í™•ì¸ì‚¬í•­:
# - Events ì„¹ì…˜ì˜ ì—ëŸ¬ ë©”ì‹œì§€
# - Node-Selectors, Tolerations
# - Resource Requests/Limits

# 2ë‹¨ê³„: ë…¸ë“œ ë¦¬ì†ŒìŠ¤ í™•ì¸
$ kubectl top nodes
$ kubectl describe nodes

# 3ë‹¨ê³„: ìŠ¤ì¼€ì¤„ëŸ¬ ë¡œê·¸ í™•ì¸
$ kubectl logs -n kube-system -l component=kube-scheduler

# 4ë‹¨ê³„: ìˆ˜ë™ ìŠ¤ì¼€ì¤„ë§ ì‹œë„
$ kubectl patch pod <pod-name> -p '{"spec":{"nodeName":"<target-node>"}}'
```

#### 2. ì„œë¹„ìŠ¤ ì ‘ê·¼ ë¶ˆê°€

```bash
# DNS í•´ìƒë„ í…ŒìŠ¤íŠ¸
$ kubectl run -it --rm debug --image=busybox --restart=Never -- sh
> nslookup <service-name>.<namespace>.svc.cluster.local
> wget -qO- <service-name>.<namespace>.svc.cluster.local:<port>

# ì„œë¹„ìŠ¤ ì—”ë“œí¬ì¸íŠ¸ í™•ì¸
$ kubectl get endpoints <service-name> -n <namespace>

# kube-proxy ë¡œê·¸ í™•ì¸
$ kubectl logs -n kube-system -l k8s-app=kube-proxy

# iptables ê·œì¹™ í™•ì¸ (ë…¸ë“œì—ì„œ)
$ sudo iptables -t nat -L | grep <service-name>
```

#### 3. etcd í´ëŸ¬ìŠ¤í„° ë³µêµ¬

```bash
# etcd ë©¤ë²„ ìƒíƒœ í™•ì¸
$ etcdctl --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  member list

# ì†ìƒëœ ë©¤ë²„ ì œê±°
$ etcdctl member remove <member-id>

# ìƒˆ ë©¤ë²„ ì¶”ê°€
$ etcdctl member add <new-member-name> --peer-urls=https://<new-ip>:2380

# ë°±ì—…ì—ì„œ ë³µêµ¬ (ìµœí›„ ìˆ˜ë‹¨)
$ etcdctl snapshot restore snapshot.db \
  --name <member-name> \
  --initial-cluster <cluster-definition> \
  --initial-advertise-peer-urls=https://<member-ip>:2380
```

### ì‹¤ì „ íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ë„êµ¬

```bash
#!/bin/bash
# k8s-debug-toolkit.sh

set -e

RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

function print_section() {
    echo -e "${GREEN}=== $1 ===${NC}"
}

function print_warning() {
    echo -e "${YELLOW}âš ï¸ $1${NC}"
}

function print_error() {
    echo -e "${RED}âŒ $1${NC}"
}

function check_cluster_health() {
    print_section "Cluster Health Check"
    
    # API Server ì‘ë‹µ í™•ì¸
    if kubectl version --short > /dev/null 2>&1; then
        echo "âœ… API Server responding"
    else
        print_error "API Server not responding"
        return 1
    fi
    
    # ë…¸ë“œ ìƒíƒœ í™•ì¸
    echo "Node Status:"
    kubectl get nodes -o custom-columns="NAME:.metadata.name,STATUS:.status.conditions[-1].type,READY:.status.conditions[-1].status"
    
    # ì¤‘ìš” ì‹œìŠ¤í…œ Pod í™•ì¸
    echo ""
    echo "System Pods:"
    kubectl get pods -n kube-system -o wide | grep -E "(api-server|etcd|scheduler|controller)"
}

function check_resource_usage() {
    print_section "Resource Usage"
    
    # ë…¸ë“œ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ë¥ 
    echo "Node Resource Usage:"
    kubectl top nodes 2>/dev/null || echo "Metrics server not available"
    
    # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ë³„ Pod ìˆ˜
    echo ""
    echo "Pods per Namespace:"
    kubectl get pods --all-namespaces --no-headers | awk '{print $1}' | sort | uniq -c | sort -nr
    
    # ë¦¬ì†ŒìŠ¤ ìš”ì²­ì´ í° Podë“¤
    echo ""
    echo "Top CPU Requesting Pods:"
    kubectl top pods --all-namespaces --sort-by=cpu 2>/dev/null | head -10 || echo "Metrics server not available"
}

function check_network() {
    print_section "Network Check"
    
    # CoreDNS ìƒíƒœ
    echo "CoreDNS Status:"
    kubectl get pods -n kube-system -l k8s-app=kube-dns -o wide
    
    # kube-proxy ìƒíƒœ  
    echo ""
    echo "Kube-proxy Status:"
    kubectl get pods -n kube-system -l k8s-app=kube-proxy -o wide | head -5
    
    # ì„œë¹„ìŠ¤ ì—†ëŠ” ì—”ë“œí¬ì¸íŠ¸ ì°¾ê¸°
    echo ""
    echo "Services without endpoints:"
    for ns in $(kubectl get ns -o name | cut -d/ -f2); do
        kubectl get endpoints -n $ns -o json | jq -r '.items[] | select(.subsets | length == 0) | "\(.metadata.namespace)/\(.metadata.name)"' 2>/dev/null
    done
}

function check_storage() {
    print_section "Storage Check"
    
    # PV ìƒíƒœ
    echo "Persistent Volumes:"
    kubectl get pv -o custom-columns="NAME:.metadata.name,STATUS:.status.phase,CLAIM:.spec.claimRef.name,STORAGECLASS:.spec.storageClassName"
    
    # Pending PVC
    echo ""
    echo "Pending PVCs:"
    kubectl get pvc --all-namespaces -o json | jq -r '.items[] | select(.status.phase == "Pending") | "\(.metadata.namespace)/\(.metadata.name)"' 2>/dev/null
}

function check_events() {
    print_section "Recent Events"
    
    # ìµœê·¼ Warning ì´ë²¤íŠ¸
    kubectl get events --all-namespaces --sort-by=.metadata.creationTimestamp | grep Warning | tail -10
}

function generate_report() {
    local report_file="/tmp/k8s-health-report-$(date +%Y%m%d-%H%M%S).txt"
    
    print_section "Generating Health Report"
    
    {
        echo "Kubernetes Cluster Health Report"
        echo "Generated: $(date)"
        echo "Cluster: $(kubectl config current-context)"
        echo ""
        
        check_cluster_health
        echo ""
        check_resource_usage  
        echo ""
        check_network
        echo ""
        check_storage
        echo ""
        check_events
        
    } > "$report_file"
    
    echo "Report saved to: $report_file"
}

# ë©”ì¸ ì‹¤í–‰
case "${1:-all}" in
    "health")
        check_cluster_health
        ;;
    "resources")
        check_resource_usage
        ;;
    "network")
        check_network
        ;;
    "storage")
        check_storage
        ;;
    "events")
        check_events
        ;;
    "report")
        generate_report
        ;;
    "all"|*)
        check_cluster_health
        check_resource_usage
        check_network
        check_storage
        check_events
        ;;
esac
```

## ì„±ëŠ¥ ìµœì í™”

### 1. etcd ìµœì í™”

```bash
# etcd ì„±ëŠ¥ ì„¤ì •
ETCD_QUOTA_BACKEND_BYTES=8589934592  # 8GB
ETCD_AUTO_COMPACTION_MODE=periodic
ETCD_AUTO_COMPACTION_RETENTION=5m
ETCD_SNAPSHOT_COUNT=10000
ETCD_HEARTBEAT_INTERVAL=100
ETCD_ELECTION_TIMEOUT=1000

# etcd ì••ì¶• ìˆ˜í–‰
$ etcdctl --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  compact $(etcdctl --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  endpoint status --write-out="json" | jq -r '.[0].Status.header.revision')

# etcd ì¡°ê° ëª¨ìŒ
$ etcdctl --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  defrag
```

### 2. kubelet ìµœì í™”

```yaml
# kubelet-config.yaml
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
maxPods: 250
podsPerCore: 10
registryPullQPS: 5
registryBurst: 10
eventRecordQPS: 0
eventBurst: 10
kubeAPIQPS: 20
kubeAPIBurst: 30
serializeImagePulls: false
hairpinMode: hairpin-veth
cgroupDriver: systemd
runtimeRequestTimeout: 10m
volumeStatsAggPeriod: 1m
systemReserved:
  cpu: 200m
  memory: 512Mi
  ephemeral-storage: 10Gi
kubeReserved:
  cpu: 200m
  memory: 512Mi
  ephemeral-storage: 10Gi
enforceNodeAllocatable:
- pods
- system-reserved
- kube-reserved
```

### 3. ë„¤íŠ¸ì›Œí¬ ìµœì í™”

```yaml
# calico-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: calico-config
  namespace: kube-system
data:
  typha_service_name: "calico-typha"
  calico_backend: "bird"
  cluster_type: "k8s,bgp"
  calico_autodetection_method: "can-reach=8.8.8.8"
  # ì„±ëŠ¥ ìµœì í™” ì„¤ì •
  felix_iptablesrefreshinterval: "60"
  felix_iptableslocktimeoutsecs: "10"
  felix_iptableslockattempts: "3"
  felix_prometheusmetricsEnabled: "true"
  felix_chaininsertmode: "insert"
  felix_defaultendpointtohostaction: "ACCEPT"
```

## ë³´ì•ˆ ê°•í™”

### 1. Pod Security Standards

```yaml
# pod-security-standards.yaml
# ì£¼ì˜: PodSecurityPolicyëŠ” Kubernetes 1.25ë¶€í„° ì œê±°ë¨
# Pod Security Standardsë¥¼ ì‚¬ìš©í•´ì•¼ í•¨
apiVersion: v1
kind: Namespace
metadata:
  name: production
  labels:
    pod-security.kubernetes.io/enforce: restricted
    pod-security.kubernetes.io/enforce-version: v1.28
    pod-security.kubernetes.io/audit: restricted
    pod-security.kubernetes.io/audit-version: v1.28
    pod-security.kubernetes.io/warn: restricted
    pod-security.kubernetes.io/warn-version: v1.28

---
# ë³´ì•ˆ ì •ì±… ì¤€ìˆ˜ Pod ì˜ˆì œ
apiVersion: v1
kind: Pod
metadata:
  name: secure-app
  namespace: production
spec:
  securityContext:
    runAsNonRoot: true
    runAsUser: 1000
    fsGroup: 2000
    seccompProfile:
      type: RuntimeDefault
  containers:
  - name: app
    image: myapp:latest
    securityContext:
      allowPrivilegeEscalation: false
      readOnlyRootFilesystem: true
      runAsNonRoot: true
      capabilities:
        drop:
        - ALL
```

### 2. Network Policies

```yaml
# network-policy.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-all
  namespace: production
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress

---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-frontend-to-backend
  namespace: production
spec:
  podSelector:
    matchLabels:
      app: backend
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: frontend
    ports:
    - protocol: TCP
      port: 8080

---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-egress-dns
  namespace: production
spec:
  podSelector: {}
  policyTypes:
  - Egress
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          name: kube-system
      podSelector:
        matchLabels:
          k8s-app: kube-dns
    ports:
    - protocol: UDP
      port: 53
```

### 3. ê°ì‚¬ ë¡œê¹… (Audit Logging)

```yaml
# audit-policy.yaml
apiVersion: audit.k8s.io/v1
kind: Policy
rules:
# ì¤‘ìš”í•œ ë¦¬ì†ŒìŠ¤ ë³€ê²½ ì‚¬í•­ ëª¨ë‘ ê¸°ë¡
- level: RequestResponse
  namespaces: ["kube-system", "kube-public", "production"]
  resources:
  - group: ""
    resources: ["secrets", "configmaps", "serviceaccounts"]
  - group: "apps"
    resources: ["deployments", "daemonsets", "statefulsets"]

# ì¸ì¦/ê¶Œí•œ ê´€ë ¨ ì´ë²¤íŠ¸
- level: Request
  users: ["system:anonymous"]
  namespaces: ["kube-system", "production"]

# exec, portforward ë“± ìœ„í—˜í•œ ë™ì‘
- level: RequestResponse
  resources:
  - group: ""
    resources: ["pods/exec", "pods/portforward", "pods/proxy"]

# ë‚˜ë¨¸ì§€ëŠ” ë©”íƒ€ë°ì´í„°ë§Œ
- level: Metadata
  omitStages:
  - RequestReceived
```

## ë°±ì—… ë° ì¬í•´ ë³µêµ¬

### etcd ë°±ì—… ìë™í™”

```bash
#!/bin/bash
# etcd-backup.sh

set -euo pipefail

BACKUP_DIR="/var/backups/etcd"
RETENTION_DAYS=30
TIMESTAMP=$(date +%Y%m%d-%H%M%S)

# ë°±ì—… ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p "$BACKUP_DIR"

# etcd ìŠ¤ëƒ…ìƒ· ìƒì„±
ETCDCTL_API=3 etcdctl snapshot save "$BACKUP_DIR/etcd-backup-$TIMESTAMP.db" \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key

# ë°±ì—… ê²€ì¦
ETCDCTL_API=3 etcdctl snapshot status "$BACKUP_DIR/etcd-backup-$TIMESTAMP.db" -w table

# ì˜¤ë˜ëœ ë°±ì—… ì‚­ì œ
find "$BACKUP_DIR" -name "etcd-backup-*.db" -mtime +$RETENTION_DAYS -delete

# í´ë¼ìš°ë“œ ìŠ¤í† ë¦¬ì§€ë¡œ ì—…ë¡œë“œ (ì„ íƒì‚¬í•­)
if command -v aws &> /dev/null; then
    aws s3 cp "$BACKUP_DIR/etcd-backup-$TIMESTAMP.db" s3://k8s-backups/etcd/
fi

echo "Backup completed: $BACKUP_DIR/etcd-backup-$TIMESTAMP.db"
```

### Veleroë¥¼ ì´ìš©í•œ í´ëŸ¬ìŠ¤í„° ë°±ì—…

```yaml
# velero-backup.yaml
apiVersion: velero.io/v1
kind: Backup
metadata:
  name: production-backup
spec:
  includedNamespaces:
  - production
  - monitoring
  excludedResources:
  - events
  - events.events.k8s.io
  storageLocation: aws-s3
  volumeSnapshotLocations:
  - aws-ebs
  ttl: 720h  # 30ì¼
  
---
# ì •ê¸° ë°±ì—… ìŠ¤ì¼€ì¤„
apiVersion: velero.io/v1
kind: Schedule
metadata:
  name: daily-backup
spec:
  schedule: "0 2 * * *"  # ë§¤ì¼ ìƒˆë²½ 2ì‹œ
  template:
    includedNamespaces:
    - production
    ttl: 168h  # 7ì¼
```

## ë¹„ìš© ìµœì í™”

### ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ë¶„ì„

```bash
#!/bin/bash
# resource-usage-report.sh

echo "=== Namespace Resource Usage ==="
kubectl top pods --all-namespaces --no-headers | \
  awk '{cpu[$1]+=$2; mem[$1]+=$3} END {for (ns in cpu) printf "%-20s CPU: %s Memory: %s, ", ns, cpu[ns], mem[ns]}' | \
  sort -k3 -nr

echo ""
echo "=== Unused PVs ==="
kubectl get pv -o json | \
  jq -r '.items[] | select(.status.phase == "Available") | "\(.metadata.name) - \(.spec.capacity.storage) - \(.spec.storageClassName)"'

echo ""
echo "=== Over-provisioned Pods (CPU > 80% unused) ==="
kubectl top pods --all-namespaces --no-headers | \
  awk '$3 ~ /m$/ && $3+0 > 100 {print $1"/"$2" - CPU: "$3" Memory: "$4}'

echo ""
echo "=== Pods without resource limits ==="
kubectl get pods --all-namespaces -o json | \
  jq -r '.items[] | select(.spec.containers[].resources.limits == null) | "\(.metadata.namespace)/\(.metadata.name)"'
```

### Cluster Autoscaler ì„¤ì •

```yaml
# cluster-autoscaler.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cluster-autoscaler
  namespace: kube-system
spec:
  selector:
    matchLabels:
      app: cluster-autoscaler
  template:
    metadata:
      labels:
        app: cluster-autoscaler
    spec:
      serviceAccountName: cluster-autoscaler
      containers:
      - image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.21.0
        name: cluster-autoscaler
        command:
        - ./cluster-autoscaler
        - --v=4
        - --stderrthreshold=info
        - --cloud-provider=aws
        - --skip-nodes-with-local-storage=false
        - --expander=least-waste
        - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/production-cluster
        - --balance-similar-node-groups
        - --scale-down-delay-after-add=10m
        - --scale-down-unneeded-time=10m
        - --scale-down-utilization-threshold=0.5
        resources:
          limits:
            cpu: 100m
            memory: 300Mi
          requests:
            cpu: 100m
            memory: 300Mi
```

## ë ˆìŠ¨ ëŸ°

### 1. ì¥ì• ëŠ” ë°˜ë“œì‹œ ë°œìƒí•œë‹¤

**ì™„ë²½í•œ ì‹œìŠ¤í…œì€ ì—†ë‹¤.** ì¥ì• ë¥¼ ì˜ˆë°©í•˜ëŠ” ê²ƒë³´ë‹¤ ë¹ ë¥´ê²Œ ê°ì§€í•˜ê³  ë³µêµ¬í•˜ëŠ” ëŠ¥ë ¥ì´ ë” ì¤‘ìš”í•˜ë‹¤.

### 2. ê´€ì°° ê°€ëŠ¥ì„±ì´ ìƒëª…ì„ ì´ë‹¤

**ë©”íŠ¸ë¦­, ë¡œê·¸, ì¶”ì ì´ ì—†ìœ¼ë©´ ì¥ì•  ìƒí™©ì—ì„œ ëˆˆê°ê³  ìˆ˜ìˆ í•˜ëŠ” ê²ƒ**ê³¼ ê°™ë‹¤. ì²˜ìŒë¶€í„° ì¶©ë¶„í•œ ê´€ì°° ê°€ëŠ¥ì„±ì„ êµ¬ì¶•í•˜ì.

### 3. ìë™í™”ê°€ ì•ˆì •ì„±ì˜ í•µì‹¬ì´ë‹¤

ìˆ˜ë™ ì‘ì—…ì€ **ì‹¤ìˆ˜ì˜ ì›ì¸**ì´ë‹¤. ë°±ì—…, ë°°í¬, ìŠ¤ì¼€ì¼ë§, ë³µêµ¬ ëª¨ë“  ê²ƒì„ ìë™í™”í•˜ì.

### 4. ë³´ì•ˆì€ ì„ íƒì‚¬í•­ì´ ì•„ë‹ˆë‹¤

Network Policy, RBAC, Pod Security PolicyëŠ” **ê¸°ë³¸**ì´ë‹¤. ë³´ì•ˆì„ ë‚˜ì¤‘ì— ì¶”ê°€í•˜ë ¤ë©´ ê³ í†µìŠ¤ëŸ½ë‹¤.

### 5. ë¹„ìš© ê´€ë¦¬ë„ ìš´ì˜ì˜ ì¼ë¶€ë‹¤

ë¦¬ì†ŒìŠ¤ë¥¼ **ê³¼ë„í•˜ê²Œ í• ë‹¹í•˜ë©´ ë¹„ìš©ì´ í­ì¦**í•œë‹¤. ì •ê¸°ì ì¸ ì‚¬ìš©ëŸ‰ ë¶„ì„ê³¼ ìµœì í™”ê°€ í•„ìš”í•˜ë‹¤.

---

**Chapter 12: Container & Kubernetes ì™„ë£Œ!** ì´ì œ ìš°ë¦¬ëŠ” ì»¨í…Œì´ë„ˆì˜ ë‚´ë¶€ êµ¬ì¡°ë¶€í„° ëŒ€ê·œëª¨ ì¿ ë²„ë„¤í‹°ìŠ¤ í´ëŸ¬ìŠ¤í„° ìš´ì˜ê¹Œì§€ **ì™„ì „í•œ ì»¨í…Œì´ë„ˆ ì—¬ì •**ì„ ë§ˆìŠ¤í„°í–ˆìŠµë‹ˆë‹¤.

## ê´€ë ¨ ë¬¸ì„œ

- [12.4 Kubernetes ê³ ê¸‰ ê¸°ëŠ¥](04-kubernetes-advanced.md) - Helm, Operator, Service Mesh ë“± ê³ ê¸‰ íŒ¨í„´ë“¤
- [Chapter 11: Performance Optimization](../chapter-11-performance-optimization/index.md) - Kubernetes í´ëŸ¬ìŠ¤í„° ì„±ëŠ¥ ìµœì í™”
- [Chapter 13: Observability & Debugging](../chapter-13-observability-debugging/index.md) - í”„ë¡œë•ì…˜ ëª¨ë‹ˆí„°ë§ê³¼ ì¥ì•  ëŒ€ì‘
- [Chapter 14: Distributed Systems](../chapter-14-distributed-systems/index.md) - ë¶„ì‚° ì‹œìŠ¤í…œ ì„¤ê³„ì™€ ìš´ì˜
- [Chapter 15: Security Engineering](../chapter-15-security-engineering/index.md) - í´ëŸ¬ìŠ¤í„° ë³´ì•ˆ ê°•í™”

ë‹¤ìŒìœ¼ë¡œëŠ” **Chapter 11 (Performance Optimization)** ë˜ëŠ” **Chapter 10 (Syscall & Kernel)**ì„ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
