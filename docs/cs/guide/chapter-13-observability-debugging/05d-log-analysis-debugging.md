---
tags:
  - Debugging
  - Log Analysis
  - Automation
  - Pattern Recognition
  - Anomaly Detection
---

# 13.5d ë¡œê·¸ ë¶„ì„ê³¼ ìë™ ë””ë²„ê¹…

## ë¡œê·¸ ê¸°ë°˜ ë””ë²„ê¹…ì˜ í˜

ìˆ˜ë§Œ ì¤„ì˜ ë¡œê·¸ë¥¼ ì‚¬ëŒì´ ì½ì„ ìˆ˜ëŠ” ì—†ë‹¤. **íŒ¨í„´ ì¸ì‹**ê³¼ **ì´ìƒ ì§•í›„ ê°ì§€**ë¥¼ ìë™í™”í•˜ì—¬ íš¨ê³¼ì ì¸ ë¡œê·¸ ë¶„ì„ì„ í†µí•œ ë””ë²„ê¹… ê¸°ë²•ì„ ì‚´í´ë³´ì.

```python
import re
import json
from collections import defaultdict, Counter
from typing import Dict, List, Pattern
import matplotlib.pyplot as plt
from datetime import datetime, timedelta

class LogAnalyzer:
    def __init__(self):
        self.log_entries: List[Dict] = []
        self.patterns: Dict[str, Pattern] = {
            'error': re.compile(r'ERROR|FATAL|Exception|Error'),
            'warning': re.compile(r'WARN|WARNING'),
            'slow_query': re.compile(r'slow.*query|query.*took.*(\d+)ms'),
            'timeout': re.compile(r'timeout|timed.*out'),
            'memory_issue': re.compile(r'OutOfMemory|MemoryError|memory.*exceeded'),
            'connection_issue': re.compile(r'connection.*refused|connection.*lost|connection.*timeout')
        }
    
    def parse_log_file(self, log_file_path: str):
        """ë¡œê·¸ íŒŒì¼ íŒŒì‹±"""
        with open(log_file_path, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    # JSON í˜•íƒœì˜ êµ¬ì¡°í™”ëœ ë¡œê·¸ íŒŒì‹±
                    if line.strip().startswith('{'):
                        entry = json.loads(line.strip())
                    else:
                        # ì¼ë°˜ í…ìŠ¤íŠ¸ ë¡œê·¸ íŒŒì‹±
                        entry = self._parse_text_log(line)
                    
                    self.log_entries.append(entry)
                except Exception as e:
                    print(f"ë¡œê·¸ íŒŒì‹± ì˜¤ë¥˜: {e}")
    
    def _parse_text_log(self, log_line: str) -> Dict:
        """í…ìŠ¤íŠ¸ ë¡œê·¸ íŒŒì‹±"""
        # ê°„ë‹¨í•œ ë¡œê·¸ í˜•ì‹: [2024-08-03 19:30:15] ERROR [service-name] ë©”ì‹œì§€
        log_pattern = re.compile(
            r'\[(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2})\] (\w+) \[([^\]]+)\] (.+)'
        )
        
        match = log_pattern.match(log_line.strip())
        if match:
            timestamp, level, service, message = match.groups()
            return {
                'timestamp': timestamp,
                'level': level,
                'service': service,
                'message': message,
                'raw_line': log_line.strip()
            }
        else:
            return {
                'timestamp': datetime.now().isoformat(),
                'level': 'UNKNOWN',
                'service': 'unknown',
                'message': log_line.strip(),
                'raw_line': log_line.strip()
            }
    
    def detect_anomalies(self, time_window_minutes: int = 30) -> Dict[str, List]:
        """ë¡œê·¸ íŒ¨í„´ ê¸°ë°˜ ì´ìƒ ì§•í›„ ê°ì§€"""
        anomalies = defaultdict(list)
        
        # ì‹œê°„ ìœˆë„ìš°ë³„ ë¡œê·¸ ë¶„ì„
        time_buckets = defaultdict(list)
        for entry in self.log_entries:
            try:
                timestamp = datetime.fromisoformat(entry['timestamp'].replace('T', ' '))
                bucket = timestamp.replace(minute=(timestamp.minute // time_window_minutes) * time_window_minutes, 
                                         second=0, microsecond=0)
                time_buckets[bucket].append(entry)
            except:
                continue
        
        for time_bucket, entries in time_buckets.items():
            # 1. ê¸‰ê²©í•œ ì—ëŸ¬ ì¦ê°€
            error_count = sum(1 for e in entries if self.patterns['error'].search(e.get('message', '')))
            total_count = len(entries)
            
            if total_count > 0:
                error_rate = error_count / total_count
                if error_rate > 0.1:  # 10% ì´ìƒ ì—ëŸ¬
                    anomalies['high_error_rate'].append({
                        'timestamp': time_bucket.isoformat(),
                        'error_rate': error_rate,
                        'error_count': error_count,
                        'total_count': total_count
                    })
            
            # 2. íŠ¹ì • íŒ¨í„´ì˜ ê¸‰ì¦
            for pattern_name, pattern in self.patterns.items():
                pattern_count = sum(1 for e in entries if pattern.search(e.get('message', '')))
                if pattern_count > 10:  # ì„ê³„ê°’
                    anomalies[f'pattern_spike_{pattern_name}'].append({
                        'timestamp': time_bucket.isoformat(),
                        'count': pattern_count,
                        'pattern': pattern_name
                    })
            
            # 3. ì„œë¹„ìŠ¤ë³„ ë¡œê·¸ ë³¼ë¥¨ ë¶„ì„
            service_counts = Counter(e.get('service', 'unknown') for e in entries)
            for service, count in service_counts.items():
                if count > 100:  # 30ë¶„ì— 100ê°œ ì´ìƒ ë¡œê·¸
                    anomalies['high_volume_service'].append({
                        'timestamp': time_bucket.isoformat(),
                        'service': service,
                        'count': count
                    })
        
        return dict(anomalies)
    
    def find_error_correlations(self) -> Dict[str, Any]:
        """ì—ëŸ¬ ê°„ì˜ ìƒê´€ê´€ê³„ ë¶„ì„"""
        correlations = {
            'error_sequences': [],
            'common_error_patterns': [],
            'service_error_propagation': {}
        }
        
        # ì—ëŸ¬ ì‹œí€€ìŠ¤ ë¶„ì„ (5ë¶„ ë‚´ì— ë°œìƒí•œ ì—°ê´€ëœ ì—ëŸ¬ë“¤)
        error_entries = [e for e in self.log_entries 
                        if self.patterns['error'].search(e.get('message', ''))]
        
        for i, error1 in enumerate(error_entries):
            try:
                timestamp1 = datetime.fromisoformat(error1['timestamp'].replace('T', ' '))
                
                related_errors = []
                for j, error2 in enumerate(error_entries[i+1:], i+1):
                    timestamp2 = datetime.fromisoformat(error2['timestamp'].replace('T', ' '))
                    
                    # 5ë¶„ ì´ë‚´ ë°œìƒí•œ ì—ëŸ¬ë“¤
                    if (timestamp2 - timestamp1).total_seconds() <= 300:
                        related_errors.append({
                            'service': error2.get('service'),
                            'message': error2.get('message'),
                            'time_diff_seconds': (timestamp2 - timestamp1).total_seconds()
                        })
                
                if len(related_errors) >= 2:  # 2ê°œ ì´ìƒì˜ ì—°ê´€ ì—ëŸ¬
                    correlations['error_sequences'].append({
                        'trigger_error': {
                            'service': error1.get('service'),
                            'message': error1.get('message'),
                            'timestamp': error1.get('timestamp')
                        },
                        'related_errors': related_errors
                    })
            except:
                continue
        
        return correlations
    
    def generate_debug_report(self) -> str:
        """ì¢…í•© ë””ë²„ê¹… ë¦¬í¬íŠ¸ ìƒì„±"""
        total_entries = len(self.log_entries)
        if total_entries == 0:
            return "ë¶„ì„í•  ë¡œê·¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        # ê¸°ë³¸ í†µê³„
        level_counts = Counter(e.get('level', 'UNKNOWN') for e in self.log_entries)
        service_counts = Counter(e.get('service', 'unknown') for e in self.log_entries)
        
        # ì´ìƒ ì§•í›„ ê°ì§€
        anomalies = self.detect_anomalies()
        correlations = self.find_error_correlations()
        
        report = f"""
=== ë¡œê·¸ ë¶„ì„ ë””ë²„ê¹… ë¦¬í¬íŠ¸ ===

ğŸ“Š ê¸°ë³¸ í†µê³„:
- ì´ ë¡œê·¸ ì—”íŠ¸ë¦¬: {total_entries:,}ê°œ
- ë¡œê·¸ ë ˆë²¨ ë¶„í¬:
"""
        
        for level, count in level_counts.most_common():
            percentage = (count / total_entries) * 100
            report += f"  â€¢ {level}: {count:,}ê°œ ({percentage:.1f}%)\n"
        
        report += f"\nğŸ¢ ì„œë¹„ìŠ¤ë³„ ë¡œê·¸ ë¶„í¬:\n"
        for service, count in service_counts.most_common(10):
            percentage = (count / total_entries) * 100
            report += f"  â€¢ {service}: {count:,}ê°œ ({percentage:.1f}%)\n"
        
        report += f"\nğŸš¨ ê°ì§€ëœ ì´ìƒ ì§•í›„:\n"
        if not anomalies:
            report += "  â€¢ íŠ¹ë³„í•œ ì´ìƒ ì§•í›„ ì—†ìŒ\n"
        else:
            for anomaly_type, incidents in anomalies.items():
                report += f"  â€¢ {anomaly_type}: {len(incidents)}ê±´\n"
                for incident in incidents[:3]:  # ìµœëŒ€ 3ê°œë§Œ í‘œì‹œ
                    report += f"    - {incident}\n"
        
        report += f"\nğŸ”— ì—ëŸ¬ ìƒê´€ê´€ê³„ ë¶„ì„:\n"
        error_sequences = correlations.get('error_sequences', [])
        if not error_sequences:
            report += "  â€¢ ëª…í™•í•œ ì—ëŸ¬ ì—°ì‡„ íŒ¨í„´ ì—†ìŒ\n"
        else:
            report += f"  â€¢ {len(error_sequences)}ê°œì˜ ì—ëŸ¬ ì—°ì‡„ íŒ¨í„´ ë°œê²¬\n"
            for seq in error_sequences[:3]:
                trigger = seq['trigger_error']
                report += f"    - {trigger['service']}: {trigger['message'][:50]}...\n"
                report += f"      â†’ {len(seq['related_errors'])}ê°œì˜ í›„ì† ì—ëŸ¬ ë°œìƒ\n"
        
        # ê¶Œì¥ ì¡°ì¹˜ì‚¬í•­
        report += f"\nğŸ’¡ ê¶Œì¥ ì¡°ì¹˜ì‚¬í•­:\n"
        
        error_rate = level_counts.get('ERROR', 0) / total_entries
        if error_rate > 0.05:
            report += f"  â€¢ ë†’ì€ ì—ëŸ¬ìœ¨({error_rate:.1%}) - ì—ëŸ¬ ë¡œê·¸ ìƒì„¸ ë¶„ì„ í•„ìš”\n"
        
        if anomalies.get('high_error_rate'):
            report += f"  â€¢ íŠ¹ì • ì‹œê°„ëŒ€ ì—ëŸ¬ ê¸‰ì¦ - í•´ë‹¹ ì‹œê°„ ë°°í¬/ë³€ê²½ì‚¬í•­ í™•ì¸\n"
        
        if error_sequences:
            report += f"  â€¢ ì—ëŸ¬ ì—°ì‡„ íŒ¨í„´ ë°œê²¬ - ì„œë¹„ìŠ¤ ê°„ ì˜ì¡´ì„± ë° íƒ€ì„ì•„ì›ƒ ì„¤ì • ê²€í† \n"
        
        return report

# ì‚¬ìš© ì˜ˆì‹œ (ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° ìƒì„±)
def simulate_log_data():
    """ì‹œë®¬ë ˆì´ì…˜ìš© ë¡œê·¸ ë°ì´í„° ìƒì„±"""
    log_analyzer = LogAnalyzer()
    
    # ìƒ˜í”Œ ë¡œê·¸ ë°ì´í„°
    sample_logs = [
        {"timestamp": "2024-08-03T19:30:15", "level": "INFO", "service": "user-service", 
         "message": "User login successful for user_id=12345"},
        {"timestamp": "2024-08-03T19:30:16", "level": "ERROR", "service": "payment-service", 
         "message": "Connection timeout to external payment API"},
        {"timestamp": "2024-08-03T19:30:17", "level": "ERROR", "service": "order-service", 
         "message": "Failed to create order: Payment service unavailable"},
        {"timestamp": "2024-08-03T19:30:18", "level": "WARN", "service": "inventory-service", 
         "message": "Low stock alert for product_id=67890"},
        {"timestamp": "2024-08-03T19:30:20", "level": "ERROR", "service": "notification-service", 
         "message": "Failed to send order confirmation: Order not found"}
    ] * 50  # 50ë²ˆ ë°˜ë³µí•˜ì—¬ íŒ¨í„´ ìƒì„±
    
    log_analyzer.log_entries = sample_logs
    
    # ë””ë²„ê¹… ë¦¬í¬íŠ¸ ìƒì„±
    report = log_analyzer.generate_debug_report()
    print(report)

# simulate_log_data()
```

## ê³ ê¸‰ ë¡œê·¸ ë¶„ì„ ê¸°ë²•

### 1. ì‹¤ì‹œê°„ ë¡œê·¸ ìŠ¤íŠ¸ë¦¬ë° ë¶„ì„

```python
import asyncio
import aiofiles
from datetime import datetime, timedelta

class RealTimeLogAnalyzer:
    def __init__(self):
        self.alert_threshold = {
            'error_rate': 0.1,  # 10% ì—ëŸ¬ìœ¨
            'response_time': 2.0,  # 2ì´ˆ ì‘ë‹µì‹œê°„
            'memory_usage': 0.8  # 80% ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ 
        }
        self.recent_entries = []
        self.max_recent_size = 1000
    
    async def monitor_log_stream(self, log_file_path: str):
        """ì‹¤ì‹œê°„ ë¡œê·¸ ëª¨ë‹ˆí„°ë§"""
        async with aiofiles.open(log_file_path, 'r') as f:
            # íŒŒì¼ ëìœ¼ë¡œ ì´ë™
            await f.seek(0, 2)
            
            while True:
                line = await f.readline()
                if line:
                    await self._process_log_line(line.strip())
                else:
                    # ìƒˆë¡œìš´ ë¼ì¸ì´ ì—†ìœ¼ë©´ ì ì‹œ ëŒ€ê¸°
                    await asyncio.sleep(0.1)
    
    async def _process_log_line(self, line: str):
        """ë¡œê·¸ ë¼ì¸ ì‹¤ì‹œê°„ ì²˜ë¦¬"""
        try:
            # JSON ë¡œê·¸ íŒŒì‹±
            if line.startswith('{'):
                entry = json.loads(line)
                
                # ìµœê·¼ ì—”íŠ¸ë¦¬ ë²„í¼ ê´€ë¦¬
                self.recent_entries.append(entry)
                if len(self.recent_entries) > self.max_recent_size:
                    self.recent_entries.pop(0)
                
                # ì‹¤ì‹œê°„ ì•Œë¦¼ ì¡°ê±´ í™•ì¸
                await self._check_alert_conditions(entry)
        except Exception as e:
            print(f"ë¡œê·¸ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
    
    async def _check_alert_conditions(self, entry: Dict):
        """ì‹¤ì‹œê°„ ì•Œë¦¼ ì¡°ê±´ í™•ì¸"""
        # ì—ëŸ¬ìœ¨ ì²´í¬
        recent_errors = sum(1 for e in self.recent_entries[-100:] 
                          if e.get('level') == 'ERROR')
        error_rate = recent_errors / min(100, len(self.recent_entries))
        
        if error_rate > self.alert_threshold['error_rate']:
            await self._send_alert(f"ë†’ì€ ì—ëŸ¬ìœ¨ ê°ì§€: {error_rate:.1%}")
        
        # ì‘ë‹µì‹œê°„ ì²´í¬
        if 'response_time' in entry:
            response_time = float(entry['response_time'])
            if response_time > self.alert_threshold['response_time']:
                await self._send_alert(
                    f"ëŠë¦° ì‘ë‹µì‹œê°„: {response_time:.2f}s in {entry.get('service', 'unknown')}"
                )
    
    async def _send_alert(self, message: str):
        """ì•Œë¦¼ ë°œì†¡"""
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        print(f"ğŸš¨ [{timestamp}] ALERT: {message}")
        # ì‹¤ì œë¡œëŠ” Slack, ì´ë©”ì¼, Webhook ë“±ìœ¼ë¡œ ì•Œë¦¼ ë°œì†¡
```

### 2. ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ì´ìƒ ì§•í›„ ê°ì§€

```python
import numpy as np
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
from typing import List, Dict, Tuple

class MLLogAnalyzer:
    def __init__(self):
        self.scaler = StandardScaler()
        self.anomaly_detector = IsolationForest(
            contamination=0.1,  # 10% ì •ë„ë¥¼ ì´ìƒì¹˜ë¡œ ê°€ì •
            random_state=42
        )
        self.is_trained = False
    
    def extract_features(self, log_entries: List[Dict]) -> np.ndarray:
        """ë¡œê·¸ ì—”íŠ¸ë¦¬ì—ì„œ íŠ¹ì„± ì¶”ì¶œ"""
        features = []
        
        for entry in log_entries:
            feature_vector = [
                # ì‹œê°„ ê¸°ë°˜ íŠ¹ì„±
                datetime.fromisoformat(entry['timestamp'].replace('T', ' ')).hour,
                datetime.fromisoformat(entry['timestamp'].replace('T', ' ')).minute,
                
                # ë¡œê·¸ ë ˆë²¨ ì›í•« ì¸ì½”ë”©
                1 if entry.get('level') == 'ERROR' else 0,
                1 if entry.get('level') == 'WARN' else 0,
                1 if entry.get('level') == 'INFO' else 0,
                
                # ë©”ì‹œì§€ ê¸¸ì´
                len(entry.get('message', '')),
                
                # ì„œë¹„ìŠ¤ë³„ ì›í•« ì¸ì½”ë”© (ì£¼ìš” ì„œë¹„ìŠ¤ë§Œ)
                1 if entry.get('service') == 'payment-service' else 0,
                1 if entry.get('service') == 'order-service' else 0,
                1 if entry.get('service') == 'user-service' else 0,
                
                # ì‘ë‹µì‹œê°„ (ìˆëŠ” ê²½ìš°)
                float(entry.get('response_time', 0)),
                
                # íŠ¹ì • í‚¤ì›Œë“œ ì¡´ì¬ ì—¬ë¶€
                1 if 'timeout' in entry.get('message', '').lower() else 0,
                1 if 'connection' in entry.get('message', '').lower() else 0,
                1 if 'memory' in entry.get('message', '').lower() else 0,
            ]
            
            features.append(feature_vector)
        
        return np.array(features)
    
    def train_anomaly_detector(self, normal_logs: List[Dict]):
        """ì •ìƒ ë¡œê·¸ë¡œ ì´ìƒ ê°ì§€ ëª¨ë¸ í›ˆë ¨"""
        features = self.extract_features(normal_logs)
        features_scaled = self.scaler.fit_transform(features)
        
        self.anomaly_detector.fit(features_scaled)
        self.is_trained = True
        
        print(f"ì´ìƒ ê°ì§€ ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ: {len(normal_logs)}ê°œ ìƒ˜í”Œ ì‚¬ìš©")
    
    def detect_anomalies(self, log_entries: List[Dict]) -> List[Tuple[int, float]]:
        """ì´ìƒ ì§•í›„ ê°ì§€"""
        if not self.is_trained:
            raise ValueError("ëª¨ë¸ì´ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        features = self.extract_features(log_entries)
        features_scaled = self.scaler.transform(features)
        
        # ì´ìƒì¹˜ ì˜ˆì¸¡ (-1: ì´ìƒì¹˜, 1: ì •ìƒ)
        predictions = self.anomaly_detector.predict(features_scaled)
        
        # ì´ìƒì¹˜ ì ìˆ˜ ê³„ì‚° (ë” ë‚®ì„ìˆ˜ë¡ ì´ìƒì¹˜)
        scores = self.anomaly_detector.score_samples(features_scaled)
        
        # ì´ìƒì¹˜ë¡œ ë¶„ë¥˜ëœ ì¸ë±ìŠ¤ì™€ ì ìˆ˜ ë°˜í™˜
        anomalies = [(i, scores[i]) for i, pred in enumerate(predictions) if pred == -1]
        
        return sorted(anomalies, key=lambda x: x[1])  # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬

# ì‚¬ìš© ì˜ˆì‹œ
ml_analyzer = MLLogAnalyzer()

# ì •ìƒ ë¡œê·¸ë¡œ í›ˆë ¨ (ì‹¤ì œë¡œëŠ” ê³¼ê±° ì •ìƒ ê¸°ê°„ì˜ ë¡œê·¸ ì‚¬ìš©)
normal_logs = [
    {"timestamp": "2024-08-03T10:30:15", "level": "INFO", "service": "user-service", 
     "message": "User login successful", "response_time": "0.2"},
    {"timestamp": "2024-08-03T10:30:16", "level": "INFO", "service": "payment-service", 
     "message": "Payment processed", "response_time": "0.5"},
    # ... ë” ë§ì€ ì •ìƒ ë¡œê·¸
] * 100

ml_analyzer.train_anomaly_detector(normal_logs)

# í˜„ì¬ ë¡œê·¸ì—ì„œ ì´ìƒ ì§•í›„ ê°ì§€
current_logs = [
    {"timestamp": "2024-08-03T19:30:15", "level": "ERROR", "service": "payment-service", 
     "message": "Connection timeout to external payment API", "response_time": "5.0"},
    {"timestamp": "2024-08-03T19:30:16", "level": "INFO", "service": "user-service", 
     "message": "User login successful", "response_time": "0.3"},
]

anomalies = ml_analyzer.detect_anomalies(current_logs)
for index, score in anomalies:
    print(f"ì´ìƒ ë¡œê·¸ ê°ì§€ (ì¸ë±ìŠ¤ {index}, ì ìˆ˜: {score:.3f}): {current_logs[index]}")
```

## í•µì‹¬ ìš”ì 

### 1. ìë™í™”ëœ ë¡œê·¸ ë¶„ì„ì˜ í•„ìš”ì„±

ìˆ˜ë§Œ ì¤„ì˜ ë¡œê·¸ë¥¼ ì‚¬ëŒì´ ì½ì„ ìˆ˜ëŠ” ì—†ë‹¤. íŒ¨í„´ ì¸ì‹ê³¼ ì´ìƒ ì§•í›„ ê°ì§€ë¥¼ ìë™í™”í•´ì•¼ í•œë‹¤.

### 2. ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ê³¼ ì•Œë¦¼

ë¬¸ì œê°€ ë°œìƒí•œ í›„ ë¶„ì„í•˜ëŠ” ê²ƒë³´ë‹¤ ì‹¤ì‹œê°„ìœ¼ë¡œ ê°ì§€í•˜ê³  ì¦‰ì‹œ ì•Œë¦¼ì„ ë°›ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤.

### 3. ë¨¸ì‹ ëŸ¬ë‹ì„ í™œìš©í•œ ê³ ë„í™”

ì •ìƒ íŒ¨í„´ì„ í•™ìŠµí•˜ì—¬ ì˜ˆìƒì¹˜ ëª»í•œ ì´ìƒ ì§•í›„ë¥¼ ìë™ìœ¼ë¡œ ê°ì§€í•  ìˆ˜ ìˆë‹¤.

### 4. ìƒê´€ê´€ê³„ ë¶„ì„

ë‹¨ì¼ ì—ëŸ¬ë³´ë‹¤ëŠ” ì—ëŸ¬ ê°„ì˜ ì—°ê´€ì„±ê³¼ ì „íŒŒ íŒ¨í„´ì„ ë¶„ì„í•˜ì—¬ ê·¼ë³¸ ì›ì¸ì„ ì°¾ì•„ì•¼ í•œë‹¤.

---

**ì´ì „**: [13.5c ìŠ¤ë§ˆíŠ¸ ë””ë²„ê¹… ë„êµ¬](05c-smart-debugging-tools.md)  
**ë‹¤ìŒ**: [13.5 ë””ë²„ê¹… ê¸°ë²• ë° ë¬¸ì œ í•´ê²°](05-debugging-troubleshooting.md) ê°œìš”ë¡œ ëŒì•„ê°‘ë‹ˆë‹¤.
