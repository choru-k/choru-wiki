---
tags:
  - DistributedSystems
  - Kafka
  - MessageQueue
  - Streaming
  - BackPressure
---

# Chapter 8-4C: ë©”ì‹œì§€ íì™€ ìŠ¤íŠ¸ë¦¬ë°: ë¹„ë™ê¸° í†µì‹ ì˜ ì¤‘ì¶”

## ğŸ¯ ì´ ì„¹ì…˜ì—ì„œ ë°°ìš¸ ë‚´ìš©

ëŒ€ìš©ëŸ‰ ë©”ì‹œì§€ ì²˜ë¦¬ì™€ ìŠ¤íŠ¸ë¦¬ë° ì‹œìŠ¤í…œì˜ í•µì‹¬ ê¸°ìˆ ë“¤ì„ ë§ˆìŠ¤í„°í•©ë‹ˆë‹¤:

1. **Kafka Event Streaming** - ëŒ€ìš©ëŸ‰ ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¬ë° í”Œë«í¼
2. **Back-pressure ë° Flow Control** - ì‹œìŠ¤í…œ ê³¼ë¶€í•˜ ë°©ì§€
3. **Exactly-Once Semantics** - ì¤‘ë³µ ë° ì†ì‹¤ ì—†ëŠ” ë©”ì‹œì§€ ì²˜ë¦¬

## 1. Kafkaë¥¼ ì´ìš©í•œ Event Streaming

KafkaëŠ” ëŒ€ìš©ëŸ‰ ì‹¤ì‹œê°„ ë°ì´í„° ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•œ ë¶„ì‚° ìŠ¤íŠ¸ë¦¬ë° í”Œë«í¼ì…ë‹ˆë‹¤.

### 1.1 ì‹ ë¢°ì„± ë†’ì€ Producer êµ¬í˜„

```java
// Kafka Producer with ì™„ë²½í•œ ì„¤ì •
@Component
public class ReliableKafkaProducer {
    private final KafkaProducer<String, byte[]> producer;
    private final ObjectMapper objectMapper;
    
    public ReliableKafkaProducer() {
        Properties props = new Properties();
        
        // ì‹ ë¢°ì„± ì„¤ì •
        props.put(ProducerConfig.ACKS_CONFIG, "all");  // ëª¨ë“  replica í™•ì¸
        props.put(ProducerConfig.RETRIES_CONFIG, Integer.MAX_VALUE);
        props.put(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, 5);
        props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);  // ì¤‘ë³µ ë°©ì§€
        
        // ì„±ëŠ¥ ì„¤ì •
        props.put(ProducerConfig.COMPRESSION_TYPE_CONFIG, "snappy");
        props.put(ProducerConfig.LINGER_MS_CONFIG, 20);  // ë°°ì¹˜ ì²˜ë¦¬
        props.put(ProducerConfig.BATCH_SIZE_CONFIG, 32768);
        
        // ì—ëŸ¬ ì²˜ë¦¬
        props.put(ProducerConfig.REQUEST_TIMEOUT_MS_CONFIG, 30000);
        props.put(ProducerConfig.DELIVERY_TIMEOUT_MS_CONFIG, 120000);
        
        this.producer = new KafkaProducer<>(props);
    }
    
    public CompletableFuture<RecordMetadata> sendEvent(
        String topic,
        String key,
        Object event,
        Map<String, String> headers
    ) {
        CompletableFuture<RecordMetadata> future = new CompletableFuture<>();
        
        try {
            // ì´ë²¤íŠ¸ ì§ë ¬í™”
            byte[] value = objectMapper.writeValueAsBytes(event);
            
            // í—¤ë” ì¶”ê°€
            ProducerRecord<String, byte[]> record = 
                new ProducerRecord<>(topic, key, value);
            
            headers.forEach((k, v) -> 
                record.headers().add(k, v.getBytes())
            );
            
            // íŠ¸ë ˆì´ì‹± ì •ë³´
            record.headers().add("trace-id", 
                MDC.get("traceId").getBytes());
            record.headers().add("timestamp", 
                String.valueOf(System.currentTimeMillis()).getBytes());
            
            // ë¹„ë™ê¸° ì „ì†¡
            producer.send(record, (metadata, exception) -> {
                if (exception != null) {
                    // Dead Letter Queueë¡œ ì „ì†¡
                    sendToDeadLetter(record, exception);
                    future.completeExceptionally(exception);
                } else {
                    // ë©”íŠ¸ë¦­ ê¸°ë¡
                    recordMetrics(metadata);
                    future.complete(metadata);
                }
            });
            
        } catch (Exception e) {
            future.completeExceptionally(e);
        }
        
        return future;
    }
    
    private void sendToDeadLetter(ProducerRecord<String, byte[]> record, 
                                  Exception exception) {
        String dlqTopic = record.topic() + ".dlq";
        
        ProducerRecord<String, byte[]> dlqRecord = 
            new ProducerRecord<>(dlqTopic, record.key(), record.value());
        
        // ì—ëŸ¬ ì •ë³´ ì¶”ê°€
        dlqRecord.headers().add("original-topic", 
            record.topic().getBytes());
        dlqRecord.headers().add("error-message", 
            exception.getMessage().getBytes());
        dlqRecord.headers().add("error-timestamp", 
            String.valueOf(System.currentTimeMillis()).getBytes());
        
        producer.send(dlqRecord);
    }
    
    private void recordMetrics(RecordMetadata metadata) {
        // ë©”íŠ¸ë¦­ ê¸°ë¡
        Metrics.counter("kafka.messages.sent")
            .tag("topic", metadata.topic())
            .increment();
        
        Metrics.timer("kafka.send.duration")
            .tag("topic", metadata.topic())
            .record(Duration.ofMillis(System.currentTimeMillis()));
    }
}
```

### 1.2 Exactly-Once Consumer êµ¬í˜„

```java
// Kafka Consumer with exactly-once semantics
@Component
public class ExactlyOnceKafkaConsumer {
    private final KafkaConsumer<String, byte[]> consumer;
    private final TransactionManager txManager;
    private final IdempotencyStore idempotencyStore;
    
    public ExactlyOnceKafkaConsumer() {
        Properties props = new Properties();
        
        // Exactly-once ì„¤ì •
        props.put(ConsumerConfig.ISOLATION_LEVEL_CONFIG, "read_committed");
        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false);
        
        // ì„±ëŠ¥ê³¼ ì‹ ë¢°ì„± ê· í˜•
        props.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, 100);
        props.put(ConsumerConfig.MAX_POLL_INTERVAL_MS_CONFIG, 300000);
        props.put(ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG, 30000);
        
        this.consumer = new KafkaConsumer<>(props);
    }
    
    public void consume(List<String> topics) {
        consumer.subscribe(topics);
        
        while (true) {
            ConsumerRecords<String, byte[]> records = 
                consumer.poll(Duration.ofMillis(100));
            
            if (!records.isEmpty()) {
                processRecords(records);
            }
        }
    }
    
    private void processRecords(ConsumerRecords<String, byte[]> records) {
        Map<TopicPartition, OffsetAndMetadata> offsetsToCommit = 
            new HashMap<>();
        
        for (ConsumerRecord<String, byte[]> record : records) {
            try {
                // íŠ¸ëœì­ì…˜ ì‹œì‘
                txManager.begin();
                
                // Idempotency ì²´í¬
                if (!isAlreadyProcessed(record)) {
                    // ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ì²˜ë¦¬
                    processRecord(record);
                    
                    // ì²˜ë¦¬ ì™„ë£Œ ë§ˆí‚¹
                    markAsProcessed(record);
                }
                
                // íŠ¸ëœì­ì…˜ ì»¤ë°‹
                txManager.commit();
                
                // ì˜¤í”„ì…‹ ì €ì¥
                offsetsToCommit.put(
                    new TopicPartition(record.topic(), record.partition()),
                    new OffsetAndMetadata(record.offset() + 1)
                );
                
            } catch (Exception e) {
                // íŠ¸ëœì­ì…˜ ë¡¤ë°±
                txManager.rollback();
                
                // ì¬ì‹œë„ ë˜ëŠ” DLQ
                handleError(record, e);
            }
        }
        
        // ë°°ì¹˜ ì»¤ë°‹
        consumer.commitSync(offsetsToCommit);
    }
    
    private boolean isAlreadyProcessed(ConsumerRecord<String, byte[]> record) {
        String idempotencyKey = new String(
            record.headers().lastHeader("idempotency-key").value()
        );
        
        return idempotencyStore.exists(idempotencyKey);
    }
    
    private void markAsProcessed(ConsumerRecord<String, byte[]> record) {
        String idempotencyKey = new String(
            record.headers().lastHeader("idempotency-key").value()
        );
        
        idempotencyStore.store(idempotencyKey, {
            "processed_at": System.currentTimeMillis(),
            "topic": record.topic(),
            "partition": record.partition(),
            "offset": record.offset()
        });
    }
    
    private void processRecord(ConsumerRecord<String, byte[]> record) {
        try {
            // ë©”ì‹œì§€ ì—­ì§ë ¬í™”
            Object event = objectMapper.readValue(record.value(), Object.class);
            
            // ì´ë²¤íŠ¸ íƒ€ì…ì— ë”°ë¥¸ ì²˜ë¦¬
            EventHandler handler = eventHandlerRegistry.getHandler(
                event.getClass()
            );
            
            handler.handle(event);
            
            // ë©”íŠ¸ë¦­ ê¸°ë¡
            Metrics.counter("kafka.messages.processed")
                .tag("topic", record.topic())
                .tag("event_type", event.getClass().getSimpleName())
                .increment();
                
        } catch (Exception e) {
            logger.error("Failed to process record: {}", record, e);
            throw e;
        }
    }
    
    private void handleError(ConsumerRecord<String, byte[]> record, Exception e) {
        // 1. ì¬ì‹œë„ ë¡œì§
        if (isRetryable(e)) {
            int retryCount = getRetryCount(record);
            if (retryCount < MAX_RETRIES) {
                sendToRetryTopic(record, retryCount + 1);
                return;
            }
        }
        
        // 2. Dead Letter Queue
        sendToDeadLetterQueue(record, e);
        
        // 3. ì•Œë¦¼
        sendAlert("Message processing failed", record, e);
    }
}
```

**Kafkaì˜ í•µì‹¬ ì¥ì :**

1. **ë†’ì€ ì²˜ë¦¬ëŸ‰**: ì´ˆë‹¹ ìˆ˜ë°±ë§Œ ë©”ì‹œì§€ ì²˜ë¦¬
2. **ë‚´ê²°í•¨ì„±**: ë³µì œì™€ ë¶„ì‚°ìœ¼ë¡œ ì¥ì•  ëŒ€ì‘
3. **ìˆœì„œ ë³´ì¥**: íŒŒí‹°ì…˜ ë‹¨ìœ„ë¡œ ë©”ì‹œì§€ ìˆœì„œ ë³´ì¥
4. **ì˜ì†ì„±**: ë””ìŠ¤í¬ì— ë©”ì‹œì§€ ì˜êµ¬ ì €ì¥

## 2. Back-pressureì™€ Flow Control

ì‹œìŠ¤í…œ ê³¼ë¶€í•˜ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•œ Back-pressure ë©”ì»¤ë‹ˆì¦˜ êµ¬í˜„:

### 2.1 Akka Streamsë¥¼ ì´ìš©í•œ Back-pressure

```scala
// Akka Streamsë¥¼ ì´ìš©í•œ Back-pressure êµ¬í˜„
import akka.stream._
import akka.stream.scaladsl._

class BackPressureStream(implicit system: ActorSystem) {
  implicit val materializer: Materializer = Materializer(system)
  
  // Source: ë°ì´í„° ìƒì‚°ì
  val fastProducer = Source
    .tick(10.milliseconds, 10.milliseconds, 1)
    .map { i =>
      println(s"Producing: $i")
      Event(s"event-$i", System.currentTimeMillis())
    }
  
  // Flow: ëŠë¦° ì²˜ë¦¬ê¸°
  val slowProcessor = Flow[Event]
    .buffer(10, OverflowStrategy.backpressure)  // ë²„í¼ ì´ˆê³¼ì‹œ back-pressure
    .mapAsync(2) { event =>
      Future {
        Thread.sleep(100)  // ëŠë¦° ì²˜ë¦¬ ì‹œë®¤ë ˆì´ì…˜
        println(s"Processing: ${event.id}")
        ProcessedEvent(event.id, event.timestamp, "processed")
      }
    }
  
  // Sink: ìµœì¢… ì†Œë¹„ì
  val sink = Sink.foreach[ProcessedEvent] { processed =>
    println(s"Consumed: ${processed.id}")
  }
  
  // Stream ì‹¤í–‰
  val runnableGraph = fastProducer
    .via(slowProcessor)
    .to(sink)
  
  val result = runnableGraph.run()
  
  // Dynamic throttling
  val dynamicThrottle = Source
    .tick(1.second, 1.second, 1)
    .conflate((_, _) => 1)  // ì••ë ¥ ìƒí™©ì—ì„œ ë©”ì‹œì§€ ë³‘í•©
    .zip(fastProducer)
    .map(_._2)
    .throttle(
      elements = 100,
      per = 1.second,
      maximumBurst = 20,
      mode = ThrottleMode.Shaping
    )
}
```

### 2.2 Reactive Streams êµ¬í˜„

```scala
// Reactive Streams êµ¬í˜„
class ReactiveStreamProcessor extends Processor[Event, ProcessedEvent] {
  private var subscription: Subscription = _
  private var subscriber: Subscriber[_ >: ProcessedEvent] = _
  private val buffer = new LinkedBlockingQueue[Event](100)
  private val requested = new AtomicLong(0)
  
  override def onSubscribe(s: Subscription): Unit = {
    this.subscription = s
    s.request(10)  // ì´ˆê¸° ìš”ì²­
  }
  
  override def onNext(event: Event): Unit = {
    if (!buffer.offer(event)) {
      // ë²„í¼ ê°€ë“ ì°¸ - back-pressure ì‹ í˜¸
      subscription.cancel()
      onError(new BufferOverflowException())
    } else {
      processIfPossible()
    }
  }
  
  private def processIfPossible(): Unit = {
    while (requested.get() > 0 && !buffer.isEmpty) {
      val event = buffer.poll()
      val processed = process(event)
      
      subscriber.onNext(processed)
      requested.decrementAndGet()
      
      // ë²„í¼ì— ì—¬ìœ ê°€ ìƒê¸°ë©´ ë” ìš”ì²­
      if (buffer.size() < 50) {
        subscription.request(10)
      }
    }
  }
  
  override def subscribe(s: Subscriber[_ >: ProcessedEvent]): Unit = {
    this.subscriber = s
    s.onSubscribe(new Subscription {
      override def request(n: Long): Unit = {
        requested.addAndGet(n)
        processIfPossible()
      }
      
      override def cancel(): Unit = {
        subscription.cancel()
      }
    })
  }
  
  override def onError(t: Throwable): Unit = {
    subscriber.onError(t)
  }
  
  override def onComplete(): Unit = {
    // ë‚¨ì€ ë²„í¼ ì²˜ë¦¬
    while (!buffer.isEmpty && requested.get() > 0) {
      val event = buffer.poll()
      val processed = process(event)
      subscriber.onNext(processed)
      requested.decrementAndGet()
    }
    
    subscriber.onComplete()
  }
  
  private def process(event: Event): ProcessedEvent = {
    // ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ì²˜ë¦¬
    Thread.sleep(100)  // ì‹œë®¤ë ˆì´ì…˜
    ProcessedEvent(event.id, event.timestamp, "processed")
  }
}
```

### 2.3 ë™ì  Flow Control

```java
// ë™ì  Flow Control êµ¬í˜„
public class DynamicFlowController {
    private final AtomicInteger currentLoad = new AtomicInteger(0);
    private final AtomicInteger maxCapacity = new AtomicInteger(100);
    private final CircularBuffer<Long> responseTimeHistory = 
        new CircularBuffer<>(100);
    
    public boolean shouldThrottle(Message message) {
        // 1. í˜„ì¬ ë¶€í•˜ ì²´í¬
        int load = currentLoad.get();
        int capacity = maxCapacity.get();
        
        if (load >= capacity) {
            return true;
        }
        
        // 2. ì‘ë‹µ ì‹œê°„ ê¸°ë°˜ ë™ì  ì¡°ì ˆ
        long avgResponseTime = responseTimeHistory.average();
        
        if (avgResponseTime > 1000) {  // 1ì´ˆ ì´ìƒì´ë©´
            // ìš©ëŸ‰ ê°ì†Œ
            maxCapacity.set(Math.max(10, capacity - 10));
            return load >= maxCapacity.get();
        } else if (avgResponseTime < 100) {  // 100ms ë¯¸ë§Œì´ë©´
            // ìš©ëŸ‰ ì¦ê°€
            maxCapacity.set(Math.min(200, capacity + 5));
        }
        
        return false;
    }
    
    public void processMessage(Message message) {
        long startTime = System.currentTimeMillis();
        currentLoad.incrementAndGet();
        
        try {
            // ë©”ì‹œì§€ ì²˜ë¦¬
            handleMessage(message);
            
        } finally {
            currentLoad.decrementAndGet();
            
            // ì‘ë‹µ ì‹œê°„ ê¸°ë¡
            long responseTime = System.currentTimeMillis() - startTime;
            responseTimeHistory.add(responseTime);
        }
    }
    
    // Circuit Breakerì™€ í†µí•©
    public void handleOverload() {
        if (currentLoad.get() > maxCapacity.get() * 1.5) {
            // ì˜¤ë²„ë¡œë“œ ìƒíƒœ - Circuit ì—´ê¸°
            circuitBreaker.open();
            
            // ê¸´ê¸‰ ì¡°ì¹˜
            emergencyThrottle();
        }
    }
    
    private void emergencyThrottle() {
        // ìµœìš°ì„  ë©”ì‹œì§€ë§Œ ì²˜ë¦¬
        messageQueue.retainOnly(msg -> msg.getPriority() == Priority.HIGH);
        
        // ìš©ëŸ‰ ëŒ€í­ ê°ì†Œ
        maxCapacity.set(10);
        
        // ì•Œë¦¼ ë°œì†¡
        alertService.sendAlert("System overload detected");
    }
}
```

## 3. ì‹¤ì „ í‹´ë‹ ì „ëµ

### 3.1 Producer Back-pressure

```python
# Producer ì¸¡ Back-pressure ì²˜ë¦¬
class BackPressureAwareProducer:
    def __init__(self):
        self.send_buffer = asyncio.Queue(maxsize=1000)
        self.metrics = ProducerMetrics()
        self.circuit_breaker = CircuitBreaker()
        
    async def send_with_backpressure(self, message: Message) -> bool:
        # 1. Circuit Breaker ì²´í¬
        if self.circuit_breaker.is_open():
            raise CircuitOpenError("Producer circuit is open")
        
        # 2. ë²„í¼ ìš©ëŸ‰ ì²´í¬
        if self.send_buffer.full():
            # Back-pressure ì‹ í˜¸
            await self.handle_backpressure(message)
            return False
        
        # 3. ë¹„ë™ê¸° ì „ì†¡
        await self.send_buffer.put(message)
        asyncio.create_task(self.process_send_queue())
        
        return True
    
    async def handle_backpressure(self, message: Message):
        # ì „ëµ 1: Drop oldest messages
        if message.priority == Priority.HIGH:
            # ìµœìš°ì„  ë©”ì‹œì§€ëŠ” ê°€ì¥ ì˜¤ë˜ëœ ê²ƒ ì œê±°
            try:
                self.send_buffer.get_nowait()
                await self.send_buffer.put(message)
                self.metrics.record_drop('old_message_dropped')
            except asyncio.QueueEmpty:
                pass
        
        # ì „ëµ 2: ëŒ€ê¸° (ì œí•œëœ ì‹œê°„)
        elif message.priority == Priority.MEDIUM:
            try:
                await asyncio.wait_for(
                    self.send_buffer.put(message),
                    timeout=1.0  # 1ì´ˆ ëŒ€ê¸°
                )
            except asyncio.TimeoutError:
                self.metrics.record_drop('timeout_drop')
                raise BackPressureError()
        
        # ì „ëµ 3: ì¦‰ì‹œ Drop
        else:
            self.metrics.record_drop('low_priority_drop')
            raise BackPressureError()
    
    async def process_send_queue(self):
        while not self.send_buffer.empty():
            try:
                message = await self.send_buffer.get()
                await self.kafka_producer.send(message)
                
                self.metrics.record_success()
                self.circuit_breaker.record_success()
                
            except Exception as e:
                self.circuit_breaker.record_failure()
                
                # ì¬ì‹œë„ ì „ëµ
                if self.should_retry(e):
                    await self.retry_queue.put(message)
                else:
                    await self.dead_letter_queue.put(message)
```

### 3.2 Consumer Back-pressure

```python
# Consumer ì¸¡ Back-pressure ì²˜ë¦¬
class BackPressureAwareConsumer:
    def __init__(self):
        self.processing_semaphore = asyncio.Semaphore(10)
        self.metrics = ConsumerMetrics()
        self.adaptive_batch_size = AdaptiveBatchSize()
        
    async def consume_with_backpressure(self):
        while True:
            # ë™ì  ë°°ì¹˜ í¬ê¸° ì¡°ì •
            batch_size = self.adaptive_batch_size.get_current_size()
            
            messages = await self.kafka_consumer.poll(
                max_records=batch_size,
                timeout=Duration.seconds(1)
            )
            
            if not messages:
                continue
            
            # Back-pressure ì—¬ë¶€ í™•ì¸
            if self.is_under_pressure():
                await self.handle_consumer_backpressure(messages)
            else:
                await self.process_messages_parallel(messages)
    
    async def is_under_pressure(self) -> bool:
        # ì—¬ëŸ¬ ì§€í‘œë¡œ Back-pressure ê°ì§€
        conditions = [
            self.processing_semaphore._value < 2,  # ê°€ìš© ìŠ¬ë¡¯ ì ìŒ
            self.metrics.get_avg_processing_time() > 5000,  # ì²˜ë¦¬ ì‹œê°„ ê¹
            self.get_memory_usage() > 0.8,  # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë†’ìŒ
            self.get_cpu_usage() > 0.9  # CPU ì‚¬ìš©ëŸ‰ ë†’ìŒ
        ]
        
        return any(conditions)
    
    async def handle_consumer_backpressure(self, messages):
        # ì „ëµ 1: ë°°ì¹˜ í¬ê¸° ê°ì†Œ
        self.adaptive_batch_size.decrease()
        
        # ì „ëµ 2: ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ì²˜ë¦¬
        high_priority = [msg for msg in messages 
                        if msg.priority == Priority.HIGH]
        low_priority = [msg for msg in messages 
                       if msg.priority == Priority.LOW]
        
        # ê³ ìš°ì„  ë©”ì‹œì§€ ë¨¼ì € ì²˜ë¦¬
        if high_priority:
            await self.process_messages_sequential(high_priority)
        
        # ì €ìš°ì„  ë©”ì‹œì§€ëŠ” ì§€ì—° ì²˜ë¦¬
        if low_priority:
            await self.defer_messages(low_priority)
    
    async def process_messages_parallel(self, messages):
        tasks = []
        for message in messages:
            async with self.processing_semaphore:
                task = asyncio.create_task(
                    self.process_single_message(message)
                )
                tasks.append(task)
        
        # ëª¨ë“  ë©”ì‹œì§€ ì²˜ë¦¬ ëŒ€ê¸°
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ê²°ê³¼ ì²˜ë¦¬
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                await self.handle_processing_error(messages[i], result)
            else:
                self.metrics.record_success()
    
    async def defer_messages(self, messages):
        # ë©”ì‹œì§€ë¥¼ ë‹¤ìŒ ë°°ì¹˜ë¡œ ì—°ê¸°
        for message in messages:
            await self.deferred_queue.put(message)
        
        # ì§€ì—°ëœ ë©”ì‹œì§€ ë©”íŠ¸ë¦­
        self.metrics.record_deferred(len(messages))
```

## í•µì‹¬ ìš”ì 

### 1. Kafkaì˜ ì „ëµì  í™œìš©

- **Exactly-once semantics**ë¡œ ë©”ì‹œì§€ ì¤‘ë³µ ë° ì†ì‹¤ ë°©ì§€
- **Partitioning**ìœ¼ë¡œ í™•ì¥ì„±ê³¼ ì„±ëŠ¥ í–¥ìƒ
- **Dead Letter Queue**ë¡œ ì¥ì•  ë©”ì‹œì§€ ì•ˆì „ ì²˜ë¦¬

### 2. Back-pressureë¡œ ì‹œìŠ¤í…œ ì•ˆì •ì„± ë³´ì¥

- **ë™ì  Flow Control**ë¡œ ì‹œìŠ¤í…œ ë¶€í•˜ì— ì ì‘
- **Buffering ì „ëµ**ìœ¼ë¡œ íŠ¸ë˜í”½ ìŠ¤íŒŒì´í¬ ì™„ì£¼
- **Priority-based Processing**ìœ¼ë¡œ ì¤‘ìš” ë©”ì‹œì§€ ìš°ì„  ì²˜ë¦¬

### 3. ëª¨ë‹ˆí„°ë§ê³¼ ë©”íŠ¸ë¦­

- **ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§**ìœ¼ë¡œ ë¬¸ì œ ì¡°ê¸° ë°œê²¬
- **Adaptive ì„¤ì •**ìœ¼ë¡œ ì‹œìŠ¤í…œ ìë™ ì—°ë‹ˆ
- **ì•Œë¦¼ ë° ìë™ ë³µêµ¬**ë¡œ ìš´ì˜ ë¹„ìš© ì ˆê°

---

**ì´ì „**: [04b-event-sourcing-cqrs.md](04b-event-sourcing-cqrs.md)
**ë‹¤ìŒ**: [04d-resilience-patterns.md](04d-resilience-patterns.md)ì—ì„œ Circuit Breakerì™€ ë³µì›ë ¥ íŒ¨í„´ì„ í•™ìŠµí•©ë‹ˆë‹¤.
