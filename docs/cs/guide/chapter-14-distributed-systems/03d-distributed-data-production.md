---
tags:
  - DistributedSystems
  - ProductionReady
  - BestPractices
  - Monitoring
  - DataConsistency
  - Guide
---

# 14.3D ë¶„ì‚° ë°ì´í„° ì‹¤ì „ ì„¤ê³„ì™€ ìš´ì˜

## ğŸ’¡ ë¶„ì‚° ë°ì´í„° ê´€ë¦¬ì—ì„œ ë°°ìš´ í•µì‹¬ êµí›ˆ

ì‹¤ì œ ìš´ì˜ í™˜ê²½ì—ì„œ ë¶„ì‚° ë°ì´í„° ì‹œìŠ¤í…œì„ êµ¬ì¶•í•˜ê³  ìš´ì˜í•˜ë©´ì„œ ì–»ì€ êµí›ˆê³¼ ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤ë¥¼ ê³µìœ í•©ë‹ˆë‹¤.

### 1. ì™„ë²½í•œ ë¶„ì‚° ë°ì´í„° ì‹œìŠ¤í…œì€ ì—†ë‹¤

```bash
âœ… ë°›ì•„ë“¤ì—¬ì•¼ í•  í˜„ì‹¤:
- Shardingí•˜ë©´ íŠ¸ëœì­ì…˜ ë³µì¡ì„± ì¦ê°€
- Replicationí•˜ë©´ ì¼ê´€ì„± ë¬¸ì œ ë°œìƒ
- í™•ì¥ì„±ê³¼ ì¼ê´€ì„±ì€ íŠ¸ë ˆì´ë“œì˜¤í”„
- ë„¤íŠ¸ì›Œí¬ ë¶„í• ê³¼ ì¶©ëŒì€ ì •ìƒ ìƒí™©
```

### 2. ë°ì´í„° íŠ¹ì„±ì— ë§ëŠ” ì „ëµ ì„ íƒ

```python
# ì‚¬ìš©ì í”„ë¡œí•„: ê°•í•œ ì¼ê´€ì„± í•„ìš”
user_data = read_with_strong_consistency(user_id)

# ìƒí’ˆ ì¶”ì²œ: ìµœì¢… ì¼ê´€ì„±ìœ¼ë¡œ ì¶©ë¶„  
recommendations = read_with_eventual_consistency(user_id)

# ì‹¤ì‹œê°„ í”¼ë“œ: ì•½í•œ ì¼ê´€ì„±ë„ í—ˆìš©
feed = read_from_nearest_replica(user_id)
```

### 3. ê´€ì°°ê³¼ ë³´ìƒì„ í†µí•œ ì‹¤ìš©ì  ì ‘ê·¼

```python
# ì´ìƒì ì¸ ACID ëŒ€ì‹  ì‹¤ìš©ì  í•´ê²°ì±…
def practical_distributed_transaction():
    try:
        # 1. ë‚™ê´€ì  ì²˜ë¦¬ (ë¹ ë¦„)
        result = process_optimistically()
        
        # 2. ë¹„ë™ê¸° ê²€ì¦
        schedule_async_validation(result.transaction_id)
        
        # 3. ë¬¸ì œ ë°œê²¬ ì‹œ ë³´ìƒ
        if validation_failed:
            compensate_transaction(result.transaction_id)
            notify_user_with_apology()
        
        return result
    except Exception:
        # 4. ì‹¤íŒ¨ ì‹œ ëª…í™•í•œ ì—ëŸ¬
        raise TransactionFailedException()
```

## ğŸ”§ ì‹¤ì „ ìš´ì˜ ê³ ë ¤ì‚¬í•­

### ğŸ“Š ë¶„ì‚° ë°ì´í„° ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§

```python
class DistributedDataMonitor:
    """ë¶„ì‚° ë°ì´í„° ì‹œìŠ¤í…œ ì¢…í•© ëª¨ë‹ˆí„°ë§"""
    
    def __init__(self):
        self.metrics_collector = MetricsCollector()
        self.alert_manager = AlertManager()
        self.health_checker = HealthChecker()
        
    def monitor_shard_health(self):
        """ìƒ¤ë“œë³„ ê±´ê°• ìƒíƒœ ëª¨ë‹ˆí„°ë§"""
        shard_metrics = {}
        
        for shard_id, shard_config in self.get_all_shards():
            metrics = {
                # ê¸°ë³¸ ì„±ëŠ¥ ì§€í‘œ
                'cpu_usage': self.get_shard_cpu(shard_id),
                'memory_usage': self.get_shard_memory(shard_id),
                'disk_usage': self.get_shard_disk(shard_id),
                
                # ë°ì´í„°ë² ì´ìŠ¤ ì§€í‘œ
                'connection_count': self.get_active_connections(shard_id),
                'query_latency_p95': self.get_query_latency_p95(shard_id),
                'lock_wait_time': self.get_lock_wait_time(shard_id),
                
                # ë¶„ì‚° ì‹œìŠ¤í…œ ì§€í‘œ
                'replication_lag': self.get_replication_lag(shard_id),
                'data_size_gb': self.get_data_size(shard_id),
                'hotspot_score': self.calculate_hotspot_score(shard_id),
            }
            
            shard_metrics[shard_id] = metrics
            
            # ì„ê³„ê°’ ì²´í¬ ë° ì•Œë¦¼
            self.check_thresholds_and_alert(shard_id, metrics)
        
        return shard_metrics
    
    def calculate_hotspot_score(self, shard_id):
        """í•«ìŠ¤íŒŸ ì ìˆ˜ ê³„ì‚° (0-100)"""
        # ìš”ì²­ ë¶„ì‚°ë„ í™•ì¸
        request_distribution = self.get_request_distribution_last_hour(shard_id)
        total_requests = sum(request_distribution.values())
        
        if total_requests == 0:
            return 0
        
        # ì§€ë‹ˆ ê³„ìˆ˜ ê³„ì‚°ìœ¼ë¡œ ë¶ˆê· ë“±ë„ ì¸¡ì •
        sorted_requests = sorted(request_distribution.values(), reverse=True)
        n = len(sorted_requests)
        
        cumsum = 0
        gini_sum = 0
        
        for i, requests in enumerate(sorted_requests):
            cumsum += requests
            gini_sum += (2 * (i + 1) - n - 1) * requests
        
        gini = gini_sum / (n * cumsum) if cumsum > 0 else 0
        
        # 0-100 ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜ (ë†’ì„ìˆ˜ë¡ í•«ìŠ¤íŒŸ)
        hotspot_score = int(gini * 100)
        
        return hotspot_score
    
    def check_thresholds_and_alert(self, shard_id, metrics):
        """ì„ê³„ê°’ ì²´í¬ ë° ì•Œë¦¼"""
        alerts = []
        
        # CPU ì„ê³„ê°’
        if metrics['cpu_usage'] > 80:
            alerts.append({
                'type': 'high_cpu',
                'shard_id': shard_id,
                'value': metrics['cpu_usage'],
                'threshold': 80,
                'severity': 'warning' if metrics['cpu_usage'] < 90 else 'critical'
            })
        
        # ë©”ëª¨ë¦¬ ì„ê³„ê°’
        if metrics['memory_usage'] > 85:
            alerts.append({
                'type': 'high_memory',
                'shard_id': shard_id,
                'value': metrics['memory_usage'],
                'threshold': 85,
                'severity': 'warning' if metrics['memory_usage'] < 95 else 'critical'
            })
        
        # ë³µì œ ì§€ì—°
        if metrics['replication_lag'] > 5:
            alerts.append({
                'type': 'replication_lag',
                'shard_id': shard_id,
                'value': metrics['replication_lag'],
                'threshold': 5,
                'severity': 'warning' if metrics['replication_lag'] < 30 else 'critical'
            })
        
        # í•«ìŠ¤íŒŸ ê°ì§€
        if metrics['hotspot_score'] > 70:
            alerts.append({
                'type': 'hotspot_detected',
                'shard_id': shard_id,
                'value': metrics['hotspot_score'],
                'threshold': 70,
                'severity': 'warning',
                'recommendation': 'Consider shard splitting or load rebalancing'
            })
        
        # ì•Œë¦¼ ë°œì†¡
        for alert in alerts:
            self.alert_manager.send_alert(alert)
    
    def auto_scaling_decision(self):
        """ìë™ ìŠ¤ì¼€ì¼ë§ ê²°ì •"""
        shard_metrics = self.monitor_shard_health()
        
        scaling_actions = []
        
        for shard_id, metrics in shard_metrics.items():
            # ìŠ¤ì¼€ì¼ ì•„ì›ƒ ê²°ì •
            if (metrics['cpu_usage'] > 80 and 
                metrics['memory_usage'] > 80 and
                metrics['hotspot_score'] > 80):
                
                scaling_actions.append({
                    'action': 'scale_out',
                    'shard_id': shard_id,
                    'reason': 'High load and hotspot detected',
                    'metrics': metrics
                })
            
            # ìŠ¤ì¼€ì¼ ì¸ ê³ ë ¤ (ëª¨ë“  ì§€í‘œê°€ ë‚®ì„ ë•Œ)
            elif (metrics['cpu_usage'] < 20 and 
                  metrics['memory_usage'] < 30 and
                  metrics['hotspot_score'] < 20):
                
                scaling_actions.append({
                    'action': 'scale_in_candidate',
                    'shard_id': shard_id,
                    'reason': 'Underutilized resources',
                    'metrics': metrics
                })
        
        return scaling_actions

# ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ì‚¬ìš©
monitor = DistributedDataMonitor()

# ì£¼ê¸°ì  í—¬ìŠ¤ ì²´í¬
health_report = monitor.monitor_shard_health()
print(f"Monitoring {len(health_report)} shards")

# ìë™ ìŠ¤ì¼€ì¼ë§ ì¶”ì²œ
scaling_recommendations = monitor.auto_scaling_decision()
for action in scaling_recommendations:
    print(f"Scaling recommendation: {action['action']} for {action['shard_id']}")
```

### ğŸ¯ ë°ì´í„° ì¼ê´€ì„± ë ˆë²¨ ì„ íƒ ì „ëµ

```python
class ConsistencyLevelManager:
    """ë°ì´í„° ìœ í˜•ë³„ ì¼ê´€ì„± ë ˆë²¨ ê´€ë¦¬"""
    
    def __init__(self):
        self.consistency_policies = {
            # ì¤‘ìš” ë¹„ì¦ˆë‹ˆìŠ¤ ë°ì´í„°
            'user_account': 'strong',
            'payment_info': 'strong',
            'user_balance': 'strong',
            
            # ì‚¬ìš©ì ìƒì„± ì»¨í…ì¸ 
            'user_posts': 'read_your_writes',
            'user_comments': 'read_your_writes',
            'user_likes': 'eventual',
            
            # ë¶„ì„/ì¶”ì²œ ë°ì´í„°
            'user_recommendations': 'eventual',
            'trending_topics': 'eventual',
            'user_analytics': 'eventual',
            
            # ìºì‹œ ë°ì´í„°
            'session_data': 'eventual',
            'temporary_data': 'weak'
        }
    
    def get_consistency_level(self, data_type, user_context=None):
        """ë°ì´í„° ìœ í˜•ê³¼ ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ì— ë”°ë¥¸ ì¼ê´€ì„± ë ˆë²¨ ê²°ì •"""
        base_level = self.consistency_policies.get(data_type, 'eventual')
        
        # íŠ¹ë³„í•œ ê²½ìš° ì¼ê´€ì„± ë ˆë²¨ ì¡°ì •
        if user_context:
            # VIP ì‚¬ìš©ìëŠ” ë” ê°•í•œ ì¼ê´€ì„±
            if user_context.get('is_vip', False):
                if base_level == 'eventual':
                    return 'read_your_writes'
                elif base_level == 'read_your_writes':
                    return 'strong'
            
            # ì‹¤ì‹œê°„ ê¸°ëŠ¥ ì‚¬ìš© ì¤‘
            if user_context.get('realtime_mode', False):
                return 'strong'
            
            # ëª¨ë°”ì¼ì—ì„œ ë°°í„°ë¦¬ ì ˆì•½ ëª¨ë“œ
            if user_context.get('battery_saving', False):
                if base_level == 'strong':
                    return 'read_your_writes'
                elif base_level == 'read_your_writes':
                    return 'eventual'
        
        return base_level
    
    def read_with_appropriate_consistency(self, data_type, key, user_context=None):
        """ì ì ˆí•œ ì¼ê´€ì„± ë ˆë²¨ë¡œ ë°ì´í„° ì½ê¸°"""
        consistency_level = self.get_consistency_level(data_type, user_context)
        
        print(f"Reading {data_type} with {consistency_level} consistency")
        
        # ì‹¤ì œ ë°ì´í„° ì½ê¸° ë¡œì§
        return self.distributed_read(key, consistency_level)

# ì‚¬ìš© ì˜ˆì‹œ
consistency_manager = ConsistencyLevelManager()

# ì¼ë°˜ ì‚¬ìš©ìì˜ í¬ìŠ¤íŠ¸ ì½ê¸°
post = consistency_manager.read_with_appropriate_consistency(
    'user_posts', 
    'post:12345',
    {'user_id': 'user789', 'is_vip': False}
)

# VIP ì‚¬ìš©ìì˜ ê²°ì œ ì •ë³´ ì½ê¸°
payment = consistency_manager.read_with_appropriate_consistency(
    'payment_info',
    'payment:user123',
    {'user_id': 'user123', 'is_vip': True, 'realtime_mode': True}
)
```

### âš¡ ì‹¤ì „ ë°ì´í„° ì´ì£¼ì™€ ë¦¬ë°¸ëŸ°ì‹±

```python
class DataMigrationManager:
    """ìš´ì˜ ì¤‘ ë°ì´í„° ì´ì£¼ ê´€ë¦¬"""
    
    def __init__(self):
        self.migration_status = {}
        self.throttle_config = {
            'max_operations_per_second': 1000,
            'max_bandwidth_mbps': 100,
            'maintenance_window_hours': [2, 6]  # ìƒˆë²½ 2-6ì‹œ
        }
    
    def plan_shard_split(self, overloaded_shard_id):
        """ìƒ¤ë“œ ë¶„í•  ê³„íš ìˆ˜ë¦½"""
        # 1. í˜„ì¬ ìƒ¤ë“œ ë¶„ì„
        shard_analysis = self.analyze_shard(overloaded_shard_id)
        
        migration_plan = {
            'source_shard': overloaded_shard_id,
            'target_shards': [
                self.create_new_shard_config(f"{overloaded_shard_id}_split1"),
                self.create_new_shard_config(f"{overloaded_shard_id}_split2")
            ],
            'migration_strategy': self.choose_migration_strategy(shard_analysis),
            'estimated_duration': self.estimate_migration_time(shard_analysis),
            'rollback_plan': self.create_rollback_plan(overloaded_shard_id)
        }
        
        return migration_plan
    
    def execute_live_migration(self, migration_plan):
        """ë¬´ì¤‘ë‹¨ ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰"""
        source_shard = migration_plan['source_shard']
        target_shards = migration_plan['target_shards']
        
        try:
            # Phase 1: ìƒˆ ìƒ¤ë“œ ì¤€ë¹„
            print("Phase 1: Setting up new shards")
            for target_shard in target_shards:
                self.setup_new_shard(target_shard)
            
            # Phase 2: ì´ˆê¸° ë°ì´í„° ë³µì‚¬ (ì½ê¸° ì „ìš©)
            print("Phase 2: Initial data copy (read-only)")
            self.copy_initial_data(source_shard, target_shards)
            
            # Phase 3: Delta ë™ê¸°í™”
            print("Phase 3: Delta synchronization")
            self.sync_delta_changes(source_shard, target_shards)
            
            # Phase 4: ì“°ê¸° íŠ¸ë˜í”½ ì „í™˜ (ë§¤ìš° ì§§ì€ ë‹¤ìš´íƒ€ì„)
            print("Phase 4: Write traffic cutover")
            with self.minimal_downtime_context():
                self.cutover_write_traffic(source_shard, target_shards)
            
            # Phase 5: ì½ê¸° íŠ¸ë˜í”½ ì „í™˜
            print("Phase 5: Read traffic cutover")
            self.cutover_read_traffic(source_shard, target_shards)
            
            # Phase 6: ê²€ì¦ ë° ì •ë¦¬
            print("Phase 6: Verification and cleanup")
            self.verify_migration_success(source_shard, target_shards)
            self.cleanup_old_shard(source_shard)
            
            print("âœ… Migration completed successfully")
            
        except Exception as e:
            print(f"ğŸš¨ Migration failed: {e}")
            self.rollback_migration(migration_plan)
            raise
    
    def copy_initial_data(self, source_shard, target_shards):
        """ì´ˆê¸° ë°ì´í„° ë³µì‚¬ (ìŠ¤ë¡œí‹€ë§ ì ìš©)"""
        source_conn = self.get_shard_connection(source_shard)
        
        # í…Œì´ë¸” ëª©ë¡ ì¡°íšŒ
        tables = source_conn.execute("SHOW TABLES")
        
        for table in tables:
            table_name = table[0]
            total_rows = source_conn.execute(f"SELECT COUNT(*) FROM {table_name}")[0][0]
            
            print(f"Migrating table {table_name} ({total_rows} rows)")
            
            # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë³µì‚¬
            batch_size = 1000
            offset = 0
            
            while offset < total_rows:
                # ìŠ¤ë¡œí‹€ë§ ì²´í¬
                self.throttle_if_needed()
                
                # ë°°ì¹˜ ë°ì´í„° ì½ê¸°
                batch_data = source_conn.execute(f"""
                    SELECT * FROM {table_name} 
                    ORDER BY id 
                    LIMIT {batch_size} OFFSET {offset}
                """)
                
                # ì ì ˆí•œ íƒ€ê²Ÿ ìƒ¤ë“œì— ì“°ê¸°
                for row in batch_data:
                    target_shard = self.determine_target_shard(row, target_shards)
                    target_conn = self.get_shard_connection(target_shard)
                    
                    target_conn.execute(f"""
                        INSERT INTO {table_name} VALUES (...)
                    """, row)
                
                offset += batch_size
                
                # ì§„í–‰ë¥  í‘œì‹œ
                progress = (offset / total_rows) * 100
                print(f"  Progress: {progress:.1f}%")
    
    def throttle_if_needed(self):
        """ë¶€í•˜ ì œì–´ë¥¼ ìœ„í•œ ìŠ¤ë¡œí‹€ë§"""
        current_hour = datetime.now().hour
        
        # ìœ ì§€ë³´ìˆ˜ ì‹œê°„ëŒ€ê°€ ì•„ë‹ˆë©´ ë” ë³´ìˆ˜ì ìœ¼ë¡œ
        if current_hour not in self.throttle_config['maintenance_window_hours']:
            time.sleep(0.1)  # 100ms ëŒ€ê¸°
        
        # ì‹œìŠ¤í…œ ë¶€í•˜ ì²´í¬
        source_cpu = self.get_system_cpu_usage()
        if source_cpu > 70:
            time.sleep(0.5)  # 500ms ëŒ€ê¸°
    
    def minimal_downtime_context(self):
        """ìµœì†Œ ë‹¤ìš´íƒ€ì„ ì»¨í…ìŠ¤íŠ¸"""
        class MinimalDowntimeContext:
            def __enter__(self):
                print("âš ï¸  Starting minimal downtime window")
                # ìƒˆë¡œìš´ ì“°ê¸° ìš”ì²­ ì°¨ë‹¨
                # ì§„í–‰ ì¤‘ì¸ íŠ¸ëœì­ì…˜ ì™„ë£Œ ëŒ€ê¸°
                return self
            
            def __exit__(self, exc_type, exc_val, exc_tb):
                print("âœ… Minimal downtime window completed")
                # ì“°ê¸° ìš”ì²­ ì¬ê°œ
        
        return MinimalDowntimeContext()
    
    def verify_migration_success(self, source_shard, target_shards):
        """ë§ˆì´ê·¸ë ˆì´ì…˜ ì„±ê³µ ê²€ì¦"""
        verification_results = {
            'data_integrity': True,
            'row_count_match': True,
            'checksum_match': True,
            'errors': []
        }
        
        source_conn = self.get_shard_connection(source_shard)
        
        for target_shard in target_shards:
            target_conn = self.get_shard_connection(target_shard)
            
            # ê° í…Œì´ë¸”ë³„ ê²€ì¦
            tables = source_conn.execute("SHOW TABLES")
            
            for table in tables:
                table_name = table[0]
                
                # í–‰ ìˆ˜ ë¹„êµ
                source_count = source_conn.execute(f"SELECT COUNT(*) FROM {table_name}")[0][0]
                target_count = target_conn.execute(f"SELECT COUNT(*) FROM {table_name}")[0][0]
                
                if source_count != target_count:
                    verification_results['row_count_match'] = False
                    verification_results['errors'].append(
                        f"Row count mismatch in {table_name}: source={source_count}, target={target_count}"
                    )
                
                # ì²´í¬ì„¬ ë¹„êµ (ìƒ˜í”Œë§)
                source_checksum = self.calculate_table_checksum(source_conn, table_name, sample_size=1000)
                target_checksum = self.calculate_table_checksum(target_conn, table_name, sample_size=1000)
                
                if source_checksum != target_checksum:
                    verification_results['checksum_match'] = False
                    verification_results['errors'].append(
                        f"Checksum mismatch in {table_name}"
                    )
        
        if not verification_results['data_integrity']:
            raise MigrationVerificationError(verification_results['errors'])
        
        return verification_results

# ì‹¤ì œ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰
migration_manager = DataMigrationManager()

# ê³¼ë¶€í•˜ ìƒ¤ë“œ ë¶„í• 
overloaded_shard = 'shard3-primary.db.company.com'
migration_plan = migration_manager.plan_shard_split(overloaded_shard)

print(f"Migration plan created:")
print(f"  Source: {migration_plan['source_shard']}")
print(f"  Targets: {[s['address'] for s in migration_plan['target_shards']]}")
print(f"  Estimated duration: {migration_plan['estimated_duration']}")

# ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰
migration_manager.execute_live_migration(migration_plan)
```

### ğŸ“ˆ ì„±ëŠ¥ ìµœì í™”ì™€ ìš´ì˜ ë…¸í•˜ìš°

```python
class DistributedDataOptimizer:
    """ë¶„ì‚° ë°ì´í„° ì‹œìŠ¤í…œ ì„±ëŠ¥ ìµœì í™”"""
    
    def __init__(self):
        self.performance_baselines = {}
        self.optimization_history = []
    
    def optimize_query_routing(self):
        """ì¿¼ë¦¬ ë¼ìš°íŒ… ìµœì í™”"""
        # 1. ì¿¼ë¦¬ íŒ¨í„´ ë¶„ì„
        query_patterns = self.analyze_query_patterns()
        
        optimizations = []
        
        for pattern in query_patterns:
            if pattern['type'] == 'range_scan':
                # ë²”ìœ„ ìŠ¤ìº”ì€ ë‹¨ì¼ ìƒ¤ë“œë¡œ ìœ ë„
                optimizations.append({
                    'pattern': pattern,
                    'optimization': 'single_shard_routing',
                    'expected_improvement': '70% latency reduction'
                })
                
            elif pattern['type'] == 'join_across_shards':
                # í¬ë¡œìŠ¤ ìƒ¤ë“œ ì¡°ì¸ ìµœì í™”
                optimizations.append({
                    'pattern': pattern,
                    'optimization': 'denormalization_or_caching',
                    'expected_improvement': '90% latency reduction'
                })
                
            elif pattern['type'] == 'hot_key_access':
                # í•« í‚¤ ì ‘ê·¼ ìµœì í™”
                optimizations.append({
                    'pattern': pattern,
                    'optimization': 'dedicated_cache_tier',
                    'expected_improvement': '95% latency reduction'
                })
        
        return optimizations
    
    def implement_smart_caching(self):
        """ìŠ¤ë§ˆíŠ¸ ìºì‹± ì „ëµ"""
        cache_strategies = {
            # ìì£¼ ì½íˆëŠ” ë°ì´í„°
            'frequent_reads': {
                'strategy': 'write_through_cache',
                'ttl': 3600,  # 1ì‹œê°„
                'consistency': 'eventual'
            },
            
            # ì—…ë°ì´íŠ¸ê°€ ë“œë¬¸ ë©”íƒ€ë°ì´í„°
            'metadata': {
                'strategy': 'write_behind_cache',
                'ttl': 86400,  # 24ì‹œê°„
                'consistency': 'eventual'
            },
            
            # ì‹¤ì‹œê°„ í•„ìš”í•œ ë°ì´í„°
            'realtime_data': {
                'strategy': 'write_around_cache',
                'ttl': 60,  # 1ë¶„
                'consistency': 'strong'
            }
        }
        
        return cache_strategies
    
    def connection_pool_optimization(self):
        """ì»¤ë„¥ì…˜ í’€ ìµœì í™”"""
        optimal_configs = {}
        
        for shard_id in self.get_all_shard_ids():
            shard_metrics = self.get_shard_metrics(shard_id)
            
            # ë™ì  ì»¤ë„¥ì…˜ í’€ í¬ê¸° ê³„ì‚°
            avg_concurrent_queries = shard_metrics['avg_concurrent_queries']
            peak_concurrent_queries = shard_metrics['peak_concurrent_queries']
            
            optimal_pool_size = min(
                max(avg_concurrent_queries * 2, 10),  # ìµœì†Œ 10
                peak_concurrent_queries * 1.2        # í”¼í¬ì˜ 120%
            )
            
            optimal_configs[shard_id] = {
                'pool_size': int(optimal_pool_size),
                'idle_timeout': 300,  # 5ë¶„
                'max_lifetime': 3600, # 1ì‹œê°„
                'validation_query': 'SELECT 1'
            }
        
        return optimal_configs
    
    def monitoring_and_alerting_setup(self):
        """ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼ ì„¤ì •"""
        monitoring_config = {
            'key_metrics': [
                {
                    'name': 'shard_cpu_usage',
                    'threshold': 80,
                    'action': 'scale_out_evaluation'
                },
                {
                    'name': 'replication_lag_seconds',
                    'threshold': 10,
                    'action': 'investigate_replication'
                },
                {
                    'name': 'query_latency_p99_ms',
                    'threshold': 1000,
                    'action': 'query_optimization_needed'
                },
                {
                    'name': 'connection_pool_exhaustion_rate',
                    'threshold': 0.1,
                    'action': 'increase_pool_size'
                }
            ],
            
            'dashboard_panels': [
                'shard_distribution_map',
                'query_latency_heatmap',
                'replication_topology',
                'data_growth_trends'
            ]
        }
        
        return monitoring_config

# ìµœì í™” ì‹¤í–‰
optimizer = DistributedDataOptimizer()

# ì¿¼ë¦¬ ë¼ìš°íŒ… ìµœì í™”
query_optimizations = optimizer.optimize_query_routing()
print(f"Found {len(query_optimizations)} query optimization opportunities")

# ìºì‹± ì „ëµ ìˆ˜ë¦½
cache_strategies = optimizer.implement_smart_caching()
print(f"Smart caching strategies configured for {len(cache_strategies)} data types")

# ì»¤ë„¥ì…˜ í’€ ìµœì í™”
pool_configs = optimizer.connection_pool_optimization()
print(f"Optimized connection pool configs for {len(pool_configs)} shards")
```

## ğŸ¯ ì‹¤ì „ ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤

### 1. ì ì§„ì  ë„ì… ì „ëµ

```python
def gradual_migration_strategy():
    phases = [
        {
            'phase': 1,
            'description': 'Read Replica ë„ì…',
            'duration': '2ì£¼',
            'risk': 'Low',
            'rollback': 'Easy'
        },
        {
            'phase': 2,
            'description': 'Vertical Sharding (í…Œì´ë¸”ë³„ ë¶„ë¦¬)',
            'duration': '1ê°œì›”',
            'risk': 'Medium',
            'rollback': 'Moderate'
        },
        {
            'phase': 3,
            'description': 'Horizontal Sharding (ì‚¬ìš©ìID ê¸°ë°˜)',
            'duration': '2ê°œì›”',
            'risk': 'High',
            'rollback': 'Difficult'
        },
        {
            'phase': 4,
            'description': 'Consistent Hashing ë„ì…',
            'duration': '1ê°œì›”',
            'risk': 'Medium',
            'rollback': 'Moderate'
        }
    ]
    
    return phases
```

### 2. ì¥ì•  ëŒ€ì‘ í”Œë ˆì´ë¶

```bash
ğŸ“š ë¶„ì‚° ë°ì´í„° ì‹œìŠ¤í…œ í•„ìˆ˜ ì§€í‘œ:
- ìƒ¤ë“œë³„ ë¶€í•˜ ë¶„ì‚°
- ë³µì œ ì§€ì—° ì‹œê°„  
- ì¶©ëŒ ë°œìƒ ë¹ˆë„
- ë°ì´í„° ì´ë™ ì§„í–‰ë¥ 
- ë…¸ë“œ í—¬ìŠ¤ì²´í¬ ìƒíƒœ
```

### 3. ìš´ì˜ ì²´í¬ë¦¬ìŠ¤íŠ¸

```python
operational_checklist = {
    'daily': [
        'ìƒ¤ë“œë³„ ì„±ëŠ¥ ë©”íŠ¸ë¦­ í™•ì¸',
        'ë³µì œ ì§€ì—° ìƒíƒœ ì ê²€',
        'ë°±ì—… ì‘ì—… ì„±ê³µ ì—¬ë¶€ í™•ì¸',
        'ì•Œë¦¼ ë° ì´ìƒ ì§•í›„ ê²€í† '
    ],
    
    'weekly': [
        'ë°ì´í„° ì¦ê°€ìœ¨ ë¶„ì„',
        'ì¿¼ë¦¬ ì„±ëŠ¥ íŠ¸ë Œë“œ ë¶„ì„',
        'ì»¤ë„¥ì…˜ í’€ ì‚¬ìš©ë¥  ìµœì í™”',
        'ìºì‹œ íˆíŠ¸ìœ¨ ë° íš¨ìœ¨ì„± ê²€í† '
    ],
    
    'monthly': [
        'ìƒ¤ë“œ ë¦¬ë°¸ëŸ°ì‹± í•„ìš”ì„± í‰ê°€',
        'ìŠ¤ì¼€ì¼ë§ ê³„íš ìˆ˜ë¦½',
        'ì¬í•´ ë³µêµ¬ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸',
        'ì„±ëŠ¥ ê¸°ì¤€ì„  ì—…ë°ì´íŠ¸'
    ]
}
```

## ğŸ¯ ë‹¤ìŒ ë‹¨ê³„

ë¶„ì‚° ë°ì´í„° ê´€ë¦¬ì˜ ê¸°ë°˜ì„ ë‹¤ì¡Œìœ¼ë‹ˆ, [14.4 ë¶„ì‚° ì‹œìŠ¤í…œ íŒ¨í„´](04-distributed-patterns.md)ì—ì„œëŠ” Circuit Breaker, Saga, CQRS ê°™ì€ ì‹¤ì „ íŒ¨í„´ë“¤ì„ ë°°ì›Œë³´ê² ìŠµë‹ˆë‹¤.

"ë°ì´í„°ë¥¼ ë‚˜ëˆ„ë©´ ë³µì¡í•´ì§€ì§€ë§Œ, ì˜¬ë°”ë¥¸ ì „ëµê³¼ ë„êµ¬ê°€ ìˆìœ¼ë©´ í™•ì¥ ê°€ëŠ¥í•œ ì‹œìŠ¤í…œì„ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤!" ğŸ—„ï¸âš¡

## í•µì‹¬ ìš”ì 

### 1. ì‹¤ìš©ì£¼ì˜ì  ì ‘ê·¼

ì™„ë²½í•œ ì¼ê´€ì„±ë³´ë‹¤ëŠ” ë¹„ì¦ˆë‹ˆìŠ¤ ìš”êµ¬ì‚¬í•­ì— ë§ëŠ” ì ì ˆí•œ íŠ¸ë ˆì´ë“œì˜¤í”„ ì„ íƒ

### 2. ì ì§„ì  ì§„í™”

í•œ ë²ˆì— ëª¨ë“  ê²ƒì„ ë°”ê¾¸ë ¤ í•˜ì§€ ë§ê³  ë‹¨ê³„ì ìœ¼ë¡œ ë¶„ì‚° ì‹œìŠ¤í…œìœ¼ë¡œ ì§„í™”

### 3. ê´€ì°° ê°€ëŠ¥ì„± ìš°ì„ 

ë¶„ì‚° ì‹œìŠ¤í…œì€ ë³µì¡í•˜ë¯€ë¡œ ëª¨ë‹ˆí„°ë§ê³¼ ë””ë²„ê¹… ë„êµ¬ê°€ í•„ìˆ˜

---

**ì´ì „**: [Vector Clockê³¼ ì¶©ëŒ í•´ê²°](03c-vector-clocks.md)  
**ë‹¤ìŒ**: [ë¶„ì‚° ì‹œìŠ¤í…œ íŒ¨í„´](04-distributed-patterns.md)ì—ì„œ ê³ ê¸‰ ë¶„ì‚° ì‹œìŠ¤í…œ íŒ¨í„´ë“¤ì„ í•™ìŠµí•©ë‹ˆë‹¤.
