---
tags:
  - advanced
  - cqrs
  - deep-study
  - distributed-systems
  - event-sourcing
  - event-streaming
  - hands-on
  - kafka
  - ì¸í”„ë¼ìŠ¤íŠ¸ëŸ­ì²˜
difficulty: ADVANCED
learning_time: "12-16ì‹œê°„"
main_topic: "ì¸í”„ë¼ìŠ¤íŠ¸ëŸ­ì²˜"
priority_score: 4
---

# 14.4.5: ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¬ë°ê³¼ ì†Œì‹±

## ğŸŒŠ Event Streaming ë°©ì‹ (Apache Kafka)

**íŠ¹ì§•**: ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¼ì„ ì—¬ëŸ¬ ì†Œë¹„ìê°€ ë…ë¦½ì ìœ¼ë¡œ ì†Œë¹„

```python
from kafka import KafkaProducer, KafkaConsumer
import json
import threading
import time
from typing import Dict, List

class KafkaEventStream:
    """Kafka ê¸°ë°˜ ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¬ë°"""
    
    def __init__(self, bootstrap_servers=['localhost:9092']):
        self.bootstrap_servers = bootstrap_servers
        self.producer = None
        self.consumers = {}
        self.connect()
    
    def connect(self):
        """Kafka í”„ë¡œë“€ì„œ ì—°ê²°"""
        try:
            self.producer = KafkaProducer(
                bootstrap_servers=self.bootstrap_servers,
                value_serializer=lambda v: json.dumps(v).encode('utf-8'),
                key_serializer=lambda k: k.encode('utf-8') if k else None,
                acks='all',  # ëª¨ë“  ë³µì œë³¸ì— ì“°ê¸° í™•ì¸
                retries=3,
                batch_size=16384,
                linger_ms=10
            )
            print("ğŸš€ Connected to Kafka")
        except Exception as e:
            print(f"âš ï¸  Kafka not available: {e}")
            self.producer = None
    
    def publish(self, topic: str, event_data: Dict, key: str = None):
        """ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¼ì— ë°œí–‰"""
        if not self.producer:
            print(f"ğŸ“¢ [MOCK] Publishing to {topic}: {event_data['event_type']}")
            return
        
        message = {
            'event_type': event_data.get('event_type', 'unknown'),
            'payload': event_data.get('payload', {}),
            'timestamp': time.time()
        }
        
        try:
            # ë¹„ë™ê¸° ì „ì†¡
            future = self.producer.send(
                topic=topic,
                value=message,
                key=key
            )
            
            # ì „ì†¡ í™•ì¸ (ì„ íƒì )
            record_metadata = future.get(timeout=10)
            print(f"ğŸ“¢ Event published to {topic} (partition: {record_metadata.partition}, offset: {record_metadata.offset})")
            
        except Exception as e:
            print(f"âŒ Failed to publish event: {e}")
    
    def subscribe(self, topics: List[str], group_id: str, handler):
        """ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¼ êµ¬ë…"""
        if not self.producer:  # Kafka ì‚¬ìš© ë¶ˆê°€ì‹œ Mock
            print(f"ğŸ” [MOCK] Subscribed to topics {topics} with group {group_id}")
            return
        
        try:
            consumer = KafkaConsumer(
                *topics,
                bootstrap_servers=self.bootstrap_servers,
                group_id=group_id,
                value_deserializer=lambda m: json.loads(m.decode('utf-8')),
                key_deserializer=lambda k: k.decode('utf-8') if k else None,
                auto_offset_reset='latest',  # ìµœì‹  ë©”ì‹œì§€ë¶€í„°
                enable_auto_commit=True
            )
            
            self.consumers[group_id] = consumer
            
            def consume_messages():
                print(f"ğŸ” Starting consumer {group_id} for topics {topics}")
                
                for message in consumer:
                    try:
                        print(f"ğŸ“¨ [{group_id}] Received from {message.topic}: {message.value['event_type']}")
                        
                        # í•¸ë“¤ëŸ¬ ì‹¤í–‰
                        handler(message.value, message.topic, message.partition, message.offset)
                        
                    except Exception as e:
                        print(f"âŒ [{group_id}] Handler error: {e}")
            
            # ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ë©”ì‹œì§€ ì†Œë¹„
            consumer_thread = threading.Thread(target=consume_messages, daemon=True)
            consumer_thread.start()
            
        except Exception as e:
            print(f"âŒ Failed to create consumer {group_id}: {e}")

# Kafkaë¥¼ ì‚¬ìš©í•œ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° í”Œë«í¼
class StreamingVideoService:
    """ìŠ¤íŠ¸ë¦¬ë° ë¹„ë””ì˜¤ ì„œë¹„ìŠ¤"""
    
    def __init__(self, event_stream: KafkaEventStream):
        self.event_stream = event_stream
        self.viewing_sessions = {}
    
    def start_streaming(self, user_id: str, video_id: str, quality: str = "1080p"):
        """ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘"""
        session_id = f"stream_{user_id}_{int(time.time())}"
        
        session_data = {
            'user_id': user_id,
            'video_id': video_id,
            'quality': quality,
            'start_time': time.time(),
            'session_id': session_id
        }
        
        self.viewing_sessions[session_id] = session_data
        
        print(f"ğŸ¥ Streaming started: {video_id} for user {user_id} in {quality}")
        
        # ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘ ì´ë²¤íŠ¸ ë°œí–‰ (íŒŒí‹°ì…”ë‹ í‚¤: user_id)
        self.event_stream.publish(
            topic='video-events',
            event_data={
                'event_type': 'streaming.started',
                'payload': session_data
            },
            key=user_id  # ê°™ì€ ì‚¬ìš©ìì˜ ì´ë²¤íŠ¸ëŠ” ê°™ì€ íŒŒí‹°ì…˜ìœ¼ë¡œ
        )
        
        return {'status': 'success', 'session_id': session_id}
    
    def update_quality(self, session_id: str, new_quality: str):
        """ìŠ¤íŠ¸ë¦¬ë° í’ˆì§ˆ ë³€ê²½"""
        if session_id not in self.viewing_sessions:
            raise ValueError(f"Session {session_id} not found")
        
        session = self.viewing_sessions[session_id]
        old_quality = session['quality']
        session['quality'] = new_quality
        
        print(f"ğŸ“Š Quality changed: {old_quality} â†’ {new_quality} for session {session_id}")
        
        # í’ˆì§ˆ ë³€ê²½ ì´ë²¤íŠ¸ ë°œí–‰
        self.event_stream.publish(
            topic='video-events',
            event_data={
                'event_type': 'streaming.quality_changed',
                'payload': {
                    'session_id': session_id,
                    'user_id': session['user_id'],
                    'old_quality': old_quality,
                    'new_quality': new_quality
                }
            },
            key=session['user_id']
        )
    
    def send_heartbeat(self, session_id: str):
        """í•˜íŠ¸ë¹„íŠ¸ ì „ì†¡ (ì‹œì²­ ì§„í–‰ ìƒí™©)"""
        if session_id not in self.viewing_sessions:
            return
        
        session = self.viewing_sessions[session_id]
        
        # í•˜íŠ¸ë¹„íŠ¸ ì´ë²¤íŠ¸ ë°œí–‰ (ê³ ë¹ˆë„)
        self.event_stream.publish(
            topic='video-heartbeat',  # ë³„ë„ í† í”½ (ê³ ë¹ˆë„ ì´ë²¤íŠ¸)
            event_data={
                'event_type': 'streaming.heartbeat',
                'payload': {
                    'session_id': session_id,
                    'user_id': session['user_id'],
                    'current_time': time.time() - session['start_time']
                }
            },
            key=session['user_id']
        )

class RealTimeAnalyticsService:
    """ì‹¤ì‹œê°„ ë¶„ì„ ì„œë¹„ìŠ¤"""
    
    def __init__(self, event_stream: KafkaEventStream):
        self.event_stream = event_stream
        self.metrics = {
            'concurrent_viewers': 0,
            'quality_distribution': {},
            'user_sessions': {}
        }
        
        # ì—¬ëŸ¬ í† í”½ êµ¬ë…
        self.event_stream.subscribe(
            topics=['video-events', 'video-heartbeat'],
            group_id='realtime_analytics',
            handler=self.process_event
        )
    
    def process_event(self, message, topic, partition, offset):
        """ì‹¤ì‹œê°„ ì´ë²¤íŠ¸ ì²˜ë¦¬"""
        event_type = message['event_type']
        payload = message['payload']
        
        if event_type == 'streaming.started':
            self._handle_streaming_started(payload)
        elif event_type == 'streaming.quality_changed':
            self._handle_quality_changed(payload)
        elif event_type == 'streaming.heartbeat':
            self._handle_heartbeat(payload)
    
    def _handle_streaming_started(self, payload):
        """ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘ ì²˜ë¦¬"""
        self.metrics['concurrent_viewers'] += 1
        
        quality = payload['quality']
        if quality not in self.metrics['quality_distribution']:
            self.metrics['quality_distribution'][quality] = 0
        self.metrics['quality_distribution'][quality] += 1
        
        self.metrics['user_sessions'][payload['session_id']] = payload
        
        print(f"ğŸ“Š Analytics: Concurrent viewers = {self.metrics['concurrent_viewers']}")
    
    def _handle_quality_changed(self, payload):
        """í’ˆì§ˆ ë³€ê²½ ì²˜ë¦¬"""
        old_quality = payload['old_quality']
        new_quality = payload['new_quality']
        
        # í’ˆì§ˆ ë¶„í¬ ì—…ë°ì´íŠ¸
        self.metrics['quality_distribution'][old_quality] -= 1
        if new_quality not in self.metrics['quality_distribution']:
            self.metrics['quality_distribution'][new_quality] = 0
        self.metrics['quality_distribution'][new_quality] += 1
        
        print(f"ğŸ“Š Quality distribution updated: {self.metrics['quality_distribution']}")
    
    def _handle_heartbeat(self, payload):
        """í•˜íŠ¸ë¹„íŠ¸ ì²˜ë¦¬ (ì„¸ì…˜ í™œì„±ë„ ì¶”ì )"""
        session_id = payload['session_id']
        if session_id in self.metrics['user_sessions']:
            self.metrics['user_sessions'][session_id]['last_heartbeat'] = time.time()
    
    def get_real_time_metrics(self):
        """ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ ì¡°íšŒ"""
        # ë¹„í™œì„± ì„¸ì…˜ ì •ë¦¬ (ë§ˆì§€ë§‰ í•˜íŠ¸ë¹„íŠ¸ í›„ 30ì´ˆ ê²½ê³¼)
        current_time = time.time()
        inactive_sessions = []
        
        for session_id, session_data in self.metrics['user_sessions'].items():
            if current_time - session_data.get('last_heartbeat', 0) > 30:
                inactive_sessions.append(session_id)
        
        for session_id in inactive_sessions:
            del self.metrics['user_sessions'][session_id]
            self.metrics['concurrent_viewers'] -= 1
        
        return self.metrics.copy()

class PersonalizationService:
    """ê°œì¸í™” ì„œë¹„ìŠ¤ (ML ê¸°ë°˜)"""
    
    def __init__(self, event_stream: KafkaEventStream):
        self.event_stream = event_stream
        self.user_profiles = {}
        
        # ì´ë²¤íŠ¸ êµ¬ë… (ë³„ë„ ì»¨ìŠˆë¨¸ ê·¸ë£¹)
        self.event_stream.subscribe(
            topics=['video-events'],
            group_id='personalization_ml',
            handler=self.update_user_profile
        )
    
    def update_user_profile(self, message, topic, partition, offset):
        """ì‚¬ìš©ì í”„ë¡œí•„ ì—…ë°ì´íŠ¸"""
        event_type = message['event_type']
        
        if event_type == 'streaming.started':
            payload = message['payload']
            user_id = payload['user_id']
            video_id = payload['video_id']
            quality = payload['quality']
            
            # ì‚¬ìš©ì í”„ë¡œí•„ ì—…ë°ì´íŠ¸
            if user_id not in self.user_profiles:
                self.user_profiles[user_id] = {
                    'watched_videos': [],
                    'preferred_quality': '1080p',
                    'viewing_hours': 0
                }
            
            profile = self.user_profiles[user_id]
            profile['watched_videos'].append(video_id)
            profile['preferred_quality'] = quality  # ìµœê·¼ ì„ íƒí•œ í’ˆì§ˆë¡œ ì—…ë°ì´íŠ¸
            
            # ML ëª¨ë¸ ì—…ë°ì´íŠ¸ (ì‹œë®¬ë ˆì´ì…˜)
            print(f"ğŸ¤– Updating ML model for user {user_id} (watched: {len(profile['watched_videos'])} videos)")
            
            # ê°œì¸í™”ëœ ì¶”ì²œ ìƒì„± í›„ ì´ë²¤íŠ¸ ë°œí–‰
            recommendations = self.generate_recommendations(user_id)
            
            self.event_stream.publish(
                topic='recommendation-events',
                event_data={
                    'event_type': 'recommendation.updated',
                    'payload': {
                        'user_id': user_id,
                        'recommendations': recommendations,
                        'model_version': 'v2.1'
                    }
                },
                key=user_id
            )
    
    def generate_recommendations(self, user_id: str) -> List[str]:
        """ê°œì¸í™”ëœ ì¶”ì²œ ìƒì„±"""
        profile = self.user_profiles.get(user_id, {})
        watched_count = len(profile.get('watched_videos', []))
        
        # ê°„ë‹¨í•œ ì¶”ì²œ ë¡œì§
        if watched_count > 5:
            return ['Premium Content A', 'Premium Content B', 'Premium Content C']
        else:
            return ['Popular Content 1', 'Popular Content 2', 'Popular Content 3']

# Event Streaming ì‹œë®¬ë ˆì´ì…˜
def simulate_event_streaming():
    print("=== Event Streaming (Kafka) ì‹œë®¬ë ˆì´ì…˜ ===")
    
    # Kafka ì—°ê²°
    event_stream = KafkaEventStream()
    
    # ì„œë¹„ìŠ¤ë“¤ ìƒì„±
    video_service = StreamingVideoService(event_stream)
    analytics_service = RealTimeAnalyticsService(event_stream)
    personalization_service = PersonalizationService(event_stream)
    
    print("\n--- ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘ ---")
    
    # ì—¬ëŸ¬ ì‚¬ìš©ìê°€ ë™ì‹œ ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘
    users = [
        ('alice', 'video_marvel_endgame', '4K'),
        ('bob', 'video_dark_knight', '1080p'),
        ('carol', 'video_inception', '720p'),
        ('david', 'video_matrix', '1080p')
    ]
    
    sessions = []
    for user_id, video_id, quality in users:
        result = video_service.start_streaming(user_id, video_id, quality)
        sessions.append(result['session_id'])
        time.sleep(0.1)  # ì•½ê°„ì˜ ì‹œê°„ì°¨
    
    print("\n--- ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì´ë²¤íŠ¸ë“¤ ---")
    
    # í’ˆì§ˆ ë³€ê²½
    video_service.update_quality(sessions[0], '1080p')  # Alice: 4K â†’ 1080p
    video_service.update_quality(sessions[2], '1080p')  # Carol: 720p â†’ 1080p
    
    # í•˜íŠ¸ë¹„íŠ¸ ì „ì†¡ (ì‹œë®¬ë ˆì´ì…˜)
    for _ in range(3):
        for session_id in sessions:
            video_service.send_heartbeat(session_id)
        time.sleep(1)
    
    print("\n--- ì´ë²¤íŠ¸ ì²˜ë¦¬ ëŒ€ê¸° ---")
    time.sleep(2)
    
    print("\n--- ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ í™•ì¸ ---")
    metrics = analytics_service.get_real_time_metrics()
    print(f"ë™ì‹œ ì‹œì²­ì: {metrics['concurrent_viewers']}ëª…")
    print(f"í’ˆì§ˆ ë¶„í¬: {metrics['quality_distribution']}")
    print(f"ê°œì¸í™” í”„ë¡œí•„: {len(personalization_service.user_profiles)}ëª…")

# ì‹¤í–‰
simulate_event_streaming()
```

## ğŸ“Š Event Sourcing: ëª¨ë“  ë³€ê²½ì„ ì´ë²¤íŠ¸ë¡œ ì €ì¥

### ğŸ—ƒï¸ Event Store êµ¬í˜„

ëª¨ë“  ë°ì´í„° ë³€ê²½ì„ ì´ë²¤íŠ¸ë¡œ ì €ì¥í•˜ì—¬ ì™„ì „í•œ ê°ì‚¬ ì¶”ì ê³¼ ì‹œì ë³„ ìƒíƒœ ë³µì›ì„ ì œê³µ:

```python
from dataclasses import dataclass
from datetime import datetime
from typing import List, Dict, Any, Optional
import json
import threading

@dataclass
class DomainEvent:
    """ë„ë©”ì¸ ì´ë²¤íŠ¸ ê¸°ë³¸ í´ë˜ìŠ¤"""
    aggregate_id: str
    event_type: str
    event_data: Dict[str, Any]
    event_version: int
    timestamp: datetime
    correlation_id: str = None
    causation_id: str = None

class EventStore:
    """ì´ë²¤íŠ¸ ì €ì¥ì†Œ"""
    
    def __init__(self):
        self.events: Dict[str, List[DomainEvent]] = {}  # aggregate_id -> events
        self.global_sequence = 0
        self.snapshots: Dict[str, Dict] = {}  # aggregate_id -> snapshot
        self.lock = threading.RLock()
        self.subscribers = []
    
    def append_events(self, aggregate_id: str, expected_version: int, events: List[DomainEvent]):
        """ì´ë²¤íŠ¸ ì¶”ê°€ (ë‚™ê´€ì  ë™ì‹œì„± ì œì–´)"""
        with self.lock:
            current_events = self.events.get(aggregate_id, [])
            current_version = len(current_events)
            
            # ë²„ì „ ì¶©ëŒ ê²€ì‚¬
            if expected_version != current_version:
                raise ConcurrencyException(
                    f"Expected version {expected_version}, but current version is {current_version}"
                )
            
            # ì´ë²¤íŠ¸ ì¶”ê°€
            if aggregate_id not in self.events:
                self.events[aggregate_id] = []
            
            for i, event in enumerate(events):
                event.event_version = current_version + i + 1
                self.events[aggregate_id].append(event)
                self.global_sequence += 1
                
                print(f"ğŸ“ Event stored: {event.event_type} v{event.event_version} for {aggregate_id}")
            
            # êµ¬ë…ìë“¤ì—ê²Œ ì•Œë¦¼
            for event in events:
                self._notify_subscribers(event)
    
    def get_events(self, aggregate_id: str, from_version: int = 0) -> List[DomainEvent]:
        """íŠ¹ì • Aggregateì˜ ì´ë²¤íŠ¸ ì¡°íšŒ"""
        events = self.events.get(aggregate_id, [])
        return [e for e in events if e.event_version > from_version]
    
    def save_snapshot(self, aggregate_id: str, snapshot: Dict[str, Any], version: int):
        """ìŠ¤ëƒ…ìƒ· ì €ì¥ (ì„±ëŠ¥ ìµœì í™”)"""
        self.snapshots[aggregate_id] = {
            'data': snapshot,
            'version': version,
            'timestamp': datetime.now()
        }
        print(f"ğŸ“¸ Snapshot saved for {aggregate_id} at version {version}")
    
    def get_snapshot(self, aggregate_id: str) -> Optional[Dict]:
        """ìŠ¤ëƒ…ìƒ· ì¡°íšŒ"""
        return self.snapshots.get(aggregate_id)
    
    def subscribe(self, handler):
        """ì´ë²¤íŠ¸ êµ¬ë…"""
        self.subscribers.append(handler)
    
    def _notify_subscribers(self, event: DomainEvent):
        """êµ¬ë…ìë“¤ì—ê²Œ ì´ë²¤íŠ¸ ì•Œë¦¼"""
        for subscriber in self.subscribers:
            try:
                threading.Thread(
                    target=subscriber,
                    args=(event,),
                    daemon=True
                ).start()
            except Exception as e:
                print(f"âŒ Subscriber notification failed: {e}")

class ConcurrencyException(Exception):
    """ë™ì‹œì„± ì¶©ëŒ ì˜ˆì™¸"""
    pass

# Event Sourcingì„ ì‚¬ìš©í•œ ì€í–‰ ê³„ì¢Œ ë„ë©”ì¸
class BankAccount:
    """ì€í–‰ ê³„ì¢Œ Aggregate"""
    
    def __init__(self, account_id: str):
        self.account_id = account_id
        self.balance = 0.0
        self.is_active = True
        self.overdraft_limit = 0.0
        self.version = 0
        self.uncommitted_events: List[DomainEvent] = []
    
    @classmethod
    def create_account(cls, account_id: str, initial_deposit: float, overdraft_limit: float = 0.0):
        """ìƒˆ ê³„ì¢Œ ìƒì„±"""
        account = cls(account_id)
        
        # ê³„ì¢Œ ìƒì„± ì´ë²¤íŠ¸ ì¶”ê°€
        event = DomainEvent(
            aggregate_id=account_id,
            event_type="AccountCreated",
            event_data={
                'initial_deposit': initial_deposit,
                'overdraft_limit': overdraft_limit
            },
            event_version=0,
            timestamp=datetime.now()
        )
        
        account._apply_event(event)
        return account
    
    def deposit(self, amount: float, description: str = ""):
        """ì…ê¸ˆ"""
        if amount <= 0:
            raise ValueError("Deposit amount must be positive")
        
        event = DomainEvent(
            aggregate_id=self.account_id,
            event_type="MoneyDeposited",
            event_data={
                'amount': amount,
                'description': description,
                'balance_before': self.balance
            },
            event_version=0,  # ì €ì¥ì‹œ ì„¤ì •ë¨
            timestamp=datetime.now()
        )
        
        self._apply_event(event)
    
    def withdraw(self, amount: float, description: str = ""):
        """ì¶œê¸ˆ"""
        if amount <= 0:
            raise ValueError("Withdrawal amount must be positive")
        
        # ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§: ì”ê³  + ë§ˆì´ë„ˆìŠ¤ í•œë„ í™•ì¸
        available_balance = self.balance + self.overdraft_limit
        if amount > available_balance:
            raise InsufficientFundsException(
                f"Insufficient funds. Available: {available_balance}, Requested: {amount}"
            )
        
        event = DomainEvent(
            aggregate_id=self.account_id,
            event_type="MoneyWithdrawn",
            event_data={
                'amount': amount,
                'description': description,
                'balance_before': self.balance
            },
            event_version=0,
            timestamp=datetime.now()
        )
        
        self._apply_event(event)
    
    def close_account(self):
        """ê³„ì¢Œ í•´ì§€"""
        if not self.is_active:
            raise ValueError("Account is already closed")
        
        if self.balance < 0:
            raise ValueError("Cannot close account with negative balance")
        
        event = DomainEvent(
            aggregate_id=self.account_id,
            event_type="AccountClosed",
            event_data={
                'final_balance': self.balance
            },
            event_version=0,
            timestamp=datetime.now()
        )
        
        self._apply_event(event)
    
    def _apply_event(self, event: DomainEvent):
        """ì´ë²¤íŠ¸ ì ìš© (ìƒíƒœ ë³€ê²½)"""
        if event.event_type == "AccountCreated":
            self.balance = event.event_data['initial_deposit']
            self.overdraft_limit = event.event_data['overdraft_limit']
            self.is_active = True
            
        elif event.event_type == "MoneyDeposited":
            self.balance += event.event_data['amount']
            
        elif event.event_type == "MoneyWithdrawn":
            self.balance -= event.event_data['amount']
            
        elif event.event_type == "AccountClosed":
            self.is_active = False
        
        self.uncommitted_events.append(event)
    
    def mark_events_as_committed(self):
        """ì´ë²¤íŠ¸ ì»¤ë°‹ í‘œì‹œ"""
        self.version += len(self.uncommitted_events)
        self.uncommitted_events.clear()
    
    def get_uncommitted_events(self) -> List[DomainEvent]:
        """ë¯¸ì»¤ë°‹ ì´ë²¤íŠ¸ ì¡°íšŒ"""
        return self.uncommitted_events.copy()
    
    @classmethod
    def from_history(cls, account_id: str, events: List[DomainEvent]):
        """ì´ë²¤íŠ¸ íˆìŠ¤í† ë¦¬ë¡œë¶€í„° Aggregate ì¬êµ¬ì„±"""
        account = cls(account_id)
        
        for event in events:
            account._apply_event_without_recording(event)
        
        account.version = len(events)
        account.uncommitted_events.clear()
        return account
    
    def _apply_event_without_recording(self, event: DomainEvent):
        """ì´ë²¤íŠ¸ ì ìš© (ê¸°ë¡ ì—†ì´ - íˆìŠ¤í† ë¦¬ ì¬êµ¬ì„±ìš©)"""
        if event.event_type == "AccountCreated":
            self.balance = event.event_data['initial_deposit']
            self.overdraft_limit = event.event_data['overdraft_limit']
            self.is_active = True
            
        elif event.event_type == "MoneyDeposited":
            self.balance += event.event_data['amount']
            
        elif event.event_type == "MoneyWithdrawn":
            self.balance -= event.event_data['amount']
            
        elif event.event_type == "AccountClosed":
            self.is_active = False

class InsufficientFundsException(Exception):
    pass

# Repository íŒ¨í„´ with Event Sourcing
class EventSourcedAccountRepository:
    """Event Sourcing ê¸°ë°˜ ê³„ì¢Œ ë¦¬í¬ì§€í† ë¦¬"""
    
    def __init__(self, event_store: EventStore):
        self.event_store = event_store
    
    def save(self, account: BankAccount):
        """ê³„ì¢Œ ì €ì¥ (ì´ë²¤íŠ¸ ì €ì¥)"""
        uncommitted_events = account.get_uncommitted_events()
        
        if uncommitted_events:
            self.event_store.append_events(
                account.account_id,
                account.version,
                uncommitted_events
            )
            account.mark_events_as_committed()
    
    def get_by_id(self, account_id: str) -> Optional[BankAccount]:
        """IDë¡œ ê³„ì¢Œ ì¡°íšŒ (ì´ë²¤íŠ¸ ì¬êµ¬ì„±)"""
        # ìŠ¤ëƒ…ìƒ· í™•ì¸
        snapshot = self.event_store.get_snapshot(account_id)
        
        if snapshot:
            # ìŠ¤ëƒ…ìƒ·ì´ ìˆìœ¼ë©´ ìŠ¤ëƒ…ìƒ· ì´í›„ ì´ë²¤íŠ¸ë§Œ ì¡°íšŒ
            account = BankAccount(account_id)
            account.balance = snapshot['data']['balance']
            account.is_active = snapshot['data']['is_active']
            account.overdraft_limit = snapshot['data']['overdraft_limit']
            account.version = snapshot['version']
            
            # ìŠ¤ëƒ…ìƒ· ì´í›„ ì´ë²¤íŠ¸ë“¤ ì ìš©
            events_after_snapshot = self.event_store.get_events(account_id, snapshot['version'])
            for event in events_after_snapshot:
                account._apply_event_without_recording(event)
            
            account.version = snapshot['version'] + len(events_after_snapshot)
        else:
            # ìŠ¤ëƒ…ìƒ·ì´ ì—†ìœ¼ë©´ ëª¨ë“  ì´ë²¤íŠ¸ë¡œ ì¬êµ¬ì„±
            events = self.event_store.get_events(account_id)
            
            if not events:
                return None
                
            account = BankAccount.from_history(account_id, events)
        
        account.uncommitted_events.clear()
        return account
    
    def create_snapshot(self, account_id: str):
        """ìŠ¤ëƒ…ìƒ· ìƒì„± (ì„±ëŠ¥ ìµœì í™”)"""
        account = self.get_by_id(account_id)
        if account:
            snapshot_data = {
                'balance': account.balance,
                'is_active': account.is_active,
                'overdraft_limit': account.overdraft_limit
            }
            
            self.event_store.save_snapshot(account_id, snapshot_data, account.version)

# Event Sourcing ì‹œë®¬ë ˆì´ì…˜
def simulate_event_sourcing():
    print("=== Event Sourcing ì‹œë®¬ë ˆì´ì…˜ ===")
    
    # Event Storeì™€ Repository ìƒì„±
    event_store = EventStore()
    account_repo = EventSourcedAccountRepository(event_store)
    
    print("\n--- ê³„ì¢Œ ìƒì„± ë° ê±°ë˜ ---")
    
    # ìƒˆ ê³„ì¢Œ ìƒì„±
    account = BankAccount.create_account("ACC001", initial_deposit=1000.0, overdraft_limit=500.0)
    account_repo.save(account)
    
    print(f"âœ… ê³„ì¢Œ ìƒì„±: ACC001, ì´ˆê¸° ì”ê³ : $1000")
    
    # ê±°ë˜ ì‹¤í–‰
    transactions = [
        ('deposit', 500.0, 'ê¸‰ì—¬ ì…ê¸ˆ'),
        ('withdraw', 200.0, 'ATM ì¶œê¸ˆ'),
        ('withdraw', 800.0, 'ì„ëŒ€ë£Œ ì§€ë¶ˆ'),  
        ('deposit', 300.0, 'í™˜ê¸‰'),
        ('withdraw', 1200.0, 'ëŒ€ì¶œ ìƒí™˜')  # ë§ˆì´ë„ˆìŠ¤ í•œë„ ì‚¬ìš©
    ]
    
    for transaction_type, amount, description in transactions:
        try:
            if transaction_type == 'deposit':
                account.deposit(amount, description)
            else:
                account.withdraw(amount, description)
            
            account_repo.save(account)
            print(f"âœ… {transaction_type.title()}: ${amount} - {description} (ì”ê³ : ${account.balance})")
            
        except Exception as e:
            print(f"âŒ {transaction_type.title()} ì‹¤íŒ¨: {e}")
    
    print(f"\ní˜„ì¬ ì”ê³ : ${account.balance}")
    
    print("\n--- ì´ë²¤íŠ¸ íˆìŠ¤í† ë¦¬ ì¡°íšŒ ---")
    events = event_store.get_events("ACC001")
    for event in events:
        print(f"ğŸ“ {event.timestamp.strftime('%H:%M:%S')} - {event.event_type}: {event.event_data}")
    
    print("\n--- íŠ¹ì • ì‹œì  ìƒíƒœ ë³µì› ---")
    # ì²˜ìŒ 3ê°œ ì´ë²¤íŠ¸ë§Œìœ¼ë¡œ ìƒíƒœ ë³µì›
    partial_events = events[:3]
    past_account = BankAccount.from_history("ACC001", partial_events)
    print(f"ğŸ“… 3ë²ˆì§¸ ì´ë²¤íŠ¸ ì‹œì  ì”ê³ : ${past_account.balance}")
    
    print("\n--- ìŠ¤ëƒ…ìƒ· ìƒì„± ë° í™œìš© ---")
    account_repo.create_snapshot("ACC001")
    
    # ìŠ¤ëƒ…ìƒ· ê¸°ë°˜ìœ¼ë¡œ ê³„ì¢Œ ì¬êµ¬ì„±
    recovered_account = account_repo.get_by_id("ACC001")
    print(f"ğŸ“¸ ìŠ¤ëƒ…ìƒ· ê¸°ë°˜ ë³µêµ¬ ì”ê³ : ${recovered_account.balance}")
    
    print("\n--- ë™ì‹œì„± í…ŒìŠ¤íŠ¸ ---")
    try:
        # ë™ì‹œì— ë‘ ê°œì˜ ê±°ë˜ ì‹œë„ (ë²„ì „ ì¶©ëŒ)
        account1 = account_repo.get_by_id("ACC001")
        account2 = account_repo.get_by_id("ACC001")
        
        account1.deposit(100.0, "Transaction 1")
        account2.deposit(200.0, "Transaction 2")
        
        account_repo.save(account1)  # ì„±ê³µ
        account_repo.save(account2)  # ë²„ì „ ì¶©ëŒë¡œ ì‹¤íŒ¨í•´ì•¼ í•¨
        
    except ConcurrencyException as e:
        print(f"âœ… ë™ì‹œì„± ì¶©ëŒ ê°ì§€: {e}")

# ì‹¤í–‰
simulate_event_sourcing()
```

## ğŸ” Kafka vs RabbitMQ ì‹¬í™” ë¹„êµ

### ğŸ“Š ì•„í‚¤í…ì²˜ ì°¨ì´ì 

```mermaid
graph TB
    subgraph "RabbitMQ Architecture"
        P1[Producer] --> E1[Exchange]
        E1 --> Q1[Queue 1]
        E1 --> Q2[Queue 2]
        Q1 --> C1[Consumer 1]
        Q2 --> C2[Consumer 2]
        
        note1[ë©”ì‹œì§€ í•œ ë²ˆ ì†Œë¹„ í›„ ì‚­ì œ]
    end
    
    subgraph "Kafka Architecture"
        P2[Producer] --> T1[Topic]
        T1 --> P2A[Partition 0]
        T1 --> P2B[Partition 1]
        P2A --> CG1[Consumer Group 1]
        P2B --> CG1
        P2A --> CG2[Consumer Group 2] 
        P2B --> CG2
        
        note2[ë©”ì‹œì§€ ë³´ì¡´, ì—¬ëŸ¬ ê·¸ë£¹ì´ ë…ë¦½ ì†Œë¹„]
    end
    
    style E1 fill:#ffeb3b
    style Q1 fill:#4caf50
    style Q2 fill:#4caf50
    style T1 fill:#2196f3
    style P2A fill:#03a9f4
    style P2B fill:#03a9f4
```

### ğŸ¯ ì‚¬ìš© ì‚¬ë¡€ë³„ ì„ íƒ ê¸°ì¤€

| ìš”êµ¬ì‚¬í•­ | RabbitMQ | Kafka |
|----------|----------|-------|
|**ì‘ì—… í**| âœ… ì™„ë²½ | âŒ ë¶€ì í•© |
|**ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°**| âš ï¸ ì œí•œì  | âœ… ìµœì  |
|**ì´ë²¤íŠ¸ ì†Œì‹±**| âŒ ì–´ë ¤ì›€ | âœ… ì™„ë²½ |
|**ë©”ì‹œì§€ ë³´ì¥**| âœ… ê°•í•¨ | âš ï¸ ì„¤ì • í•„ìš” |
|**ìˆœì„œ ë³´ì¥**| âš ï¸ íë³„ | âœ… íŒŒí‹°ì…˜ë³„ |
|**ì²˜ë¦¬ëŸ‰**| ì¤‘ê°„ | ë§¤ìš° ë†’ìŒ |
|**ìš´ì˜ ë³µì¡ì„±**| ë‚®ìŒ | ë†’ìŒ |

## í•µì‹¬ ìš”ì 

### 1. Event Streamingì˜ ê°•ë ¥í•¨

KafkaëŠ” ì´ë²¤íŠ¸ë¥¼ ì˜êµ¬ì ìœ¼ë¡œ ë³´ì¡´í•˜ê³  ì—¬ëŸ¬ ì†Œë¹„ìê°€ ë…ë¦½ì ìœ¼ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆì–´ ë³µì¡í•œ ë°ì´í„° íŒŒì´í”„ë¼ì¸ êµ¬ì¶•ì— ì í•©í•©ë‹ˆë‹¤.

### 2. Event Sourcingì˜ ê°ì‚¬ ì¶”ì 

ëª¨ë“  ìƒíƒœ ë³€ê²½ì„ ì´ë²¤íŠ¸ë¡œ ì €ì¥í•˜ë©´ ì™„ì „í•œ ê°ì‚¬ ì¶”ì ê³¼ ì‹œì ë³„ ìƒíƒœ ë³µì›ì´ ê°€ëŠ¥í•˜ì§€ë§Œ, ì‹œìŠ¤í…œ ë³µì¡ë„ê°€ í¬ê²Œ ì¦ê°€í•©ë‹ˆë‹¤.

### 3. ë™ì‹œì„± ì œì–´ì˜ ì¤‘ìš”ì„±

Event Sourcingì—ì„œëŠ” ë‚™ê´€ì  ë™ì‹œì„± ì œì–´ì™€ ë²„ì „ ê´€ë¦¬ë¥¼ í†µí•´ ë°ì´í„° ì¼ê´€ì„±ì„ ë³´ì¥í•´ì•¼ í•©ë‹ˆë‹¤.

---

**ì´ì „**: [Message Queue êµ¬í˜„](14-04-04-message-queue-implementation.md)  
**ë‹¤ìŒ**: [ì‹¤ë¬´ ì ìš©ê³¼ ì„±ìˆ™ë„ ëª¨ë¸](14-04-03-practical-implementation-guide.md)ì—ì„œ ì‹¤ì „ ë„ì… ì „ëµì„ í•™ìŠµí•©ë‹ˆë‹¤.

## ğŸ“š ê´€ë ¨ ë¬¸ì„œ

### ğŸ“– í˜„ì¬ ë¬¸ì„œ ì •ë³´

-**ë‚œì´ë„**: ADVANCED
-**ì£¼ì œ**: ì¸í”„ë¼ìŠ¤íŠ¸ëŸ­ì²˜
-**ì˜ˆìƒ ì‹œê°„**: 12-16ì‹œê°„

### ğŸ¯ í•™ìŠµ ê²½ë¡œ

- [ğŸ“š ADVANCED ë ˆë²¨ ì „ì²´ ë³´ê¸°](../learning-paths/advanced/)
- [ğŸ  ë©”ì¸ í•™ìŠµ ê²½ë¡œ](../learning-paths/)
- [ğŸ“‹ ì „ì²´ ê°€ì´ë“œ ëª©ë¡](../README.md)

### ğŸ“‚ ê°™ì€ ì±•í„° (chapter-14-distributed-systems)

- [14.1 ë¶„ì‚° ì‹œìŠ¤í…œ ê¸°ì´ˆ ì´ë¡  - CAP ì •ë¦¬ì™€ ì¼ê´€ì„±ì˜ ê³¼í•™](./14-01-01-distributed-fundamentals.md)
- [14.2 í•©ì˜ ì•Œê³ ë¦¬ì¦˜ - ë¶„ì‚°ëœ ë…¸ë“œë“¤ì´ í•˜ë‚˜ê°€ ë˜ëŠ” ë°©ë²•](./14-02-01-consensus-algorithms.md)
- [14.3 ë¶„ì‚° ë°ì´í„° ê´€ë¦¬ ê°œìš”](./14-02-02-distributed-data.md)
- [14.3A Sharding ì „ëµê³¼ êµ¬í˜„](./14-02-03-sharding-strategies.md)
- [14.3B Replication íŒ¨í„´ê³¼ êµ¬í˜„](./14-05-01-replication-patterns.md)

### ğŸ·ï¸ ê´€ë ¨ í‚¤ì›Œë“œ

`kafka`, `event-sourcing`, `cqrs`, `event-streaming`, `distributed-systems`

### â­ï¸ ë‹¤ìŒ ë‹¨ê³„ ê°€ì´ë“œ

- ì‹œìŠ¤í…œ ì „ì²´ì˜ ê´€ì ì—ì„œ ì´í•´í•˜ë ¤ ë…¸ë ¥í•˜ì„¸ìš”
- ë‹¤ë¥¸ ê³ ê¸‰ ì£¼ì œë“¤ê³¼ì˜ ì—°ê´€ì„±ì„ íŒŒì•…í•´ë³´ì„¸ìš”
