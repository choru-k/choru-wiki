---
tags:
  - Event-Driven
  - Architecture
  - Kafka
  - Event Sourcing
  - Async Processing
  - Guide
---

# 15.2 ì´ë²¤íŠ¸ ë“œë¦¬ë¸ ì•„í‚¤í…ì²˜ - ë³€í™”ì— ë°˜ì‘í•˜ëŠ” ì‹œìŠ¤í…œì˜ ì˜ˆìˆ 

## ğŸ¯ 2021ë…„ 11ì›” - ì‹¤ì‹œê°„ ì¶”ì²œ ì‹œìŠ¤í…œì˜ ê¸°ì 

ì œê°€ OTT ìŠ¤íŠ¸ë¦¬ë° ì„œë¹„ìŠ¤ì˜ ì‹œë‹ˆì–´ ì•„í‚¤í…íŠ¸ë¡œ ì¼í•  ë•Œ ê²ªì—ˆë˜ ì‹¤í™”ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì‹œì²­ í–‰ë™ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ë¶„ì„í•´ ê°œì¸í™”ëœ ì¶”ì²œì„ ì œê³µí•˜ëŠ” ì‹œìŠ¤í…œì„ êµ¬ì¶•í•˜ë©´ì„œ Event-Driven Architectureì˜ ì§„ì •í•œ ìœ„ë ¥ì„ ê²½í—˜í–ˆìŠµë‹ˆë‹¤.

### ğŸ’¥ ê¸°ì¡´ ë°°ì¹˜ ì²˜ë¦¬ ì‹œìŠ¤í…œì˜ í•œê³„

**2021ë…„ 11ì›” 1ì¼ - ê³ ê° ë¶ˆë§Œ í­ì¦**

```bash
ğŸ˜° ìš°ë¦¬ê°€ ì§ë©´í•œ í˜„ì‹¤ì  ë¬¸ì œë“¤:

ğŸ“Š ê¸°ì¡´ ì‹œìŠ¤í…œ í˜„í™©:
- ì¶”ì²œ ì—…ë°ì´íŠ¸ ì£¼ê¸°: 24ì‹œê°„ (ë„ˆë¬´ ëŠë¦¼!)
- ì‚¬ìš©ì ë°˜ì‘ ë°˜ì˜: ë‹¤ìŒë‚  ìƒˆë²½ì—ì•¼ ê°€ëŠ¥
- ì¸ê¸° ì½˜í…ì¸  ë°œê²¬: 2-3ì¼ í›„ì—ë‚˜ ì¶”ì²œì— ë°˜ì˜
- ì‹¤ì‹œê°„ íŠ¸ë Œë“œ ë†“ì¹¨: í™”ì œì˜ ì½˜í…ì¸ ë¥¼ ë†“ì¹˜ëŠ” ë¹ˆë„ ì¦ê°€

ğŸ’¸ ë¹„ì¦ˆë‹ˆìŠ¤ ì„íŒ©íŠ¸:
- ì‚¬ìš©ì ì´íƒˆë¥ : 15% ì¦ê°€
- ì‹œì²­ ì‹œê°„: 20% ê°ì†Œ  
- ì‹ ê·œ ì½˜í…ì¸  ë°œê²¬ìœ¨: 30% í•˜ë½
- ê³ ê° ë§Œì¡±ë„: 3.2/5.0 (ê²½ìŸì‚¬ ëŒ€ë¹„ ë‚®ìŒ)

# ê¸°ì¡´ ë°°ì¹˜ ê¸°ë°˜ ì•„í‚¤í…ì²˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ì‚¬ìš©ì í–‰ë™ â”‚â”€â”€â”€â–¶â”‚ ë¡œê·¸ ìˆ˜ì§‘    â”‚â”€â”€â”€â–¶â”‚ ë°ì´í„° ì›¨ì–´  â”‚
â”‚ (ì‹œì²­/ì¢‹ì•„ìš”) â”‚    â”‚ (1ì‹œê°„ë§ˆë‹¤)  â”‚    â”‚ í•˜ìš°ìŠ¤      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                             â”‚
                                             â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚ ì¶”ì²œ ê²°ê³¼   â”‚â—€â”€â”€â”€â”‚ ML íŒŒì´í”„ë¼ì¸â”‚ 
                   â”‚ (ìƒˆë²½ ê°±ì‹ ) â”‚    â”‚ (ìƒˆë²½ 2ì‹œ)   â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸš¨ ë¬¸ì œì : "ì–´ì œì˜ ë‚˜"ë¥¼ ê¸°ì¤€ìœ¼ë¡œ "ì˜¤ëŠ˜ì˜ ë‚˜"ì—ê²Œ ì¶”ì²œ!
```

### ğŸš€ Event-Driven ë³€í™˜ - ì‹¤ì‹œê°„ì˜ ë§ˆë²•

**ì‹œìŠ¤í…œ ì„¤ê³„ ì „í™˜**

```mermaid
graph TB
    subgraph "Event Sources (ì´ë²¤íŠ¸ ë°œìƒì›)"
        User[ì‚¬ìš©ì ì•±]
        CDN[CDN Edge]
        Payment[ê²°ì œ ì‹œìŠ¤í…œ]
        CMS[ì½˜í…ì¸  ê´€ë¦¬]
    end
    
    subgraph "Event Streaming Layer"
        Kafka[Apache Kafka, - user-events, - content-events, - system-events]
    end
    
    subgraph "Event Processing Services"
        Analytics[ì‹¤ì‹œê°„ ë¶„ì„, Apache Flink]
        Recommendation[ì¶”ì²œ ì—”ì§„, ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸]
        Notification[ì•Œë¦¼ ì„œë¹„ìŠ¤, ê°œì¸í™” ì•Œë¦¼]
        Trending[íŠ¸ë Œë”© ë¶„ì„, ì¸ê¸° ì½˜í…ì¸ ]
    end
    
    subgraph "Event Stores & Views"
        EventStore[(Event Store, ì „ì²´ ì´ë²¤íŠ¸)]
        UserProfile[(ì‚¬ìš©ì í”„ë¡œí•„, ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸)]
        ContentMetrics[(ì½˜í…ì¸  ë©”íŠ¸ë¦­, ì‹¤ì‹œê°„ ì§‘ê³„)]
        RecommendationDB[(ì¶”ì²œ ê²°ê³¼, ì‹¤ì‹œê°„ ê°±ì‹ )]
    end
    
    subgraph "Consumer Applications"
        WebApp[ì›¹ ì•±]
        MobileApp[ëª¨ë°”ì¼ ì•±]
        SmartTV[ìŠ¤ë§ˆíŠ¸ TV]
        AdminDash[ê´€ë¦¬ì ëŒ€ì‹œë³´ë“œ]
    end
    
    User --> Kafka
    CDN --> Kafka
    Payment --> Kafka
    CMS --> Kafka
    
    Kafka --> Analytics
    Kafka --> Recommendation
    Kafka --> Notification
    Kafka --> Trending
    
    Analytics --> EventStore
    Recommendation --> UserProfile
    Recommendation --> RecommendationDB
    Trending --> ContentMetrics
    
    UserProfile --> WebApp
    RecommendationDB --> MobileApp
    ContentMetrics --> SmartTV
    EventStore --> AdminDash
    
    style Kafka fill:#fff3e0
    style Analytics fill:#e3f2fd
    style Recommendation fill:#e8f5e8
    style EventStore fill:#fce4ec
```

### ğŸ‰ 3ì£¼ í›„ì˜ ë†€ë¼ìš´ ê²°ê³¼

**2021ë…„ 11ì›” 22ì¼ - ì™„ì „íˆ ë°”ë€ ì§€í‘œë“¤**

```bash
âœ… ì„±ê³¼ ì§€í‘œ:

ğŸš€ ì‹¤ì‹œê°„ì„± í–¥ìƒ:
- ì¶”ì²œ ì—…ë°ì´íŠ¸: 24ì‹œê°„ â†’ 10ì´ˆ ì´ë‚´
- ì‚¬ìš©ì ë°˜ì‘ ë°˜ì˜: ì¦‰ì‹œ ë°˜ì˜
- ì‹ ê·œ ì½˜í…ì¸  ì¶”ì²œ: ì—…ë¡œë“œ í›„ 5ë¶„ ì´ë‚´
- íŠ¸ë Œë”© ì½˜í…ì¸ : ì‹¤ì‹œê°„ ë°œê²¬ ë° ì¶”ì²œ

ğŸ“Š ë¹„ì¦ˆë‹ˆìŠ¤ ì„íŒ©íŠ¸:
- ì‚¬ìš©ì ì´íƒˆë¥ : 15% ê°ì†Œ â†’ 8% ê°ì†Œ
- í‰ê·  ì‹œì²­ ì‹œê°„: 35% ì¦ê°€
- ì‹ ê·œ ì½˜í…ì¸  ë°œê²¬ìœ¨: 60% ì¦ê°€
- ê³ ê° ë§Œì¡±ë„: 4.2/5.0 (ì—…ê³„ ìµœê³  ìˆ˜ì¤€)

ğŸ’° ìˆ˜ìµ ì¦ëŒ€:
- ì›” êµ¬ë…ì ìˆ˜: 20% ì¦ê°€
- í”„ë¦¬ë¯¸ì—„ ê°€ì…ë¥ : 25% ì¦ê°€
- ê´‘ê³  ìˆ˜ìµ: 30% ì¦ê°€ (ë” ì •í™•í•œ íƒ€ê²ŸíŒ…)

# ì‹¤ì‹œê°„ ì´ë²¤íŠ¸ í”Œë¡œìš° ì˜ˆì‹œ
09:15:32 ì‚¬ìš©ìê°€ "ì˜¤ì§•ì–´ ê²Œì„" 3í™”ë¥¼ ì‹œì²­ ì‹œì‘
09:15:33 [ì´ë²¤íŠ¸ ë°œìƒ] user_started_watching
09:15:35 [ì‹¤ì‹œê°„ ë¶„ì„] ì¥ë¥´ ì„ í˜¸ë„ ì—…ë°ì´íŠ¸
09:15:36 [ì¶”ì²œ ì—”ì§„] ìœ ì‚¬ ì½˜í…ì¸  í›„ë³´ ê°±ì‹ 
09:15:37 [ì‚¬ìš©ì í™”ë©´] ìƒˆë¡œìš´ ì¶”ì²œ ëª©ë¡ í‘œì‹œ

ğŸ¯ í•µì‹¬: ì‚¬ìš©ìì˜ í˜„ì¬ ê´€ì‹¬ì‚¬ì— ë§ëŠ” ì¦‰ì‹œ ì¶”ì²œ!
```

---

## ğŸ“¡ Event-Driven Architecture í•µì‹¬ ê°œë…

### 1. ì´ë²¤íŠ¸ (Event)ì˜ ì •ì˜ì™€ íŠ¹ì„±

```typescript
// ì´ë²¤íŠ¸ ì„¤ê³„ ì›ì¹™ê³¼ êµ¬í˜„
interface DomainEvent {
    // ì´ë²¤íŠ¸ ë©”íƒ€ë°ì´í„°
    eventId: string;          // ê³ ìœ  ì‹ë³„ì
    eventType: string;        // ì´ë²¤íŠ¸ íƒ€ì…
    eventVersion: string;     // ìŠ¤í‚¤ë§ˆ ë²„ì „
    timestamp: Date;          // ë°œìƒ ì‹œê°
    source: string;           // ì´ë²¤íŠ¸ ë°œìƒì›
    
    // ì´ë²¤íŠ¸ ë°ì´í„°
    aggregateId: string;      // ì§‘í•©ì²´ ID
    aggregateType: string;    // ì§‘í•©ì²´ íƒ€ì…
    data: any;               // ì‹¤ì œ ì´ë²¤íŠ¸ ë°ì´í„°
    
    // ì¶”ê°€ ë©”íƒ€ë°ì´í„°
    correlationId?: string;   // ìš”ì²­ ì¶”ì  ID
    causationId?: string;     // ì›ì¸ ì´ë²¤íŠ¸ ID
    userId?: string;          // ì‚¬ìš©ì ID (ê°ì‚¬ìš©)
}

// êµ¬ì²´ì ì¸ ì´ë²¤íŠ¸ íƒ€ì…ë“¤
interface UserRegisteredEvent extends DomainEvent {
    eventType: "user.registered";
    data: {
        userId: string;
        email: string;
        registrationMethod: "email" | "social" | "mobile";
        referralSource?: string;
        initialPreferences?: string[];
    };
}

interface ContentWatchedEvent extends DomainEvent {
    eventType: "content.watched";
    data: {
        userId: string;
        contentId: string;
        sessionId: string;
        watchDurationSeconds: number;
        completionPercentage: number;
        device: "mobile" | "web" | "smarttv" | "tablet";
        quality: "SD" | "HD" | "4K";
        timestamp: Date;
    };
}

interface PaymentProcessedEvent extends DomainEvent {
    eventType: "payment.processed";
    data: {
        paymentId: string;
        userId: string;
        amount: number;
        currency: string;
        subscriptionPlan: string;
        paymentMethod: string;
        status: "success" | "failed" | "pending";
    };
}

// ì´ë²¤íŠ¸ íŒ©í† ë¦¬ - ì¼ê´€ëœ ì´ë²¤íŠ¸ ìƒì„±
class EventFactory {
    static createUserRegisteredEvent(
        userId: string, 
        email: string, 
        registrationData: any
    ): UserRegisteredEvent {
        return {
            eventId: this.generateEventId(),
            eventType: "user.registered",
            eventVersion: "1.0",
            timestamp: new Date(),
            source: "user-service",
            aggregateId: userId,
            aggregateType: "User",
            correlationId: RequestContext.getCorrelationId(),
            data: {
                userId,
                email,
                registrationMethod: registrationData.method,
                referralSource: registrationData.referralSource,
                initialPreferences: registrationData.preferences
            }
        };
    }
    
    static createContentWatchedEvent(
        userId: string, 
        contentId: string, 
        watchingSession: WatchingSession
    ): ContentWatchedEvent {
        return {
            eventId: this.generateEventId(),
            eventType: "content.watched",
            eventVersion: "1.0",
            timestamp: new Date(),
            source: "streaming-service",
            aggregateId: contentId,
            aggregateType: "Content",
            correlationId: RequestContext.getCorrelationId(),
            data: {
                userId,
                contentId,
                sessionId: watchingSession.id,
                watchDurationSeconds: watchingSession.duration,
                completionPercentage: watchingSession.completionPercentage,
                device: watchingSession.device,
                quality: watchingSession.quality,
                timestamp: watchingSession.endTime
            }
        };
    }
    
    private static generateEventId(): string {
        return `evt_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
    }
}
```

### 2. ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¬ë° with Apache Kafka

```java
// Javaë¡œ êµ¬í˜„í•œ Kafka ì´ë²¤íŠ¸ í”„ë¡œë“€ì„œ
@Service
public class EventPublisher {
    
    private final KafkaTemplate<String, DomainEvent> kafkaTemplate;
    private final ObjectMapper objectMapper;
    
    public EventPublisher(KafkaTemplate<String, DomainEvent> kafkaTemplate) {
        this.kafkaTemplate = kafkaTemplate;
        this.objectMapper = new ObjectMapper();
        this.kafkaTemplate.setDefaultTopic("domain-events");
    }
    
    public void publishEvent(DomainEvent event) {
        try {
            // ì´ë²¤íŠ¸ ê²€ì¦
            validateEvent(event);
            
            // í† í”½ ë¼ìš°íŒ… (ì´ë²¤íŠ¸ íƒ€ì…ë³„ë¡œ í† í”½ ë¶„ë¦¬)
            String topic = determineTopicForEvent(event);
            
            // íŒŒí‹°ì…˜ í‚¤ ê²°ì • (ê°™ì€ ì§‘í•©ì²´ì˜ ì´ë²¤íŠ¸ëŠ” ê°™ì€ íŒŒí‹°ì…˜ìœ¼ë¡œ)
            String partitionKey = event.getAggregateId();
            
            // í—¤ë” ì„¤ì •
            ProducerRecord<String, DomainEvent> record = new ProducerRecord<>(
                topic, 
                partitionKey, 
                event
            );
            
            // ì¶”ê°€ ë©”íƒ€ë°ì´í„° í—¤ë”
            record.headers().add("event-type", event.getEventType().getBytes());
            record.headers().add("event-version", event.getEventVersion().getBytes());
            record.headers().add("source-service", event.getSource().getBytes());
            record.headers().add("correlation-id", 
                event.getCorrelationId() != null ? 
                    event.getCorrelationId().getBytes() : "".getBytes());
            
            // ë¹„ë™ê¸° ë°œí–‰ with ì½œë°±
            kafkaTemplate.send(record).addCallback(
                result -> {
                    log.info("ì´ë²¤íŠ¸ ë°œí–‰ ì„±ê³µ: {} -> {}", 
                        event.getEventType(), result.getRecordMetadata());
                    
                    // ì„±ê³µ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
                    Metrics.counter("event.published.success", 
                        Tags.of("event-type", event.getEventType())).increment();
                },
                failure -> {
                    log.error("ì´ë²¤íŠ¸ ë°œí–‰ ì‹¤íŒ¨: {}", event.getEventType(), failure);
                    
                    // ì‹¤íŒ¨ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
                    Metrics.counter("event.published.failure", 
                        Tags.of("event-type", event.getEventType())).increment();
                    
                    // ì‹¤íŒ¨í•œ ì´ë²¤íŠ¸ëŠ” ë³„ë„ ì €ì¥ì†Œì— ë³´ê´€ (ì¬ì‹œë„ìš©)
                    storeFailedEvent(event, failure);
                }
            );
            
        } catch (Exception e) {
            log.error("ì´ë²¤íŠ¸ ë°œí–‰ ì¤‘ ì˜ˆì™¸ ë°œìƒ", e);
            throw new EventPublishingException("ì´ë²¤íŠ¸ ë°œí–‰ ì‹¤íŒ¨", e);
        }
    }
    
    private void validateEvent(DomainEvent event) {
        if (event.getEventId() == null || event.getEventId().isEmpty()) {
            throw new IllegalArgumentException("EventIdëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤");
        }
        
        if (event.getEventType() == null || event.getEventType().isEmpty()) {
            throw new IllegalArgumentException("EventTypeì€ í•„ìˆ˜ì…ë‹ˆë‹¤");
        }
        
        if (event.getAggregateId() == null || event.getAggregateId().isEmpty()) {
            throw new IllegalArgumentException("AggregateIdëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤");
        }
        
        // ì´ë²¤íŠ¸ ìŠ¤í‚¤ë§ˆ ê²€ì¦
        validateEventSchema(event);
    }
    
    private String determineTopicForEvent(DomainEvent event) {
        // ì´ë²¤íŠ¸ íƒ€ì…ë³„ í† í”½ ë¼ìš°íŒ… ê·œì¹™
        String eventType = event.getEventType();
        
        if (eventType.startsWith("user.")) {
            return "user-events";
        } else if (eventType.startsWith("content.")) {
            return "content-events";
        } else if (eventType.startsWith("payment.")) {
            return "payment-events";
        } else if (eventType.startsWith("system.")) {
            return "system-events";
        } else {
            return "domain-events"; // ê¸°ë³¸ í† í”½
        }
    }
    
    private void validateEventSchema(DomainEvent event) {
        // JSON Schema ê²€ì¦ (ì‹¤ì œë¡œëŠ” Confluent Schema Registry í™œìš©)
        try {
            String schemaKey = event.getEventType() + ":" + event.getEventVersion();
            JsonSchema schema = schemaRegistry.getSchema(schemaKey);
            
            String eventJson = objectMapper.writeValueAsString(event);
            Set<ValidationMessage> errors = schema.validate(eventJson);
            
            if (!errors.isEmpty()) {
                throw new EventValidationException("ì´ë²¤íŠ¸ ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì‹¤íŒ¨: " + errors);
            }
            
        } catch (Exception e) {
            log.warn("ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì‹¤íŒ¨, ê¸°ë³¸ ê²€ì¦ìœ¼ë¡œ ì§„í–‰: {}", e.getMessage());
        }
    }
    
    private void storeFailedEvent(DomainEvent event, Throwable failure) {
        // ì‹¤íŒ¨í•œ ì´ë²¤íŠ¸ëŠ” ë°ë“œë ˆí„° íë‚˜ ë³„ë„ ì €ì¥ì†Œì— ë³´ê´€
        failedEventRepository.save(FailedEvent.builder()
            .eventId(event.getEventId())
            .eventType(event.getEventType())
            .eventData(objectMapper.writeValueAsString(event))
            .failureReason(failure.getMessage())
            .failureTimestamp(Instant.now())
            .retryCount(0)
            .build());
    }
}

// Kafka ì»¨ìŠˆë¨¸ - ì´ë²¤íŠ¸ ì²˜ë¦¬
@Component
public class ContentRecommendationEventHandler {
    
    private final RecommendationEngine recommendationEngine;
    private final UserProfileService userProfileService;
    
    @KafkaListener(
        topics = "content-events",
        groupId = "recommendation-service",
        containerFactory = "kafkaListenerContainerFactory"
    )
    public void handleContentEvent(
        @Payload DomainEvent event,
        @Header Map<String, Object> headers,
        Acknowledgment ack
    ) {
        try {
            log.info("ì½˜í…ì¸  ì´ë²¤íŠ¸ ì²˜ë¦¬ ì‹œì‘: {}", event.getEventType());
            
            // ì´ë²¤íŠ¸ íƒ€ì…ë³„ ì²˜ë¦¬ ë¶„ê¸°
            switch (event.getEventType()) {
                case "content.watched":
                    handleContentWatched((ContentWatchedEvent) event);
                    break;
                case "content.rated":
                    handleContentRated((ContentRatedEvent) event);
                    break;
                case "content.shared":
                    handleContentShared((ContentSharedEvent) event);
                    break;
                default:
                    log.warn("ì²˜ë¦¬í•  ìˆ˜ ì—†ëŠ” ì´ë²¤íŠ¸ íƒ€ì…: {}", event.getEventType());
            }
            
            // ìˆ˜ë™ ì»¤ë°‹
            ack.acknowledge();
            
            // ì²˜ë¦¬ ì„±ê³µ ë©”íŠ¸ë¦­
            Metrics.counter("event.processed.success", 
                Tags.of("event-type", event.getEventType())).increment();
                
        } catch (Exception e) {
            log.error("ì´ë²¤íŠ¸ ì²˜ë¦¬ ì‹¤íŒ¨: {}", event.getEventType(), e);
            
            // ì²˜ë¦¬ ì‹¤íŒ¨ ë©”íŠ¸ë¦­
            Metrics.counter("event.processed.failure", 
                Tags.of("event-type", event.getEventType())).increment();
            
            // ì¬ì‹œë„ ê°€ëŠ¥í•œ ì˜¤ë¥˜ì¸ì§€ íŒë‹¨
            if (isRetryableError(e)) {
                // DLQë¡œ ë³´ë‚´ì§€ ì•Šê³  ì¬ì‹œë„
                throw e;
            } else {
                // ë³µêµ¬ ë¶ˆê°€ëŠ¥í•œ ì˜¤ë¥˜ - ë¡œê·¸ë§Œ ë‚¨ê¸°ê³  ACK
                log.error("ë³µêµ¬ ë¶ˆê°€ëŠ¥í•œ ì˜¤ë¥˜ë¡œ ì´ë²¤íŠ¸ ìŠ¤í‚µ: {}", event.getEventId());
                ack.acknowledge();
            }
        }
    }
    
    private void handleContentWatched(ContentWatchedEvent event) {
        String userId = event.getData().getUserId();
        String contentId = event.getData().getContentId();
        double completionRate = event.getData().getCompletionPercentage();
        
        // 1. ì‚¬ìš©ì í”„ë¡œí•„ ì—…ë°ì´íŠ¸ (ì¥ë¥´ ì„ í˜¸ë„, ì‹œì²­ íŒ¨í„´ ë“±)
        userProfileService.updateWatchingPreferences(
            userId, 
            contentId, 
            completionRate
        );
        
        // 2. ì‹¤ì‹œê°„ ì¶”ì²œ ì—…ë°ì´íŠ¸
        if (completionRate > 0.8) { // 80% ì´ìƒ ì‹œì²­í•œ ê²½ìš°
            recommendationEngine.updateUserRecommendations(userId, contentId);
        }
        
        // 3. ì½˜í…ì¸  ì¸ê¸°ë„ ì—…ë°ì´íŠ¸
        recommendationEngine.updateContentPopularity(contentId, completionRate);
        
        log.info("ì½˜í…ì¸  ì‹œì²­ ì´ë²¤íŠ¸ ì²˜ë¦¬ ì™„ë£Œ: user={}, content={}, completion={}%", 
            userId, contentId, completionRate * 100);
    }
    
    private void handleContentRated(ContentRatedEvent event) {
        String userId = event.getData().getUserId();
        String contentId = event.getData().getContentId();
        int rating = event.getData().getRating();
        
        // í‰ì  ê¸°ë°˜ ì¶”ì²œ ì•Œê³ ë¦¬ì¦˜ ì—…ë°ì´íŠ¸
        recommendationEngine.updateRatingBasedRecommendations(userId, contentId, rating);
        
        // ìœ ì‚¬ ì‚¬ìš©ì ì°¾ê¸° ë° í˜‘ì—… í•„í„°ë§ ì—…ë°ì´íŠ¸
        recommendationEngine.updateCollaborativeFiltering(userId, contentId, rating);
        
        log.info("ì½˜í…ì¸  í‰ì  ì´ë²¤íŠ¸ ì²˜ë¦¬ ì™„ë£Œ: user={}, content={}, rating={}", 
            userId, contentId, rating);
    }
    
    private boolean isRetryableError(Exception e) {
        // ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜, íƒ€ì„ì•„ì›ƒ ë“±ì€ ì¬ì‹œë„ ê°€ëŠ¥
        return e instanceof ConnectException 
            || e instanceof SocketTimeoutException
            || e instanceof TransientDataAccessResourceException;
    }
}
```

---

## âš¡ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬

### Apache Flinkë¡œ ë³µí•© ì´ë²¤íŠ¸ ì²˜ë¦¬

```scala
// Scalaë¡œ êµ¬í˜„í•œ Flink ì‹¤ì‹œê°„ ë¶„ì„ íŒŒì´í”„ë¼ì¸
import org.apache.flink.streaming.api.scala._
import org.apache.flink.streaming.api.windowing.time.Time
import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer
import org.apache.flink.cep.scala.CEP
import org.apache.flink.cep.scala.pattern.Pattern

case class UserEvent(
  userId: String,
  eventType: String,
  contentId: String,
  timestamp: Long,
  sessionId: String,
  device: String
)

case class ContentTrendingSignal(
  contentId: String,
  trendingScore: Double,
  timestamp: Long,
  reason: String
)

case class UserEngagementScore(
  userId: String,
  engagementScore: Double,
  sessionDuration: Long,
  contentCount: Int,
  timestamp: Long
)

object RealTimeAnalyticsPipeline {
  
  def main(args: Array[String]): Unit = {
    val env = StreamExecutionEnvironment.getExecutionEnvironment
    env.setParallelism(4)
    env.enableCheckpointing(60000) // 1ë¶„ë§ˆë‹¤ ì²´í¬í¬ì¸íŠ¸
    
    // Kafka ì†ŒìŠ¤ ì„¤ì •
    val kafkaProps = new Properties()
    kafkaProps.setProperty("bootstrap.servers", "kafka-cluster:9092")
    kafkaProps.setProperty("group.id", "flink-analytics")
    
    val kafkaConsumer = new FlinkKafkaConsumer[UserEvent](
      "user-events",
      new UserEventDeserializer(),
      kafkaProps
    )
    
    // ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¼ ìƒì„±
    val eventStream = env.addSource(kafkaConsumer)
      .assignTimestampsAndWatermarks(
        WatermarkStrategy
          .forBoundedOutOfOrderness[UserEvent](Duration.ofSeconds(5))
          .withTimestampAssigner((event, _) => event.timestamp)
      )
    
    // 1. ì‹¤ì‹œê°„ ì½˜í…ì¸  íŠ¸ë Œë”© ë¶„ì„
    val trendingAnalysis = eventStream
      .filter(_.eventType == "content.watched")
      .keyBy(_.contentId)
      .window(SlidingEventTimeWindows.of(Time.minutes(15), Time.minutes(1)))
      .aggregate(new ContentTrendingAggregator())
      .filter(_.trendingScore > 0.7) // íŠ¸ë Œë”© ì„ê³„ê°’
    
    // 2. ì‚¬ìš©ì ì°¸ì—¬ë„ ì‹¤ì‹œê°„ ë¶„ì„
    val engagementAnalysis = eventStream
      .keyBy(_.userId)
      .window(SessionWindows.withGap(Time.minutes(30)))
      .aggregate(new UserEngagementAggregator())
    
    // 3. ë³µí•© ì´ë²¤íŠ¸ íŒ¨í„´ íƒì§€ (CEP - Complex Event Processing)
    val bingeWatchingPattern = Pattern
      .begin[UserEvent]("start")
      .where(_.eventType == "content.watched")
      .next("continue")
      .where(_.eventType == "content.watched")
      .times(3) // 3ê°œ ì´ìƒì˜ ì—°ì† ì‹œì²­
      .within(Time.hours(2)) // 2ì‹œê°„ ì´ë‚´
    
    val bingeWatchingDetection = CEP
      .pattern(eventStream.keyBy(_.userId), bingeWatchingPattern)
      .select(new BingeWatchingSelector())
    
    // 4. ì‹¤ì‹œê°„ ì´ìƒ íƒì§€ (ê°‘ì‘ìŠ¤ëŸ¬ìš´ íŠ¸ë˜í”½ ì¦ê°€)
    val anomalyDetection = eventStream
      .windowAll(TumblingEventTimeWindows.of(Time.minutes(1)))
      .aggregate(new TrafficAnomalyDetector())
      .filter(_.isAnomaly)
    
    // ê²°ê³¼ë¥¼ ë‹¤ì‹œ Kafkaë¡œ ì „ì†¡
    trendingAnalysis.addSink(new FlinkKafkaProducer[ContentTrendingSignal](
      "trending-signals",
      new TrendingSignalSerializer(),
      kafkaProps
    ))
    
    engagementAnalysis.addSink(new FlinkKafkaProducer[UserEngagementScore](
      "user-engagement",
      new EngagementScoreSerializer(),
      kafkaProps
    ))
    
    bingeWatchingDetection.addSink(new FlinkKafkaProducer[BingeWatchingAlert](
      "user-behavior-alerts",
      new BingeWatchingAlertSerializer(),
      kafkaProps
    ))
    
    anomalyDetection.addSink(new FlinkKafkaProducer[TrafficAnomalyAlert](
      "system-alerts",
      new TrafficAnomalySerializer(),
      kafkaProps
    ))
    
    env.execute("Real-time Analytics Pipeline")
  }
}

// ì½˜í…ì¸  íŠ¸ë Œë”© ì ìˆ˜ ê³„ì‚° Aggregator
class ContentTrendingAggregator extends AggregateFunction[UserEvent, TrendingAccumulator, ContentTrendingSignal] {
  
  override def createAccumulator(): TrendingAccumulator = 
    TrendingAccumulator(0, 0, 0, Set.empty, 0L)
  
  override def add(event: UserEvent, acc: TrendingAccumulator): TrendingAccumulator = {
    val newUniqueUsers = acc.uniqueUsers + event.userId
    val newWatchCount = acc.watchCount + 1
    val newTotalDuration = acc.totalDuration + extractDuration(event)
    val newDeviceCount = acc.deviceTypes + event.device
    
    acc.copy(
      watchCount = newWatchCount,
      uniqueUserCount = newUniqueUsers.size,
      totalDuration = newTotalDuration,
      uniqueUsers = newUniqueUsers,
      deviceTypes = newDeviceCount
    )
  }
  
  override def getResult(acc: TrendingAccumulator): ContentTrendingSignal = {
    // íŠ¸ë Œë”© ì ìˆ˜ ê³„ì‚° ì•Œê³ ë¦¬ì¦˜
    val userDiversityScore = math.min(acc.uniqueUserCount / 100.0, 1.0) // ìµœëŒ€ 100ëª… ê¸°ì¤€
    val watchIntensityScore = math.min(acc.watchCount / 500.0, 1.0) // ìµœëŒ€ 500íšŒ ê¸°ì¤€  
    val deviceDiversityScore = acc.deviceTypes.size / 4.0 // ìµœëŒ€ 4ê°œ ê¸°ê¸° íƒ€ì…
    val avgWatchTime = if (acc.watchCount > 0) acc.totalDuration / acc.watchCount else 0
    val completionScore = math.min(avgWatchTime / 3600.0, 1.0) // 1ì‹œê°„ ê¸°ì¤€
    
    val trendingScore = (userDiversityScore * 0.3 + 
                        watchIntensityScore * 0.3 + 
                        deviceDiversityScore * 0.2 + 
                        completionScore * 0.2)
    
    ContentTrendingSignal(
      contentId = acc.contentId,
      trendingScore = trendingScore,
      timestamp = System.currentTimeMillis(),
      reason = s"users:${acc.uniqueUserCount}, watches:${acc.watchCount}, devices:${acc.deviceTypes.size}"
    )
  }
  
  override def merge(acc1: TrendingAccumulator, acc2: TrendingAccumulator): TrendingAccumulator = {
    acc1.copy(
      watchCount = acc1.watchCount + acc2.watchCount,
      uniqueUserCount = (acc1.uniqueUsers ++ acc2.uniqueUsers).size,
      totalDuration = acc1.totalDuration + acc2.totalDuration,
      uniqueUsers = acc1.uniqueUsers ++ acc2.uniqueUsers,
      deviceTypes = acc1.deviceTypes ++ acc2.deviceTypes
    )
  }
  
  private def extractDuration(event: UserEvent): Long = {
    // ì´ë²¤íŠ¸ì—ì„œ ì‹œì²­ ì‹œê°„ ì¶”ì¶œ ë¡œì§
    // ì‹¤ì œë¡œëŠ” event.dataì—ì„œ duration í•„ë“œ íŒŒì‹±
    1800L // 30ë¶„ ê¸°ë³¸ê°’
  }
}

case class TrendingAccumulator(
  watchCount: Int,
  uniqueUserCount: Int,
  totalDuration: Long,
  uniqueUsers: Set[String],
  deviceTypes: Set[String]
) {
  def contentId: String = "" // ì‹¤ì œë¡œëŠ” í‚¤ì—ì„œ ê°€ì ¸ì˜´
}

// ì‚¬ìš©ì ì°¸ì—¬ë„ ë¶„ì„ Aggregator  
class UserEngagementAggregator extends AggregateFunction[UserEvent, EngagementAccumulator, UserEngagementScore] {
  
  override def createAccumulator(): EngagementAccumulator = 
    EngagementAccumulator(0, 0, 0L, 0L, Set.empty)
  
  override def add(event: UserEvent, acc: EngagementAccumulator): EngagementAccumulator = {
    val isWatchEvent = event.eventType == "content.watched"
    val isInteractionEvent = Set("content.liked", "content.shared", "content.rated").contains(event.eventType)
    
    acc.copy(
      contentCount = if (isWatchEvent) acc.contentCount + 1 else acc.contentCount,
      interactionCount = if (isInteractionEvent) acc.interactionCount + 1 else acc.interactionCount,
      sessionStart = if (acc.sessionStart == 0) event.timestamp else math.min(acc.sessionStart, event.timestamp),
      sessionEnd = math.max(acc.sessionEnd, event.timestamp),
      contentTypes = acc.contentTypes + extractContentType(event)
    )
  }
  
  override def getResult(acc: EngagementAccumulator): UserEngagementScore = {
    val sessionDuration = acc.sessionEnd - acc.sessionStart
    val contentDiversity = acc.contentTypes.size
    val interactionRate = if (acc.contentCount > 0) acc.interactionCount.toDouble / acc.contentCount else 0.0
    
    // ì°¸ì—¬ë„ ì ìˆ˜ ê³„ì‚°
    val engagementScore = math.min(
      (acc.contentCount * 0.3 + 
       contentDiversity * 0.3 + 
       interactionRate * 0.4) / 3.0 * 100, 
      100.0
    )
    
    UserEngagementScore(
      userId = "", // í‚¤ì—ì„œ ì¶”ì¶œ
      engagementScore = engagementScore,
      sessionDuration = sessionDuration,
      contentCount = acc.contentCount,
      timestamp = acc.sessionEnd
    )
  }
  
  override def merge(acc1: EngagementAccumulator, acc2: EngagementAccumulator): EngagementAccumulator = {
    acc1.copy(
      contentCount = acc1.contentCount + acc2.contentCount,
      interactionCount = acc1.interactionCount + acc2.interactionCount,
      sessionStart = if (acc1.sessionStart == 0) acc2.sessionStart else math.min(acc1.sessionStart, acc2.sessionStart),
      sessionEnd = math.max(acc1.sessionEnd, acc2.sessionEnd),
      contentTypes = acc1.contentTypes ++ acc2.contentTypes
    )
  }
  
  private def extractContentType(event: UserEvent): String = {
    // ì‹¤ì œë¡œëŠ” contentIdë¡œ ì½˜í…ì¸  íƒ€ì… ì¡°íšŒ
    "drama" // ê¸°ë³¸ê°’
  }
}

case class EngagementAccumulator(
  contentCount: Int,
  interactionCount: Int,
  sessionStart: Long,
  sessionEnd: Long,
  contentTypes: Set[String]
)
```

---

## ğŸ”„ ì´ë²¤íŠ¸ ì†Œì‹± (Event Sourcing) ì‹¬í™”

### ì´ë²¤íŠ¸ ìŠ¤í† ì–´ êµ¬í˜„

```python
# Pythonìœ¼ë¡œ êµ¬í˜„í•œ ì´ë²¤íŠ¸ ìŠ¤í† ì–´
from dataclasses import dataclass, field
from typing import List, Optional, Dict, Any, Iterator
from datetime import datetime
import json
import uuid
from abc import ABC, abstractmethod

@dataclass
class EventMetadata:
    event_id: str
    event_type: str
    event_version: str
    timestamp: datetime
    correlation_id: Optional[str] = None
    causation_id: Optional[str] = None
    user_id: Optional[str] = None

@dataclass
class StoredEvent:
    """ì €ì¥ëœ ì´ë²¤íŠ¸ í‘œí˜„"""
    stream_id: str
    stream_version: int
    event_id: str
    event_type: str
    event_data: Dict[str, Any]
    metadata: EventMetadata
    timestamp: datetime

class EventStore(ABC):
    """ì´ë²¤íŠ¸ ìŠ¤í† ì–´ ì¶”ìƒ ì¸í„°í˜ì´ìŠ¤"""
    
    @abstractmethod
    async def append_to_stream(
        self, 
        stream_id: str, 
        expected_version: int,
        events: List[Dict[str, Any]]
    ) -> None:
        """ìŠ¤íŠ¸ë¦¼ì— ì´ë²¤íŠ¸ ì¶”ê°€"""
        pass
    
    @abstractmethod
    async def read_stream(
        self, 
        stream_id: str, 
        from_version: int = 0,
        max_count: Optional[int] = None
    ) -> List[StoredEvent]:
        """ìŠ¤íŠ¸ë¦¼ì—ì„œ ì´ë²¤íŠ¸ ì½ê¸°"""
        pass
    
    @abstractmethod
    async def read_all_events(
        self,
        from_position: int = 0,
        max_count: Optional[int] = None
    ) -> List[StoredEvent]:
        """ëª¨ë“  ì´ë²¤íŠ¸ ì½ê¸° (ê¸€ë¡œë²Œ ìˆœì„œ)"""
        pass

class PostgreSQLEventStore(EventStore):
    """PostgreSQL ê¸°ë°˜ ì´ë²¤íŠ¸ ìŠ¤í† ì–´ êµ¬í˜„"""
    
    def __init__(self, connection_pool):
        self.pool = connection_pool
        
    async def append_to_stream(
        self, 
        stream_id: str, 
        expected_version: int,
        events: List[Dict[str, Any]]
    ) -> None:
        async with self.pool.acquire() as conn:
            async with conn.transaction():
                # ë™ì‹œì„± ì œì–´: í˜„ì¬ ìŠ¤íŠ¸ë¦¼ ë²„ì „ í™•ì¸
                current_version = await self._get_stream_version(conn, stream_id)
                
                if current_version != expected_version:
                    raise ConcurrencyException(
                        f"ì˜ˆìƒ ë²„ì „ {expected_version}, ì‹¤ì œ ë²„ì „ {current_version}"
                    )
                
                # ì´ë²¤íŠ¸ ì €ì¥
                for i, event in enumerate(events):
                    next_version = expected_version + i + 1
                    
                    await conn.execute("""
                        INSERT INTO events (
                            event_id, stream_id, stream_version, event_type,
                            event_data, metadata, timestamp
                        ) VALUES ($1, $2, $3, $4, $5, $6, $7)
                    """, 
                        event['event_id'],
                        stream_id,
                        next_version,
                        event['event_type'],
                        json.dumps(event['data']),
                        json.dumps(event['metadata'].__dict__),
                        event['timestamp']
                    )
                
                # ìŠ¤íŠ¸ë¦¼ ë²„ì „ ì—…ë°ì´íŠ¸
                final_version = expected_version + len(events)
                await conn.execute("""
                    INSERT INTO streams (stream_id, version, last_updated)
                    VALUES ($1, $2, $3)
                    ON CONFLICT (stream_id)
                    DO UPDATE SET version = $2, last_updated = $3
                """, stream_id, final_version, datetime.utcnow())
    
    async def read_stream(
        self, 
        stream_id: str, 
        from_version: int = 0,
        max_count: Optional[int] = None
    ) -> List[StoredEvent]:
        async with self.pool.acquire() as conn:
            query = """
                SELECT event_id, stream_id, stream_version, event_type,
                       event_data, metadata, timestamp
                FROM events 
                WHERE stream_id = $1 AND stream_version > $2
                ORDER BY stream_version
            """
            params = [stream_id, from_version]
            
            if max_count:
                query += " LIMIT $3"
                params.append(max_count)
            
            rows = await conn.fetch(query, *params)
            return [self._row_to_stored_event(row) for row in rows]
    
    async def read_all_events(
        self,
        from_position: int = 0,
        max_count: Optional[int] = None
    ) -> List[StoredEvent]:
        async with self.pool.acquire() as conn:
            query = """
                SELECT event_id, stream_id, stream_version, event_type,
                       event_data, metadata, timestamp, global_position
                FROM events 
                WHERE global_position > $1
                ORDER BY global_position
            """
            params = [from_position]
            
            if max_count:
                query += " LIMIT $2"
                params.append(max_count)
            
            rows = await conn.fetch(query, *params)
            return [self._row_to_stored_event(row) for row in rows]
    
    async def _get_stream_version(self, conn, stream_id: str) -> int:
        row = await conn.fetchrow(
            "SELECT version FROM streams WHERE stream_id = $1", 
            stream_id
        )
        return row['version'] if row else 0
    
    def _row_to_stored_event(self, row) -> StoredEvent:
        metadata_dict = json.loads(row['metadata'])
        
        return StoredEvent(
            stream_id=row['stream_id'],
            stream_version=row['stream_version'],
            event_id=row['event_id'],
            event_type=row['event_type'],
            event_data=json.loads(row['event_data']),
            metadata=EventMetadata(**metadata_dict),
            timestamp=row['timestamp']
        )

# ì§‘í•©ì²´ ë£¨íŠ¸ ê¸°ë°˜ í´ë˜ìŠ¤
class AggregateRoot:
    """ì´ë²¤íŠ¸ ì†Œì‹± ê¸°ë°˜ ì§‘í•©ì²´ ë£¨íŠ¸"""
    
    def __init__(self, aggregate_id: str):
        self.aggregate_id = aggregate_id
        self.version = 0
        self.uncommitted_events: List[Dict[str, Any]] = []
        
    def apply_event(self, event_data: Dict[str, Any], metadata: EventMetadata) -> None:
        """ì´ë²¤íŠ¸ë¥¼ ì§‘í•©ì²´ì— ì ìš©"""
        event_handler_name = f"_handle_{metadata.event_type.replace('.', '_')}"
        event_handler = getattr(self, event_handler_name, None)
        
        if event_handler:
            event_handler(event_data)
        
        self.version += 1
    
    def raise_event(self, event_type: str, event_data: Dict[str, Any]) -> None:
        """ìƒˆ ì´ë²¤íŠ¸ ë°œìƒ"""
        metadata = EventMetadata(
            event_id=str(uuid.uuid4()),
            event_type=event_type,
            event_version="1.0",
            timestamp=datetime.utcnow(),
            correlation_id=self._get_current_correlation_id()
        )
        
        # ë¨¼ì € ì´ë²¤íŠ¸ë¥¼ ì§‘í•©ì²´ì— ì ìš©
        self.apply_event(event_data, metadata)
        
        # ì»¤ë°‹ë˜ì§€ ì•Šì€ ì´ë²¤íŠ¸ ëª©ë¡ì— ì¶”ê°€
        self.uncommitted_events.append({
            'event_id': metadata.event_id,
            'event_type': event_type,
            'data': event_data,
            'metadata': metadata,
            'timestamp': metadata.timestamp
        })
    
    def mark_events_as_committed(self) -> None:
        """ì´ë²¤íŠ¸ë“¤ì„ ì»¤ë°‹ëœ ê²ƒìœ¼ë¡œ í‘œì‹œ"""
        self.uncommitted_events.clear()
    
    def get_uncommitted_events(self) -> List[Dict[str, Any]]:
        """ì»¤ë°‹ë˜ì§€ ì•Šì€ ì´ë²¤íŠ¸ ëª©ë¡ ë°˜í™˜"""
        return self.uncommitted_events.copy()
    
    def _get_current_correlation_id(self) -> Optional[str]:
        # í˜„ì¬ ìš”ì²­ ì»¨í…ìŠ¤íŠ¸ì—ì„œ correlation ID ì¶”ì¶œ
        # ì‹¤ì œë¡œëŠ” ThreadLocalì´ë‚˜ AsyncLocal ì‚¬ìš©
        return getattr(self, '_correlation_id', None)

# ì‚¬ìš©ì ì§‘í•©ì²´ ì˜ˆì œ
class User(AggregateRoot):
    def __init__(self, user_id: str):
        super().__init__(user_id)
        self.email: Optional[str] = None
        self.name: Optional[str] = None
        self.subscription_plan: Optional[str] = None
        self.preferences: List[str] = []
        self.is_active: bool = True
    
    def register(self, email: str, name: str, initial_preferences: List[str]) -> None:
        """ì‚¬ìš©ì ë“±ë¡"""
        if self.email is not None:
            raise DomainException("ì´ë¯¸ ë“±ë¡ëœ ì‚¬ìš©ìì…ë‹ˆë‹¤")
        
        self.raise_event("user.registered", {
            "user_id": self.aggregate_id,
            "email": email,
            "name": name,
            "initial_preferences": initial_preferences,
            "registration_timestamp": datetime.utcnow().isoformat()
        })
    
    def subscribe_to_plan(self, plan: str, payment_method: str) -> None:
        """êµ¬ë… í”Œëœ ê°€ì…"""
        if not self.is_active:
            raise DomainException("ë¹„í™œì„± ì‚¬ìš©ìëŠ” êµ¬ë…í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        self.raise_event("user.subscribed", {
            "user_id": self.aggregate_id,
            "plan": plan,
            "payment_method": payment_method,
            "subscription_timestamp": datetime.utcnow().isoformat()
        })
    
    def update_preferences(self, new_preferences: List[str]) -> None:
        """ì‚¬ìš©ì ì„ í˜¸ë„ ì—…ë°ì´íŠ¸"""
        if not self.is_active:
            raise DomainException("ë¹„í™œì„± ì‚¬ìš©ìëŠ” ì„ í˜¸ë„ë¥¼ ë³€ê²½í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        self.raise_event("user.preferences_updated", {
            "user_id": self.aggregate_id,
            "old_preferences": self.preferences.copy(),
            "new_preferences": new_preferences,
            "update_timestamp": datetime.utcnow().isoformat()
        })
    
    # ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ë“¤
    def _handle_user_registered(self, event_data: Dict[str, Any]) -> None:
        self.email = event_data["email"]
        self.name = event_data["name"]
        self.preferences = event_data["initial_preferences"]
    
    def _handle_user_subscribed(self, event_data: Dict[str, Any]) -> None:
        self.subscription_plan = event_data["plan"]
    
    def _handle_user_preferences_updated(self, event_data: Dict[str, Any]) -> None:
        self.preferences = event_data["new_preferences"]

# ë¦¬í¬ì§€í† ë¦¬ íŒ¨í„´ìœ¼ë¡œ ì§‘í•©ì²´ ì €ì¥/ë¡œë“œ
class UserRepository:
    def __init__(self, event_store: EventStore):
        self.event_store = event_store
    
    async def save(self, user: User) -> None:
        """ì‚¬ìš©ì ì§‘í•©ì²´ ì €ì¥"""
        uncommitted_events = user.get_uncommitted_events()
        
        if uncommitted_events:
            await self.event_store.append_to_stream(
                stream_id=f"user-{user.aggregate_id}",
                expected_version=user.version - len(uncommitted_events),
                events=uncommitted_events
            )
            
            user.mark_events_as_committed()
    
    async def load(self, user_id: str) -> Optional[User]:
        """ì‚¬ìš©ì ì§‘í•©ì²´ ë¡œë“œ"""
        stream_id = f"user-{user_id}"
        events = await self.event_store.read_stream(stream_id)
        
        if not events:
            return None
        
        user = User(user_id)
        
        for event in events:
            user.apply_event(event.event_data, event.metadata)
        
        return user

# ì‚¬ìš© ì˜ˆì œ
async def example_usage():
    # ì´ë²¤íŠ¸ ìŠ¤í† ì–´ ì´ˆê¸°í™”
    event_store = PostgreSQLEventStore(connection_pool)
    user_repo = UserRepository(event_store)
    
    # ìƒˆ ì‚¬ìš©ì ë“±ë¡
    user = User("user-123")
    user.register(
        email="john@example.com",
        name="John Doe",
        initial_preferences=["drama", "comedy", "action"]
    )
    
    # êµ¬ë… ê°€ì…
    user.subscribe_to_plan("premium", "credit_card")
    
    # ì„ í˜¸ë„ ë³€ê²½
    user.update_preferences(["drama", "thriller", "documentary"])
    
    # ì§‘í•©ì²´ ì €ì¥ (ëª¨ë“  ì´ë²¤íŠ¸ê°€ ì›ìì ìœ¼ë¡œ ì €ì¥ë¨)
    await user_repo.save(user)
    
    # ë‚˜ì¤‘ì— ì‚¬ìš©ì ë¡œë“œ (ì´ë²¤íŠ¸ ì¬ìƒìœ¼ë¡œ ìƒíƒœ ë³µì›)
    loaded_user = await user_repo.load("user-123")
    print(f"ë¡œë“œëœ ì‚¬ìš©ì: {loaded_user.name}, ì„ í˜¸ë„: {loaded_user.preferences}")
```

---

## ğŸ¯ ì´ë²¤íŠ¸ ë“œë¦¬ë¸ ì•„í‚¤í…ì²˜ ì„±ê³µ ìš”ì¸

### âœ… í•µì‹¬ ì„±ê³µ ìš”ì¸ë“¤

```bash
1. ì´ë²¤íŠ¸ ì„¤ê³„ ì›ì¹™
   âœ… ê³¼ê±° ì‹œì œ ì‚¬ìš© (UserRegistered, OrderCreated)
   âœ… ë¶ˆë³€ì„± ë³´ì¥ (ì´ë²¤íŠ¸ëŠ” ìˆ˜ì •ë˜ì§€ ì•ŠìŒ)
   âœ… ë¹„ì¦ˆë‹ˆìŠ¤ ì˜ë¯¸ ìˆëŠ” ì´ë²¤íŠ¸
   âœ… ì ì ˆí•œ ì´ë²¤íŠ¸ í¬ê¸° (ë„ˆë¬´ í¬ì§€ë„ ì‘ì§€ë„ ì•Šê²Œ)

2. ì´ë²¤íŠ¸ ìŠ¤í‚¤ë§ˆ ê´€ë¦¬
   âœ… ìŠ¤í‚¤ë§ˆ ì§„í™” ì „ëµ (ë²„ì „ ê´€ë¦¬)
   âœ… í•˜ìœ„ í˜¸í™˜ì„± ë³´ì¥
   âœ… Schema Registry í™œìš©
   âœ… ì´ë²¤íŠ¸ ê²€ì¦ ìë™í™”

3. ë‚´ê²°í•¨ì„± ì„¤ê³„
   âœ… At-least-once ì „ì†¡ ë³´ì¥
   âœ… ë©±ë“±ì„± ì²˜ë¦¬
   âœ… ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜
   âœ… Circuit Breaker íŒ¨í„´

4. ëª¨ë‹ˆí„°ë§ê³¼ ë””ë²„ê¹…
   âœ… ë¶„ì‚° íŠ¸ë ˆì´ì‹±
   âœ… ì´ë²¤íŠ¸ í”Œë¡œìš° ì‹œê°í™”
   âœ… ì§€ì—° ì‹œê°„ ëª¨ë‹ˆí„°ë§
   âœ… ì˜¤ë¥˜ìœ¨ ì¶”ì 
```

### âŒ ì£¼ì˜í•´ì•¼ í•  ì•ˆí‹°íŒ¨í„´ë“¤

```bash
1. ì´ë²¤íŠ¸ ì„¤ê³„ ì‹¤ìˆ˜
   âŒ ë°ì´í„° ë³€ê²½ì´ ì•„ë‹Œ ë‹¨ìˆœ ì•Œë¦¼ìš© ì´ë²¤íŠ¸
   âŒ ë„ˆë¬´ í° ì´ë²¤íŠ¸ í˜ì´ë¡œë“œ
   âŒ ê¸°ìˆ ì  ì„¸ë¶€ì‚¬í•­ì´ ë…¸ì¶œëœ ì´ë²¤íŠ¸
   âŒ ì¼ê´€ì„± ì—†ëŠ” ì´ë²¤íŠ¸ ë„¤ì´ë°

2. ì•„í‚¤í…ì²˜ ì‹¤ìˆ˜
   âŒ ë™ê¸°ì‹ ì²˜ë¦¬ì— ì´ë²¤íŠ¸ ì–µì§€ë¡œ ì ìš©
   âŒ ì´ë²¤íŠ¸ë¡œ ëª¨ë“  ì„œë¹„ìŠ¤ ê°„ í†µì‹  ëŒ€ì²´
   âŒ ìˆœí™˜ ì´ë²¤íŠ¸ ì˜ì¡´ì„±
   âŒ ì´ë²¤íŠ¸ ìˆœì„œì— ëŒ€í•œ ê³¼ë„í•œ ì˜ì¡´

3. ìš´ì˜ìƒ ì‹¤ìˆ˜
   âŒ ë°±í”„ë ˆì…” ì²˜ë¦¬ ë¯¸í¡
   âŒ íŒŒë… í ê´€ë¦¬ ì†Œí™€
   âŒ ì´ë²¤íŠ¸ ìŠ¤í‚¤ë§ˆ ê²€ì¦ ìƒëµ
   âŒ ëª¨ë‹ˆí„°ë§ ë¶€ì¡±
```

### ğŸ› ï¸ ì‹¤ë¬´ êµ¬í˜„ ì²´í¬ë¦¬ìŠ¤íŠ¸

```yaml
# ì´ë²¤íŠ¸ ë“œë¦¬ë¸ ì‹œìŠ¤í…œ êµ¬ì¶• ì²´í¬ë¦¬ìŠ¤íŠ¸

ì´ë²¤íŠ¸ ì„¤ê³„:
  - [ ] ë„ë©”ì¸ ì´ë²¤íŠ¸ ì‹ë³„ ì™„ë£Œ
  - [ ] ì´ë²¤íŠ¸ ìŠ¤í‚¤ë§ˆ ì •ì˜
  - [ ] ì´ë²¤íŠ¸ ë²„ì „ ê´€ë¦¬ ì „ëµ ìˆ˜ë¦½
  - [ ] ì´ë²¤íŠ¸ ìƒëª…ì£¼ê¸° ì •ì˜

ì¸í”„ë¼ìŠ¤íŠ¸ëŸ­ì²˜:
  - [ ] ë©”ì‹œì§€ ë¸Œë¡œì»¤ ì„ íƒ (Kafka/RabbitMQ/Pulsar)
  - [ ] ìŠ¤í‚¤ë§ˆ ë ˆì§€ìŠ¤íŠ¸ë¦¬ êµ¬ì¶•
  - [ ] ì´ë²¤íŠ¸ ìŠ¤í† ì–´ êµ¬í˜„
  - [ ] ë°±ì—…/ë³µêµ¬ ì „ëµ ìˆ˜ë¦½

ì„œë¹„ìŠ¤ êµ¬í˜„:
  - [ ] ì´ë²¤íŠ¸ í”„ë¡œë“€ì„œ êµ¬í˜„
  - [ ] ì´ë²¤íŠ¸ ì»¨ìŠˆë¨¸ êµ¬í˜„
  - [ ] ë©±ë“±ì„± ì²˜ë¦¬ êµ¬í˜„
  - [ ] ì¬ì‹œë„ ë¡œì§ êµ¬í˜„

ëª¨ë‹ˆí„°ë§:
  - [ ] ì´ë²¤íŠ¸ í”Œë¡œìš° ì¶”ì 
  - [ ] ì§€ì—° ì‹œê°„ ëª¨ë‹ˆí„°ë§
  - [ ] ì˜¤ë¥˜ìœ¨ ì•Œë¦¼ ì„¤ì •
  - [ ] ì²˜ë¦¬ëŸ‰ ë©”íŠ¸ë¦­ ìˆ˜ì§‘

í…ŒìŠ¤íŒ…:
  - [ ] ì´ë²¤íŠ¸ ê¸°ë°˜ í…ŒìŠ¤íŠ¸ ì‘ì„±
  - [ ] í†µí•© í…ŒìŠ¤íŠ¸ ìë™í™”
  - [ ] ì¹´ì˜¤ìŠ¤ ì—”ì§€ë‹ˆì–´ë§ ì ìš©
  - [ ] ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤ì‹œ
```

---

## ğŸš€ ë‹¤ìŒ ë‹¨ê³„

Event-Driven Architectureì˜ ê¸°ì´ˆë¥¼ íƒ„íƒ„íˆ ë‹¤ì¡Œìœ¼ë‹ˆ, ì´ì œ ë” ê³ ê¸‰ íŒ¨í„´ë“¤ì„ í•™ìŠµí•  ì‹œê°„ì…ë‹ˆë‹¤.

[15.3 CQRSì™€ ì´ë²¤íŠ¸ ì†Œì‹±](03-cqrs-event-sourcing.md)ì—ì„œëŠ” ëª…ë ¹ê³¼ ì¡°íšŒë¥¼ ë¶„ë¦¬í•˜ê³  ì´ë²¤íŠ¸ë¡œ ìƒíƒœë¥¼ ê´€ë¦¬í•˜ëŠ” ê³ ê¸‰ ì•„í‚¤í…ì²˜ íŒ¨í„´ì„ ì‹¬ë„ ìˆê²Œ ë‹¤ë¤„ë³´ê² ìŠµë‹ˆë‹¤.

"ì´ë²¤íŠ¸ëŠ” ê³¼ê±°ì— ì¼ì–´ë‚œ ì‚¬ì‹¤ì…ë‹ˆë‹¤. ê·¸ ì‚¬ì‹¤ì„ ë°”íƒ•ìœ¼ë¡œ í˜„ì¬ì™€ ë¯¸ë˜ë¥¼ ê²°ì •í•˜ëŠ” ê²ƒì´ Event-Driven Architectureì˜ í•µì‹¬ì…ë‹ˆë‹¤."

ì‹¤ì‹œê°„ìœ¼ë¡œ ë°˜ì‘í•˜ëŠ” ì‹œìŠ¤í…œì„ í†µí•´ ì‚¬ìš©ìì—ê²Œ ë” ë‚˜ì€ ê²½í—˜ì„ ì œê³µí•´ë´…ì‹œë‹¤! ğŸ¯âš¡
